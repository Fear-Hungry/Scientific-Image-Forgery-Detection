{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18faa304",
   "metadata": {},
   "source": [
    "# Pipeline completo — Recod.ai/LUC (Kaggle)\n",
    "\n",
    "Neste notebook, iremos desenvolver um **pipeline completo** para o desafio\n",
    "**Recod.ai/LUC — Scientific Image Forgery Detection** do Kaggle. O objetivo do\n",
    "desafio é detectar e segmentar regiões manipuladas (*copy-move forgeries*) em\n",
    "imagens científicas biomédicas.\n",
    "\n",
    "O problema envolve duas tarefas principais:\n",
    "\n",
    "1) **Classificação binária** — identificar se a imagem contém fraude\n",
    "   (*authentic* vs. *forged*)\n",
    "2) **Segmentação** — indicar os **pixels exatos** onde há fraude\n",
    "\n",
    "Este pipeline cobre tudo **do treinamento** dos modelos **até a geração do\n",
    "`submission.csv`**, incluindo boas práticas de participantes de alto desempenho:\n",
    "**TTA** (Test Time Augmentation) e **pós-processamento morfológico**.\n",
    "\n",
    "Fluxo do notebook:\n",
    "\n",
    "- **Setup** (paths, cache offline, validações)\n",
    "- **Treino opcional** do classificador\n",
    "- **Treino opcional** do segmentador\n",
    "- **Inferência + pós-processamento** (TTA + morfologia)\n",
    "- **Geração do `submission.csv`**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9444ef47",
   "metadata": {},
   "source": [
    "## 1. Modo Kaggle offline (internet OFF) + setup\n",
    "\n",
    "Para rodar **offline** no Kaggle (modo submissão), este notebook foi pensado\n",
    "para funcionar com:\n",
    "\n",
    "- **Dataset da competição** montado em `/kaggle/input/...` (padrão).\n",
    "- **Este repositório** empacotado como Kaggle Dataset (recomendado).\n",
    "- *(Opcional)* **wheels** locais em `.../recodai_bundle/wheels/` para instalar\n",
    "  dependências que não vêm no ambiente do Kaggle.\n",
    "- *(Opcional)* **cache de pesos** (timm/torch hub e/ou HuggingFace) para usar\n",
    "  modelos/pretrained sem downloads.\n",
    "\n",
    "Variáveis de ambiente úteis (todas opcionais):\n",
    "\n",
    "- `FORGERYSEG_DATA_ROOT`: força o path do dataset (ex.: `/kaggle/input/.../recodai`).\n",
    "- `FORGERYSEG_REPO_ROOT`: força o path do repo (ex.: `/kaggle/input/<ds>/recodai_bundle`).\n",
    "- `FORGERYSEG_WHEELS_ROOT`: força o path dos wheels (ex.: `/kaggle/input/<ds>/recodai_bundle/wheels`).\n",
    "- `FORGERYSEG_CACHE_ROOT`: força o path do cache (ex.: `/kaggle/input/<ds>/recodai_bundle/weights_cache`).\n",
    "- `FORGERYSEG_ALLOW_DOWNLOAD=1`: libera downloads (NÃO use na submissão offline).\n",
    "\n",
    "Dica: para “empacotar” tudo em um Dataset para Kaggle, use `recodai_bundle/`\n",
    "(há pastas `wheels/` e `weights_cache/` prontas).\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Instalação de Pacotes (opcional)\n",
    "\n",
    "Nesta seção, instalamos as bibliotecas necessárias e configuramos parâmetros\n",
    "globais (diretórios de dados e *device*). Garantimos uso de GPU quando disponível.\n",
    "\n",
    "Bibliotecas usadas:\n",
    "\n",
    "- **PyTorch**: framework de deep learning.\n",
    "- **Albumentations**: aumentos de dados (imagem + máscara).\n",
    "- **segmentation_models_pytorch (SMP)**: U-Net, U-Net++, DeepLabV3+, etc.\n",
    "- **Transformers (HuggingFace)**: modelos como DINOv2 e SegFormer.\n",
    "- **OpenCV**: operações morfológicas no pós-processamento.\n",
    "- **numpy/pandas**: manipulação de dados.\n",
    "\n",
    "Instalação (se necessário no Kaggle):\n",
    "\n",
    "```\n",
    "!pip install -q segmentation-models-pytorch==0.3.0 albumentations==1.3.0 transformers==4.34.0 timm==0.9.2\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3019c116",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df4c7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers de ambiente\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def env_str(name: str, default: str = \"\") -> str:\n",
    "    value = os.environ.get(name, \"\")\n",
    "    if value == \"\":\n",
    "        return str(default)\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def env_bool(name: str, default: bool = False) -> bool:\n",
    "    value = os.environ.get(name, \"\")\n",
    "    if value == \"\":\n",
    "        return bool(default)\n",
    "    return str(value).strip().lower() in {\"1\", \"true\", \"yes\", \"y\", \"on\"}\n",
    "\n",
    "\n",
    "def env_path(name: str) -> Path | None:\n",
    "    value = env_str(name, \"\").strip()\n",
    "    if not value:\n",
    "        return None\n",
    "    return Path(value)\n",
    "\n",
    "\n",
    "def run_cmd(cmd: list[str], cwd: Path | None = None) -> None:\n",
    "    cmd_str = \" \".join(str(c) for c in cmd)\n",
    "    print(\"[cmd]\", cmd_str)\n",
    "    subprocess.run(cmd, check=True, cwd=str(cwd) if cwd else None)\n",
    "\n",
    "\n",
    "def find_repo_root() -> Path | None:\n",
    "    here = Path(\".\").resolve()\n",
    "    candidates = [here] + list(here.parents)\n",
    "    for cand in candidates:\n",
    "        if (cand / \"src\" / \"forgeryseg\" / \"__init__.py\").exists() and (cand / \"scripts\").exists():\n",
    "            return cand\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                for base in (ds, ds / \"recodai_bundle\"):\n",
    "                    if (base / \"src\" / \"forgeryseg\" / \"__init__.py\").exists():\n",
    "                        return base\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_data_root() -> Path | None:\n",
    "    candidates = [\n",
    "        Path(\"data/recodai\"),\n",
    "        Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection/recodai\"),\n",
    "        Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"),\n",
    "    ]\n",
    "    for cand in candidates:\n",
    "        if (cand / \"train_images\").exists():\n",
    "            return cand\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                if (ds / \"train_images\").exists():\n",
    "                    return ds\n",
    "                if (ds / \"recodai\" / \"train_images\").exists():\n",
    "                    return ds / \"recodai\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_wheels_root() -> Path | None:\n",
    "    explicit = env_path(\"FORGERYSEG_WHEELS_ROOT\")\n",
    "    if explicit is not None:\n",
    "        return explicit if explicit.exists() else None\n",
    "\n",
    "    local_candidates = [Path(\"recodai_bundle\") / \"wheels\", Path(\"wheels\")]\n",
    "    for cand in local_candidates:\n",
    "        if cand.exists() and any(cand.glob(\"*.whl\")):\n",
    "            return cand\n",
    "\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                for cand in (ds / \"wheels\", ds / \"recodai_bundle\" / \"wheels\"):\n",
    "                    if cand.exists() and any(cand.glob(\"*.whl\")):\n",
    "                        return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_cache_root() -> Path | None:\n",
    "    explicit = env_path(\"FORGERYSEG_CACHE_ROOT\")\n",
    "    if explicit is not None:\n",
    "        return explicit if explicit.exists() else None\n",
    "\n",
    "    local_candidates = [Path(\"recodai_bundle\") / \"weights_cache\", Path(\"weights_cache\")]\n",
    "    for cand in local_candidates:\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                for cand in (ds / \"weights_cache\", ds / \"recodai_bundle\" / \"weights_cache\"):\n",
    "                    if cand.exists():\n",
    "                        return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_requirements_file() -> Path | None:\n",
    "    candidates: list[Path] = []\n",
    "    here = Path(\".\").resolve()\n",
    "    for cand in [here] + list(here.parents):\n",
    "        candidates.append(cand / \"requirements.txt\")\n",
    "        candidates.append(cand / \"recodai_bundle\" / \"requirements.txt\")\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                candidates.append(ds / \"requirements.txt\")\n",
    "                candidates.append(ds / \"recodai_bundle\" / \"requirements.txt\")\n",
    "    for cand in candidates:\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def _missing_modules(mod_names: list[str]) -> list[str]:\n",
    "    missing: list[str] = []\n",
    "    for name in mod_names:\n",
    "        try:\n",
    "            __import__(name)\n",
    "        except Exception:\n",
    "            missing.append(name)\n",
    "    return missing\n",
    "\n",
    "\n",
    "def maybe_install_from_wheels(wheels_root: Path | None) -> None:\n",
    "    \"\"\"Try to install only what is missing, using local wheels (offline-safe).\"\"\"\n",
    "    if wheels_root is None:\n",
    "        return\n",
    "\n",
    "    module_to_pip = {\n",
    "        \"segmentation_models_pytorch\": \"segmentation-models-pytorch\",\n",
    "        \"huggingface_hub\": \"huggingface-hub\",\n",
    "        \"safetensors\": \"safetensors\",\n",
    "        \"tqdm\": \"tqdm\",\n",
    "    }\n",
    "    wanted_modules = list(module_to_pip.keys())\n",
    "    missing = _missing_modules(wanted_modules)\n",
    "    if not missing:\n",
    "        print(\"[wheels] ok (nada a instalar).\")\n",
    "        return\n",
    "\n",
    "    packages = [module_to_pip[m] for m in missing if m in module_to_pip]\n",
    "    if not packages:\n",
    "        return\n",
    "\n",
    "    print(\"[wheels] faltando:\", \", \".join(missing))\n",
    "    print(\"[wheels] instalando via --no-index/--find-links:\", wheels_root)\n",
    "    run_cmd(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheels_root),\n",
    "            *packages,\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def has_any_ckpt(dir_name: str, pattern: str, outputs_root: Path) -> bool:\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                for base in (ds, ds / \"recodai_bundle\"):\n",
    "                    cand = base / \"outputs\" / dir_name\n",
    "                    if cand.exists() and any(cand.glob(pattern)):\n",
    "                        return True\n",
    "    cand = outputs_root / dir_name\n",
    "    return cand.exists() and any(cand.glob(pattern))\n",
    "\n",
    "\n",
    "def write_config(base_cfg: Path, out_path: Path, overrides: dict) -> Path:\n",
    "    with base_cfg.open(\"r\") as f:\n",
    "        cfg = json.load(f)\n",
    "    cfg.update(overrides)\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with out_path.open(\"w\") as f:\n",
    "        json.dump(cfg, f, indent=2)\n",
    "    return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5049ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config base do notebook (edite via env vars, se quiser)\n",
    "KAGGLE = is_kaggle()\n",
    "ALLOW_DOWNLOAD = env_bool(\"FORGERYSEG_ALLOW_DOWNLOAD\", default=not KAGGLE)\n",
    "OFFLINE = bool(KAGGLE and not ALLOW_DOWNLOAD)\n",
    "\n",
    "# Instaladores (opcionais)\n",
    "RUN_PIP_INSTALL = env_bool(\"FORGERYSEG_PIP_INSTALL\", default=ALLOW_DOWNLOAD)\n",
    "INSTALL_WHEELS = env_bool(\"FORGERYSEG_INSTALL_WHEELS\", default=KAGGLE and not ALLOW_DOWNLOAD)\n",
    "\n",
    "WHEELS_ROOT = find_wheels_root() if INSTALL_WHEELS else None\n",
    "if INSTALL_WHEELS:\n",
    "    if WHEELS_ROOT is None:\n",
    "        print(\"[wheels] nenhum wheel root encontrado (ok se já tiver as libs no ambiente).\")\n",
    "    else:\n",
    "        maybe_install_from_wheels(WHEELS_ROOT)\n",
    "else:\n",
    "    print(\"INSTALL_WHEELS=False (pulando).\")\n",
    "\n",
    "if RUN_PIP_INSTALL:\n",
    "    # Atenção: isso usa internet (a menos que você esteja apontando um index local).\n",
    "    req_path = find_requirements_file()\n",
    "    if req_path is None:\n",
    "        raise FileNotFoundError(\n",
    "            \"RUN_PIP_INSTALL=True, mas não encontrei `requirements.txt`. \"\n",
    "            \"Defina `FORGERYSEG_REPO_ROOT`/mude o cwd, ou desative `FORGERYSEG_PIP_INSTALL`.\"\n",
    "        )\n",
    "    run_cmd([sys.executable, \"-m\", \"pip\", \"install\", \"-r\", str(req_path)])\n",
    "else:\n",
    "    print(\"RUN_PIP_INSTALL=False (pulando).\")\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(\"Dispositivo:\", device)\n",
    "except Exception as exc:\n",
    "    raise RuntimeError(\"PyTorch não disponível. No Kaggle ele já vem instalado; localmente instale `torch`.\") from exc\n",
    "\n",
    "# Validação rápida de deps críticas (falha cedo, com mensagem clara)\n",
    "required_modules = [\n",
    "    \"numpy\",\n",
    "    \"cv2\",\n",
    "    \"timm\",\n",
    "    \"segmentation_models_pytorch\",\n",
    "]\n",
    "missing = _missing_modules(required_modules)\n",
    "if missing:\n",
    "    msg = (\n",
    "        \"Dependências Python faltando: \"\n",
    "        + \", \".join(missing)\n",
    "        + \"\\n- No Kaggle offline: anexe um Dataset com wheels em `.../recodai_bundle/wheels/` e rode com \"\n",
    "        + \"`FORGERYSEG_INSTALL_WHEELS=1`.\"\n",
    "        + \"\\n- Alternativa: habilite internet e use `FORGERYSEG_PIP_INSTALL=1` (não serve para submissão offline).\"\n",
    "    )\n",
    "    raise ImportError(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1469ae48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths dos dados da competição (Kaggle)\n",
    "DATA_ROOT = env_path(\"FORGERYSEG_DATA_ROOT\") or find_data_root() or Path(\"data/recodai\")\n",
    "\n",
    "if not DATA_ROOT.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"DATA_ROOT não existe. Defina `FORGERYSEG_DATA_ROOT` ou anexe o dataset do Kaggle.\"\n",
    "        f\"\\nTentativa: {DATA_ROOT}\"\n",
    "    )\n",
    "\n",
    "SAMPLE_SUB_PATH = DATA_ROOT / \"sample_submission.csv\"\n",
    "if SAMPLE_SUB_PATH.exists():\n",
    "    try:\n",
    "        import pandas as pd\n",
    "\n",
    "        sample_df = pd.read_csv(SAMPLE_SUB_PATH)\n",
    "        print(\"Colunas do sample submission:\", sample_df.columns.tolist())\n",
    "        print(sample_df.head(3))\n",
    "    except Exception:\n",
    "        print(\"[warn] pandas não disponível para preview do sample_submission.csv (ok).\")\n",
    "else:\n",
    "    print(\"sample_submission.csv não encontrado (ok fora do Kaggle).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f38c6ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Fase 0 — Sanidade Kaggle (lembrete)\n",
    "print(\"Kaggle constraints (lembrete):\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF no submit\")\n",
    "print(\"- Outputs: /kaggle/working/outputs (checkpoints)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae83f4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 0 — Config (edite esta célula)\n",
    "REPO_ROOT = env_path(\"FORGERYSEG_REPO_ROOT\") or find_repo_root() or Path(\".\").resolve()\n",
    "if not (REPO_ROOT / \"src\" / \"forgeryseg\" / \"__init__.py\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Não encontrei o código do repo (src/forgeryseg). \"\n",
    "        \"No Kaggle, anexe um Dataset com `recodai_bundle/` e/ou defina `FORGERYSEG_REPO_ROOT`.\"\n",
    "        f\"\\nTentativa: {REPO_ROOT}\"\n",
    "    )\n",
    "\n",
    "# Onde salvar checkpoints e logs\n",
    "OUTPUTS_ROOT = Path(\"/kaggle/working/outputs\") if is_kaggle() else REPO_ROOT / \"outputs\"\n",
    "OUTPUTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Cache offline (opcional). Ex.: /kaggle/input/<dataset>/weights_cache\n",
    "CACHE_ROOT = find_cache_root()\n",
    "\n",
    "# Configs base (pode trocar por outras em configs/)\n",
    "CLS_CONFIG = REPO_ROOT / \"configs\" / \"cls_effnet_b4.json\"\n",
    "SEG_CONFIG = REPO_ROOT / \"configs\" / \"seg_unetpp_tu_convnext_small.json\"\n",
    "INFER_CONFIG = REPO_ROOT / \"configs\" / \"infer_ensemble.json\"\n",
    "\n",
    "for cfg_path in (CLS_CONFIG, SEG_CONFIG, INFER_CONFIG):\n",
    "    if not cfg_path.exists():\n",
    "        raise FileNotFoundError(f\"Config não encontrado: {cfg_path}\")\n",
    "\n",
    "# CV / folds (FOLD=-1 para treinar todos)\n",
    "N_FOLDS = int(os.environ.get(\"FORGERYSEG_N_FOLDS\", \"5\"))\n",
    "FOLD = int(os.environ.get(\"FORGERYSEG_FOLD\", \"0\"))\n",
    "\n",
    "# Heurística: se já existem checkpoints, não treina (a menos que você force via env)\n",
    "HAS_CLS_CKPT = has_any_ckpt(\"models_cls\", \"fold_*/best.pt\", OUTPUTS_ROOT)\n",
    "HAS_SEG_CKPT = has_any_ckpt(\"models_seg\", \"*/*/best.pt\", OUTPUTS_ROOT)\n",
    "\n",
    "RUN_TRAIN_CLS = env_bool(\"FORGERYSEG_RUN_TRAIN_CLS\", default=not HAS_CLS_CKPT)\n",
    "RUN_TRAIN_SEG = env_bool(\"FORGERYSEG_RUN_TRAIN_SEG\", default=not HAS_SEG_CKPT)\n",
    "RUN_SUBMISSION = env_bool(\"FORGERYSEG_RUN_SUBMISSION\", default=True)\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"OUTPUTS_ROOT:\", OUTPUTS_ROOT)\n",
    "print(\"CACHE_ROOT:\", CACHE_ROOT)\n",
    "print(\"HAS_CLS_CKPT:\", HAS_CLS_CKPT)\n",
    "print(\"HAS_SEG_CKPT:\", HAS_SEG_CKPT)\n",
    "print(\"RUN_TRAIN_CLS:\", RUN_TRAIN_CLS)\n",
    "print(\"RUN_TRAIN_SEG:\", RUN_TRAIN_SEG)\n",
    "print(\"RUN_SUBMISSION:\", RUN_SUBMISSION)\n",
    "print(\"ALLOW_DOWNLOAD:\", ALLOW_DOWNLOAD)\n",
    "print(\"RUN_PIP_INSTALL:\", RUN_PIP_INSTALL)\n",
    "print(\"INSTALL_WHEELS:\", INSTALL_WHEELS)\n",
    "print(\"WHEELS_ROOT:\", WHEELS_ROOT)\n",
    "print(\"N_FOLDS:\", N_FOLDS)\n",
    "print(\"FOLD:\", FOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a8744d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 0 — Import do projeto + cache offline\n",
    "if (REPO_ROOT / \"src\" / \"forgeryseg\" / \"__init__.py\").exists() and str(REPO_ROOT / \"src\") not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT / \"src\"))\n",
    "\n",
    "from forgeryseg.dataset import build_supplemental_index, build_test_index, build_train_index\n",
    "from forgeryseg.offline import configure_cache_dirs\n",
    "\n",
    "if CACHE_ROOT is not None:\n",
    "    configure_cache_dirs(CACHE_ROOT)\n",
    "    print(\"[CACHE] using\", CACHE_ROOT)\n",
    "\n",
    "OFFLINE_NO_DOWNLOAD = bool(is_kaggle() and not ALLOW_DOWNLOAD)\n",
    "if OFFLINE_NO_DOWNLOAD:\n",
    "    os.environ.setdefault(\"HF_HUB_OFFLINE\", \"1\")\n",
    "    os.environ.setdefault(\"TRANSFORMERS_OFFLINE\", \"1\")\n",
    "    print(\"[OFFLINE] downloads disabled (Kaggle submission-safe).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3947670b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 0 — Sanidade rápida do dataset\n",
    "if not (DATA_ROOT / \"train_images\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Dataset não encontrado. Verifique DATA_ROOT: {DATA_ROOT} (esperado: .../train_images).\"\n",
    "    )\n",
    "\n",
    "train_samples = build_train_index(DATA_ROOT, strict=False)\n",
    "supp_samples = build_supplemental_index(DATA_ROOT, strict=False)\n",
    "test_samples = build_test_index(DATA_ROOT)\n",
    "\n",
    "n_auth = sum(1 for s in train_samples if s.is_authentic)\n",
    "n_forged = sum(1 for s in train_samples if s.is_authentic is False)\n",
    "\n",
    "print(\"train/authentic:\", n_auth)\n",
    "print(\"train/forged:\", n_forged)\n",
    "print(\"supplemental:\", len(supp_samples))\n",
    "print(\"test:\", len(test_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5c2da5",
   "metadata": {},
   "source": [
    "## Fase 1 — Treino do classificador (opcional)\n",
    "\n",
    "O classificador é usado como **gate** para evitar segmentar imagens\n",
    "claramente autênticas. O threshold ótimo (F1) é salvo no checkpoint e\n",
    "pode ser usado automaticamente na inferência (`cls_skip_threshold=auto`).\n",
    "\n",
    "Dica: se você **não** treinar o classificador, desative o gate na inferência\n",
    "(`--cls-skip-threshold 0.0`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ebf044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 1 — Treino do classificador (via script)\n",
    "cls_config_path = CLS_CONFIG\n",
    "if OFFLINE_NO_DOWNLOAD:\n",
    "    cls_config_path = write_config(\n",
    "        CLS_CONFIG,\n",
    "        OUTPUTS_ROOT / \"configs\" / \"cls_offline.json\",\n",
    "        {\"pretrained\": False},\n",
    "    )\n",
    "\n",
    "if RUN_TRAIN_CLS:\n",
    "    train_cls_script = REPO_ROOT / \"scripts\" / \"train_cls_cv.py\"\n",
    "    if not train_cls_script.exists():\n",
    "        raise FileNotFoundError(f\"Não encontrei {train_cls_script}.\")\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(train_cls_script),\n",
    "        \"--config\",\n",
    "        str(cls_config_path),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--output-dir\",\n",
    "        str(OUTPUTS_ROOT),\n",
    "        \"--folds\",\n",
    "        str(N_FOLDS),\n",
    "    ]\n",
    "    if FOLD >= 0:\n",
    "        cmd += [\"--fold\", str(FOLD)]\n",
    "    if CACHE_ROOT is not None:\n",
    "        cmd += [\"--cache-root\", str(CACHE_ROOT)]\n",
    "    run_cmd(cmd)\n",
    "else:\n",
    "    print(\"[CLS] RUN_TRAIN_CLS=False (pulando).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2b22e2",
   "metadata": {},
   "source": [
    "## Fase 2 — Treino do segmentador (opcional)\n",
    "\n",
    "Aqui treinamos o modelo de segmentação que gera as máscaras de fraude.\n",
    "Você pode testar diferentes arquiteturas em `configs/` e combinar tudo\n",
    "na inferência com *ensemble* e **TTA**.\n",
    "\n",
    "Observação: configs com *DINOv2/HF* exigem cache local e `local_files_only=true`\n",
    "no Kaggle (internet OFF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a902d48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 2 — Treino do segmentador (via script)\n",
    "seg_config_path = SEG_CONFIG\n",
    "if OFFLINE_NO_DOWNLOAD:\n",
    "    seg_config_path = write_config(\n",
    "        SEG_CONFIG,\n",
    "        OUTPUTS_ROOT / \"configs\" / \"seg_offline.json\",\n",
    "        {\"encoder_weights\": \"\", \"pretrained\": False},\n",
    "    )\n",
    "\n",
    "if RUN_TRAIN_SEG:\n",
    "    train_seg_script = REPO_ROOT / \"scripts\" / \"train_seg_smp_cv.py\"\n",
    "    if not train_seg_script.exists():\n",
    "        raise FileNotFoundError(f\"Não encontrei {train_seg_script}.\")\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(train_seg_script),\n",
    "        \"--config\",\n",
    "        str(seg_config_path),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--output-dir\",\n",
    "        str(OUTPUTS_ROOT),\n",
    "        \"--folds\",\n",
    "        str(N_FOLDS),\n",
    "    ]\n",
    "    if FOLD >= 0:\n",
    "        cmd += [\"--fold\", str(FOLD)]\n",
    "    if CACHE_ROOT is not None:\n",
    "        cmd += [\"--cache-root\", str(CACHE_ROOT)]\n",
    "    run_cmd(cmd)\n",
    "else:\n",
    "    print(\"[SEG] RUN_TRAIN_SEG=False (pulando).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01088294",
   "metadata": {},
   "source": [
    "## Fase 3 — Inferência + pós-processamento (TTA + morfologia)\n",
    "\n",
    "A inferência usa `scripts/submit_ensemble.py` e o arquivo\n",
    "`configs/infer_ensemble.json`, que já inclui:\n",
    "\n",
    "- **TTA**: `none`, `hflip`, `vflip`\n",
    "- **Pós-processamento morfológico**: *closing*, *opening*, *fill holes*\n",
    "- **Filtro por área/confiança** e threshold adaptativo\n",
    "\n",
    "Para ajustar, edite `configs/infer_ensemble.json` ou passe flags no CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d332d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase 3 — Gerar submission.csv\n",
    "infer_config_path = INFER_CONFIG\n",
    "if OFFLINE_NO_DOWNLOAD and env_bool(\"FORGERYSEG_DISABLE_HF_MODELS\", default=True):\n",
    "    # Remove modelos que dependem de HuggingFace (ex.: DINOv2) quando não queremos\n",
    "    # depender de cache HF no Kaggle offline.\n",
    "    try:\n",
    "        with INFER_CONFIG.open(\"r\") as f:\n",
    "            infer_cfg = json.load(f)\n",
    "        models = infer_cfg.get(\"models\", [])\n",
    "        filtered = []\n",
    "        dropped = []\n",
    "        for m in models:\n",
    "            mid = str(m.get(\"model_id\", \"\"))\n",
    "            if \"dinov2\" in mid.lower():\n",
    "                dropped.append(mid)\n",
    "                continue\n",
    "            filtered.append(m)\n",
    "        if filtered and dropped:\n",
    "            total_w = sum(float(m.get(\"weight\", 1.0)) for m in filtered) or 1.0\n",
    "            for m in filtered:\n",
    "                m[\"weight\"] = float(m.get(\"weight\", 1.0)) / total_w\n",
    "            infer_cfg[\"models\"] = filtered\n",
    "            infer_config_path = write_config(INFER_CONFIG, OUTPUTS_ROOT / \"configs\" / \"infer_offline.json\", infer_cfg)\n",
    "            print(\"[INFER] OFFLINE: removendo modelos HF:\", \", \".join(dropped))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "submission_path = Path(\"/kaggle/working/submission.csv\") if is_kaggle() else OUTPUTS_ROOT / \"submission.csv\"\n",
    "models_seg_dir = OUTPUTS_ROOT / \"models_seg\"\n",
    "models_cls_dir = OUTPUTS_ROOT / \"models_cls\"\n",
    "\n",
    "# Recalcula presença de checkpoints após treino\n",
    "has_cls_now = has_any_ckpt(\"models_cls\", \"fold_*/best.pt\", OUTPUTS_ROOT)\n",
    "\n",
    "if RUN_SUBMISSION:\n",
    "    submit_script = REPO_ROOT / \"scripts\" / \"submit_ensemble.py\"\n",
    "    if not submit_script.exists():\n",
    "        raise FileNotFoundError(f\"Não encontrei {submit_script}.\")\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(submit_script),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--out-csv\",\n",
    "        str(submission_path),\n",
    "        \"--config\",\n",
    "        str(infer_config_path),\n",
    "    ]\n",
    "    if models_seg_dir.exists():\n",
    "        cmd += [\"--models-dir\", str(models_seg_dir)]\n",
    "    if models_cls_dir.exists():\n",
    "        cmd += [\"--cls-models-dir\", str(models_cls_dir)]\n",
    "    if not has_cls_now:\n",
    "        cmd += [\"--cls-skip-threshold\", \"0.0\"]\n",
    "        print(\"[CLS] sem checkpoints -> gate desativado (cls_skip_threshold=0.0).\")\n",
    "\n",
    "    run_cmd(cmd)\n",
    "    print(\"submission.csv ->\", submission_path)\n",
    "else:\n",
    "    print(\"[SUBMISSION] RUN_SUBMISSION=False (pulando).\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "language_info"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
