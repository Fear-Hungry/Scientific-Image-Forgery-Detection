{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recod.ai/LUC — Submission (Kaggle)\n",
        "\n",
        "Gera `submission.csv` no formato da competição a partir de 1+ configs em `configs/*.json`,\n",
        "com suporte a:\n",
        "\n",
        "- Segmentação (DINOv2) + pós-processamento + TTA (via config)\n",
        "- `fft_gate` (opcional) para revisar casos `authentic`\n",
        "- `dinov2_freq_fusion` (opcional) via `model_type`\n",
        "- Ensemble de múltiplas submissões (opcional)\n",
        "\n",
        "**No Kaggle**:\n",
        "1. Anexe o dataset da competição.\n",
        "2. (Opcional) Anexe um dataset com seus checkpoints em `outputs/models/*.pth`.\n",
        "3. Rode todas as células; o arquivo final fica em `/kaggle/working/submission.csv`.\n",
        "\n",
        "Observação: por regra do repo, a lógica nasce aqui (`.py`) e é espelhada no `.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import platform\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "print(f\"python={sys.version.split()[0]} platform={platform.platform()}\")\n",
        "print(f\"torch={torch.__version__}\")\n",
        "try:\n",
        "    import torchvision  # type: ignore\n",
        "\n",
        "    print(f\"torchvision={torchvision.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] torchvision not available ({type(e).__name__}: {e})\")\n",
        "\n",
        "try:\n",
        "    import timm  # type: ignore\n",
        "\n",
        "    print(f\"timm={timm.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] timm not available ({type(e).__name__}: {e})\")\n",
        "\n",
        "try:\n",
        "    import cv2  # type: ignore\n",
        "\n",
        "    print(f\"opencv={cv2.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] opencv not available ({type(e).__name__}: {e})\")\n",
        "\n",
        "print(f\"numpy={np.__version__} pandas={pd.__version__}\")\n",
        "\n",
        "def _find_code_root() -> Path:\n",
        "    cwd = Path.cwd()\n",
        "    for p in [cwd, *cwd.parents]:\n",
        "        if (p / \"src\" / \"forgeryseg\").exists():\n",
        "            return p\n",
        "\n",
        "    kaggle_input = Path(\"/kaggle/input\")\n",
        "    if kaggle_input.exists():\n",
        "        for d in kaggle_input.iterdir():\n",
        "            if not d.is_dir():\n",
        "                continue\n",
        "            if (d / \"src\" / \"forgeryseg\").exists():\n",
        "                return d\n",
        "            # common: dataset root contains a single folder with the repo inside\n",
        "            try:\n",
        "                for child in d.iterdir():\n",
        "                    if child.is_dir() and (child / \"src\" / \"forgeryseg\").exists():\n",
        "                        return child\n",
        "            except PermissionError:\n",
        "                continue\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Não encontrei o código (src/forgeryseg). \"\n",
        "        \"No Kaggle: anexe um Dataset contendo este repo (com pastas src/ e configs/).\"\n",
        "    )\n",
        "\n",
        "\n",
        "CODE_ROOT = _find_code_root()\n",
        "SRC = CODE_ROOT / \"src\"\n",
        "CONFIG_ROOT = CODE_ROOT / \"configs\"\n",
        "print(f\"code_root={CODE_ROOT}\")\n",
        "\n",
        "if str(SRC) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC))\n",
        "\n",
        "from forgeryseg.ensemble import ensemble_annotations, rank_weights_by_score\n",
        "from forgeryseg.submission import list_ordered_cases, write_submission_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Config (edite aqui)\n",
        "# -------------------------\n",
        "\n",
        "DATA_ROOT: Path | None = None  # None => auto-detect (Kaggle -> local)\n",
        "SPLIT = \"test\"  # \"test\" no Kaggle (train/supplemental só para debug)\n",
        "LIMIT = 0  # 0 = sem limite\n",
        "SKIP_MISSING_CONFIGS = True  # se faltar config/ckpt, pula ao invés de quebrar\n",
        "\n",
        "# 1+ configs para gerar submissões individuais\n",
        "CONFIG_PATHS = [\n",
        "    CONFIG_ROOT / \"dino_v3_518_r69_fft_gate.json\",\n",
        "    # CONFIG_ROOT / \"dino_v2_518_basev1.json\",\n",
        "    # CONFIG_ROOT / \"dino_v1_718_u52.json\",\n",
        "    # CONFIG_ROOT / \"dino_v3_518_r69_freq_fusion.json\",\n",
        "]\n",
        "\n",
        "# ensemble (opcional) se CONFIG_PATHS tiver 2+\n",
        "DO_ENSEMBLE = len(CONFIG_PATHS) > 1\n",
        "ENSEMBLE_METHOD = \"weighted\"  # weighted | majority | union | intersection\n",
        "ENSEMBLE_THRESHOLD = 0.5  # só para method=\"weighted\"\n",
        "\n",
        "# Se quiser pesos fixos, preencha WEIGHTS (mesmo tamanho de CONFIG_PATHS).\n",
        "# Caso contrário, se SCORES for fornecido, os pesos vêm de rank_weights_by_score(scores).\n",
        "WEIGHTS: list[float] | None = None\n",
        "SCORES: list[float] | None = None\n",
        "\n",
        "OUT_DIR = Path(\"/kaggle/working\") if Path(\"/kaggle/working\").exists() else Path(\"outputs\")\n",
        "FINAL_OUT = OUT_DIR / \"submission.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _find_recodai_root() -> Path:\n",
        "    if DATA_ROOT is not None:\n",
        "        return Path(DATA_ROOT)\n",
        "\n",
        "    kaggle_input = Path(\"/kaggle/input\")\n",
        "    if kaggle_input.exists():\n",
        "        for d in kaggle_input.iterdir():\n",
        "            if not d.is_dir():\n",
        "                continue\n",
        "            if (d / \"recodai\" / \"sample_submission.csv\").exists():\n",
        "                return d / \"recodai\"\n",
        "            if (d / \"sample_submission.csv\").exists() and (d / \"test_images\").exists():\n",
        "                return d\n",
        "\n",
        "    local = Path(\"data/recodai\")\n",
        "    if local.exists():\n",
        "        return local\n",
        "    local2 = CODE_ROOT / \"data\" / \"recodai\"\n",
        "    if local2.exists():\n",
        "        return local2\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Não encontrei o data root. Defina DATA_ROOT manualmente \"\n",
        "        \"(ex.: /kaggle/input/<dataset>/recodai ou data/recodai).\"\n",
        "    )\n",
        "\n",
        "\n",
        "def ensemble_submissions_from_csvs(\n",
        "    *,\n",
        "    sub_paths: list[Path],\n",
        "    data_root: Path,\n",
        "    split: str,\n",
        "    out_path: Path,\n",
        "    method: str = \"weighted\",\n",
        "    weights: list[float] | None = None,\n",
        "    scores: list[float] | None = None,\n",
        "    threshold: float = 0.5,\n",
        ") -> None:\n",
        "    tables: list[dict[str, str]] = []\n",
        "    for p in sub_paths:\n",
        "        df = pd.read_csv(p)\n",
        "        if \"case_id\" not in df.columns or \"annotation\" not in df.columns:\n",
        "            raise ValueError(f\"{p} precisa ter colunas case_id,annotation\")\n",
        "        tables.append(dict(zip(df[\"case_id\"].astype(str), df[\"annotation\"], strict=True)))\n",
        "\n",
        "    if method == \"weighted\":\n",
        "        if weights is None:\n",
        "            if scores is None:\n",
        "                weights = [1.0 / len(sub_paths)] * len(sub_paths)\n",
        "            else:\n",
        "                weights = rank_weights_by_score(scores)\n",
        "        if len(weights) != len(sub_paths):\n",
        "            raise ValueError(\"weights precisa ter o mesmo tamanho de sub_paths\")\n",
        "        print(f\"ensemble weights={weights}\")\n",
        "\n",
        "    cases = list_ordered_cases(data_root, split)  # type: ignore[arg-type]\n",
        "\n",
        "    import cv2\n",
        "\n",
        "    rows: list[dict[str, str]] = []\n",
        "    for case in tqdm(cases, desc=\"Ensemble\"):\n",
        "        h, w = cv2.imread(str(case.image_path), cv2.IMREAD_UNCHANGED).shape[:2]\n",
        "        anns = [t.get(case.case_id, \"authentic\") for t in tables]\n",
        "        ann_out = ensemble_annotations(\n",
        "            anns,\n",
        "            shape=(h, w),\n",
        "            method=method,  # type: ignore[arg-type]\n",
        "            weights=weights if method == \"weighted\" else None,\n",
        "            threshold=float(threshold),\n",
        "        )\n",
        "        rows.append({\"case_id\": case.case_id, \"annotation\": ann_out})\n",
        "\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    out_df = pd.DataFrame(rows)\n",
        "    out_df.to_csv(out_path, index=False)\n",
        "    n_auth = int((out_df[\"annotation\"] == \"authentic\").sum())\n",
        "    print(f\"Wrote {out_path} ({n_auth}/{len(out_df)} authentic)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Run\n",
        "# -------------------------\n",
        "\n",
        "data_root = _find_recodai_root()\n",
        "print(f\"data_root={data_root}\")\n",
        "print(f\"configs={[p.as_posix() for p in CONFIG_PATHS]}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device={device}\")\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception:\n",
        "    pass\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "sub_paths: list[Path] = []\n",
        "for cfg_path in CONFIG_PATHS:\n",
        "    if not cfg_path.exists():\n",
        "        msg = f\"[warn] Config não encontrado: {cfg_path}\"\n",
        "        if SKIP_MISSING_CONFIGS:\n",
        "            print(msg)\n",
        "            continue\n",
        "        raise FileNotFoundError(msg)\n",
        "\n",
        "    cfg = json.loads(cfg_path.read_text())\n",
        "    name = str(cfg.get(\"name\", cfg_path.stem))\n",
        "    out_path = OUT_DIR / f\"submission_{name}.csv\"\n",
        "    try:\n",
        "        write_submission_csv(\n",
        "            config_path=cfg_path,\n",
        "            data_root=data_root,\n",
        "            split=SPLIT,  # type: ignore[arg-type]\n",
        "            out_path=out_path,\n",
        "            device=device,\n",
        "            limit=LIMIT,\n",
        "            path_roots=[Path.cwd(), CODE_ROOT],\n",
        "        )\n",
        "        sub_paths.append(out_path)\n",
        "    except FileNotFoundError as e:\n",
        "        if SKIP_MISSING_CONFIGS:\n",
        "            print(f\"[warn] {e} (pulando {cfg_path.name})\")\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "if not sub_paths:\n",
        "    raise RuntimeError(\"Nenhuma submissão foi gerada (verifique configs/checkpoints).\")\n",
        "\n",
        "if DO_ENSEMBLE and len(sub_paths) > 1:\n",
        "    ensemble_submissions_from_csvs(\n",
        "        sub_paths=sub_paths,\n",
        "        data_root=data_root,\n",
        "        split=SPLIT,\n",
        "        out_path=FINAL_OUT,\n",
        "        method=str(ENSEMBLE_METHOD),\n",
        "        weights=WEIGHTS,\n",
        "        scores=SCORES,\n",
        "        threshold=float(ENSEMBLE_THRESHOLD),\n",
        "    )\n",
        "else:\n",
        "    # 1 config => apenas renomeia como submission.csv\n",
        "    FINAL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if sub_paths:\n",
        "        Path(sub_paths[0]).replace(FINAL_OUT)\n",
        "    print(f\"Wrote {FINAL_OUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Score / Sanity-check do submission.csv\n",
        "# -------------------------\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from forgeryseg.dataset import list_cases, load_mask_instances\n",
        "from forgeryseg.metric import of1_score\n",
        "from forgeryseg.rle import annotation_to_masks\n",
        "\n",
        "try:\n",
        "    from PIL import Image\n",
        "except Exception:\n",
        "    Image = None\n",
        "\n",
        "\n",
        "# >>> AJUSTE AQUI <<<\n",
        "EVAL_CSV = FINAL_OUT  # por padrão usa o submission final gerado\n",
        "EVAL_SPLIT = \"test\"  # \"train\" ou \"supplemental\" para calcular score; \"test\" só valida formato\n",
        "# --------------------\n",
        "\n",
        "\n",
        "def _is_authentic(ann) -> bool:\n",
        "    if ann is None:\n",
        "        return True\n",
        "    if isinstance(ann, float) and np.isnan(ann):\n",
        "        return True\n",
        "    s = str(ann).strip().lower()\n",
        "    return (s == \"\") or (s == \"authentic\")\n",
        "\n",
        "\n",
        "def validate_submission_format(csv_path: Path, *, data_root: Path, split: str) -> dict:\n",
        "    \"\"\"\n",
        "    Valida:\n",
        "      - CSV tem colunas case_id, annotation\n",
        "      - case_id existe no split\n",
        "      - annotations decodificam sem erro (usando shape da imagem)\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if \"case_id\" not in df.columns or \"annotation\" not in df.columns:\n",
        "        raise ValueError(f\"{csv_path} precisa ter colunas: case_id, annotation\")\n",
        "\n",
        "    df[\"case_id\"] = df[\"case_id\"].astype(str)\n",
        "    if df[\"case_id\"].duplicated().any():\n",
        "        dup = df.loc[df[\"case_id\"].duplicated(), \"case_id\"].iloc[:5].tolist()\n",
        "        raise ValueError(f\"{csv_path} tem case_id duplicado (ex.: {dup})\")\n",
        "\n",
        "    pred = dict(zip(df[\"case_id\"], df[\"annotation\"], strict=True))\n",
        "\n",
        "    cases = list_cases(data_root, split, include_authentic=True, include_forged=True)\n",
        "    case_by_id = {c.case_id: c for c in cases}\n",
        "\n",
        "    if split == \"test\":\n",
        "        expected_ids = pd.read_csv(data_root / \"sample_submission.csv\")[\"case_id\"].astype(str).tolist()\n",
        "    else:\n",
        "        expected_ids = list(case_by_id.keys())\n",
        "\n",
        "    missing_in_csv = [cid for cid in expected_ids if cid not in pred]\n",
        "    extra_in_csv = [cid for cid in pred.keys() if cid not in case_by_id]\n",
        "\n",
        "    decode_errors = []\n",
        "    decoded_non_empty = 0\n",
        "\n",
        "    # validar decodificação usando H/W da imagem\n",
        "    if Image is None:\n",
        "        print(\"[warn] PIL não disponível; pulando validação de decode por shape da imagem.\")\n",
        "    else:\n",
        "        for cid, ann in tqdm(pred.items(), desc=\"Validating RLE\"):\n",
        "            if cid not in case_by_id:\n",
        "                continue\n",
        "            if _is_authentic(ann):\n",
        "                continue\n",
        "\n",
        "            case = case_by_id[cid]\n",
        "            try:\n",
        "                with Image.open(case.image_path) as im:\n",
        "                    w, h = im.size\n",
        "                masks = annotation_to_masks(ann, (h, w))\n",
        "                if len(masks) > 0 and any(np.any(m) for m in masks):\n",
        "                    decoded_non_empty += 1\n",
        "            except Exception as e:\n",
        "                decode_errors.append((cid, str(e)))\n",
        "\n",
        "    return {\n",
        "        \"csv_path\": str(csv_path),\n",
        "        \"split\": split,\n",
        "        \"n_cases_in_split\": len(cases),\n",
        "        \"n_rows_in_csv\": len(df),\n",
        "        \"missing_case_ids_in_csv\": len(missing_in_csv),\n",
        "        \"extra_case_ids_in_csv\": len(extra_in_csv),\n",
        "        \"n_decode_errors\": len(decode_errors),\n",
        "        \"n_non_empty_decoded\": decoded_non_empty,\n",
        "        \"sample_missing_ids\": missing_in_csv[:5],\n",
        "        \"sample_extra_ids\": extra_in_csv[:5],\n",
        "        \"sample_decode_errors\": decode_errors[:5],\n",
        "    }\n",
        "\n",
        "\n",
        "def score_submission(csv_path: Path, *, data_root: Path, split: str) -> dict:\n",
        "    \"\"\"\n",
        "    Score local no estilo do sanity_submissions.py:\n",
        "      - autêntica: 1 se pred \"authentic\", senão 0\n",
        "      - forjada: 0 se pred \"authentic\", senão oF1(pred_masks, gt_masks)\n",
        "    \"\"\"\n",
        "    df = pd.read_csv(csv_path)\n",
        "    if \"case_id\" not in df.columns or \"annotation\" not in df.columns:\n",
        "        raise ValueError(f\"{csv_path} precisa ter colunas: case_id, annotation\")\n",
        "    df[\"case_id\"] = df[\"case_id\"].astype(str)\n",
        "\n",
        "    pred = dict(zip(df[\"case_id\"], df[\"annotation\"], strict=True))\n",
        "\n",
        "    cases = list_cases(data_root, split, include_authentic=True, include_forged=True)\n",
        "\n",
        "    scores_all = []\n",
        "    scores_auth = []\n",
        "    scores_forg = []\n",
        "    n_auth_pred_as_forged = 0\n",
        "    n_forg_pred_as_auth = 0\n",
        "    decode_errors = 0\n",
        "\n",
        "    for case in tqdm(cases, desc=f\"Scoring {split}\"):\n",
        "        ann = pred.get(case.case_id, \"authentic\")\n",
        "\n",
        "        # Caso autêntico (sem máscara GT)\n",
        "        if case.mask_path is None:\n",
        "            s = 1.0 if _is_authentic(ann) else 0.0\n",
        "            if s == 0.0:\n",
        "                n_auth_pred_as_forged += 1\n",
        "            scores_all.append(s)\n",
        "            scores_auth.append(s)\n",
        "            continue\n",
        "\n",
        "        # Caso forjado (com GT)\n",
        "        if _is_authentic(ann):\n",
        "            n_forg_pred_as_auth += 1\n",
        "            s = 0.0\n",
        "            scores_all.append(s)\n",
        "            scores_forg.append(s)\n",
        "            continue\n",
        "\n",
        "        gt_masks = load_mask_instances(case.mask_path)\n",
        "        h, w = gt_masks[0].shape\n",
        "\n",
        "        try:\n",
        "            pred_masks = annotation_to_masks(ann, (h, w))\n",
        "            s = of1_score(pred_masks, gt_masks)\n",
        "        except ImportError:\n",
        "            raise\n",
        "        except Exception:\n",
        "            decode_errors += 1\n",
        "            s = 0.0\n",
        "\n",
        "        scores_all.append(float(s))\n",
        "        scores_forg.append(float(s))\n",
        "\n",
        "    def _mean(x):\n",
        "        return float(np.mean(x)) if len(x) else 0.0\n",
        "\n",
        "    return {\n",
        "        \"csv_path\": str(csv_path),\n",
        "        \"split\": split,\n",
        "        \"mean_score\": _mean(scores_all),\n",
        "        \"mean_authentic\": _mean(scores_auth),\n",
        "        \"mean_forged\": _mean(scores_forg),\n",
        "        \"n_cases\": len(scores_all),\n",
        "        \"n_authentic\": len(scores_auth),\n",
        "        \"n_forged\": len(scores_forg),\n",
        "        \"auth_pred_as_forged\": int(n_auth_pred_as_forged),\n",
        "        \"forg_pred_as_auth\": int(n_forg_pred_as_auth),\n",
        "        \"decode_errors_scoring\": int(decode_errors),\n",
        "    }\n",
        "\n",
        "\n",
        "# --------- RUN ---------\n",
        "EVAL_CSV = Path(EVAL_CSV)\n",
        "print(\"data_root =\", data_root)\n",
        "print(\"EVAL_CSV  =\", EVAL_CSV)\n",
        "print(\"EVAL_SPLIT=\", EVAL_SPLIT)\n",
        "\n",
        "fmt = validate_submission_format(EVAL_CSV, data_root=data_root, split=EVAL_SPLIT)\n",
        "print(\"\\n[Format check]\")\n",
        "print(json.dumps(fmt, indent=2, ensure_ascii=False))\n",
        "\n",
        "if EVAL_SPLIT in (\"train\", \"supplemental\"):\n",
        "    print(\"\\n[Local score]\")\n",
        "    try:\n",
        "        res = score_submission(EVAL_CSV, data_root=data_root, split=EVAL_SPLIT)\n",
        "        print(json.dumps(res, indent=2, ensure_ascii=False))\n",
        "    except ImportError as e:\n",
        "        print(\"\\n[ERRO] Para calcular o oF1, precisa de SciPy (Hungarian matching).\")\n",
        "        print(\"Detalhe:\", e)\n",
        "else:\n",
        "    print(\"\\nSplit=test => não há ground truth; não dá para calcular score real aqui.\")\n",
        "    print(\"Use train/supplemental para score local ou apenas confie no Format check acima.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
