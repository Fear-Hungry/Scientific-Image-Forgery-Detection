{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recod.ai/LUC — Submission (Kaggle)\n",
        "\n",
        "Gera `submission.csv` no formato da competição a partir de 1+ configs em `configs/*.json`,\n",
        "com suporte a:\n",
        "\n",
        "- Segmentação (DINOv2) + pós-processamento + TTA (via config)\n",
        "- `fft_gate` (opcional) para revisar casos `authentic`\n",
        "- `dinov2_freq_fusion` (opcional) via `model_type`\n",
        "- Ensemble de múltiplas submissões (opcional)\n",
        "\n",
        "**No Kaggle**:\n",
        "1. Anexe o dataset da competição.\n",
        "2. (Opcional) Anexe um dataset com seus checkpoints em `outputs/models/*.pth`.\n",
        "3. Rode todas as células; o arquivo final fica em `/kaggle/working/submission.csv`.\n",
        "\n",
        "Observação: por regra do repo, a lógica nasce aqui (`.py`) e é espelhada no `.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05fa9fbc",
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import platform\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "print(f\"python={sys.version.split()[0]} platform={platform.platform()}\")\n",
        "print(f\"torch={torch.__version__}\")\n",
        "try:\n",
        "    import torchvision  # type: ignore\n",
        "\n",
        "    print(f\"torchvision={torchvision.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] torchvision not available ({type(e).__name__}: {e})\")\n",
        "\n",
        "try:\n",
        "    import timm  # type: ignore\n",
        "\n",
        "    print(f\"timm={timm.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] timm not available ({type(e).__name__}: {e})\")\n",
        "\n",
        "try:\n",
        "    import cv2  # type: ignore\n",
        "\n",
        "    print(f\"opencv={cv2.__version__}\")\n",
        "except Exception as e:\n",
        "    print(f\"[warn] opencv not available ({type(e).__name__}: {e})\")\n",
        "\n",
        "print(f\"numpy={np.__version__} pandas={pd.__version__}\")\n",
        "\n",
        "\n",
        "def _find_code_root() -> Path:\n",
        "    cwd = Path.cwd()\n",
        "    for p in [cwd, *cwd.parents]:\n",
        "        if (p / \"src\" / \"forgeryseg\").exists():\n",
        "            return p\n",
        "\n",
        "    kaggle_input = Path(\"/kaggle/input\")\n",
        "    if kaggle_input.exists():\n",
        "        for d in kaggle_input.iterdir():\n",
        "            if not d.is_dir():\n",
        "                continue\n",
        "            if (d / \"src\" / \"forgeryseg\").exists():\n",
        "                return d\n",
        "            # common: dataset root contains a single folder with the repo inside\n",
        "            try:\n",
        "                for child in d.iterdir():\n",
        "                    if child.is_dir() and (child / \"src\" / \"forgeryseg\").exists():\n",
        "                        return child\n",
        "            except PermissionError:\n",
        "                continue\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Não encontrei o código (src/forgeryseg). \"\n",
        "        \"No Kaggle: anexe um Dataset contendo este repo (com pastas src/ e configs/).\"\n",
        "    )\n",
        "\n",
        "\n",
        "CODE_ROOT = _find_code_root()\n",
        "SRC = CODE_ROOT / \"src\"\n",
        "CONFIG_ROOT = CODE_ROOT / \"configs\"\n",
        "print(f\"code_root={CODE_ROOT}\")\n",
        "\n",
        "if str(SRC) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC))\n",
        "\n",
        "from forgeryseg.ensemble_io import ensemble_submissions_from_csvs\n",
        "from forgeryseg.eval import score_submission_csv, validate_submission_format\n",
        "from forgeryseg.submission import write_submission_csv\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f68b8ba9",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Config (edite aqui)\n",
        "# -------------------------\n",
        "\n",
        "DATA_ROOT: Path | None = None  # None => auto-detect (Kaggle -> local)\n",
        "SPLIT = \"test\"  # \"test\" no Kaggle (train/supplemental só para debug)\n",
        "LIMIT = 0  # 0 = sem limite\n",
        "SKIP_MISSING_CONFIGS = True  # se faltar config/ckpt, pula ao invés de quebrar\n",
        "\n",
        "# 1+ configs para gerar submissões individuais\n",
        "BASE_CONFIG_PATHS = [\n",
        "    # Base + TTA forte (bom custo/benefício)\n",
        "    CONFIG_ROOT / \"dino_v3_518_r69_fft_gate_tta_plus.json\",\n",
        "    # Multi-escala (melhor para regiões pequenas + grandes)\n",
        "    CONFIG_ROOT / \"dino_v4_518_r69_multiscale_fft_gate_tta_plus.json\",\n",
        "    # Fusão espacial+frequência (diversidade)\n",
        "    CONFIG_ROOT / \"dino_v3_518_r69_freq_fusion_fft_gate_tta_plus.json\",\n",
        "]\n",
        "\n",
        "# Se existir `configs/tuned_<stem>_optuna_<objective>.json` (vindo do notebook de treino),\n",
        "# troca automaticamente pelo tuned (melhor score no val subset).\n",
        "AUTO_USE_TUNED = True\n",
        "TUNED_OBJECTIVE = \"combo\"  # mean_score | mean_forged | combo\n",
        "\n",
        "# ensemble (opcional) se CONFIG_PATHS tiver 2+\n",
        "DO_ENSEMBLE = len(BASE_CONFIG_PATHS) > 1\n",
        "ENSEMBLE_METHOD = \"weighted\"  # weighted | majority | union | intersection\n",
        "ENSEMBLE_THRESHOLD = 0.5  # só para method=\"weighted\"\n",
        "\n",
        "# Se quiser pesos fixos, preencha WEIGHTS (mesmo tamanho de CONFIG_PATHS).\n",
        "# Caso contrário, se SCORES for fornecido, os pesos são derivados automaticamente (melhor score => maior peso).\n",
        "WEIGHTS: list[float] | None = None\n",
        "SCORES: list[float] | None = None\n",
        "AUTO_SCORES_FROM_CKPT = True  # usa `val_of1` do checkpoint para derivar pesos (se disponível)\n",
        "\n",
        "OUT_DIR = Path(\"/kaggle/working\") if Path(\"/kaggle/working\").exists() else Path(\"outputs\")\n",
        "FINAL_OUT = OUT_DIR / \"submission.csv\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24fa6464",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _find_recodai_root() -> Path:\n",
        "    if DATA_ROOT is not None:\n",
        "        return Path(DATA_ROOT)\n",
        "\n",
        "    kaggle_input = Path(\"/kaggle/input\")\n",
        "    if kaggle_input.exists():\n",
        "        for d in kaggle_input.iterdir():\n",
        "            if not d.is_dir():\n",
        "                continue\n",
        "            if (d / \"recodai\" / \"sample_submission.csv\").exists():\n",
        "                return d / \"recodai\"\n",
        "            if (d / \"sample_submission.csv\").exists() and (d / \"test_images\").exists():\n",
        "                return d\n",
        "\n",
        "    local = Path(\"data/recodai\")\n",
        "    if local.exists():\n",
        "        return local\n",
        "    local2 = CODE_ROOT / \"data\" / \"recodai\"\n",
        "    if local2.exists():\n",
        "        return local2\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Não encontrei o data root. Defina DATA_ROOT manualmente \"\n",
        "        \"(ex.: /kaggle/input/<dataset>/recodai ou data/recodai).\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42290209",
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Run\n",
        "# -------------------------\n",
        "\n",
        "data_root = _find_recodai_root()\n",
        "print(f\"data_root={data_root}\")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"device={device}\")\n",
        "try:\n",
        "    torch.set_float32_matmul_precision(\"high\")\n",
        "except Exception:\n",
        "    pass\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "from forgeryseg.paths import resolve_existing_path\n",
        "\n",
        "\n",
        "def _resolve_config_path(p: Path) -> Path:\n",
        "    if p.exists():\n",
        "        return p\n",
        "    rel = Path(\"configs\") / p.name\n",
        "    return resolve_existing_path(rel, roots=[CODE_ROOT], search_kaggle_input=True)\n",
        "\n",
        "\n",
        "def _maybe_use_tuned(base: Path) -> Path:\n",
        "    base = _resolve_config_path(base)\n",
        "    if not bool(AUTO_USE_TUNED):\n",
        "        return base\n",
        "    tuned_name = f\"tuned_{base.stem}_optuna_{TUNED_OBJECTIVE}.json\"\n",
        "    tuned_rel = Path(\"configs\") / tuned_name\n",
        "    tuned = resolve_existing_path(tuned_rel, roots=[CODE_ROOT], search_kaggle_input=True)\n",
        "    return tuned if tuned.exists() else base\n",
        "\n",
        "\n",
        "CONFIG_PATHS = [_maybe_use_tuned(p) for p in BASE_CONFIG_PATHS]\n",
        "print(f\"configs={[p.as_posix() for p in CONFIG_PATHS]}\")\n",
        "\n",
        "sub_paths: list[Path] = []\n",
        "sub_scores: list[float] = []\n",
        "for cfg_path in CONFIG_PATHS:\n",
        "    if not cfg_path.exists():\n",
        "        msg = f\"[warn] Config não encontrado: {cfg_path}\"\n",
        "        if SKIP_MISSING_CONFIGS:\n",
        "            print(msg)\n",
        "            continue\n",
        "        raise FileNotFoundError(msg)\n",
        "\n",
        "    cfg = json.loads(cfg_path.read_text())\n",
        "    name = str(cfg.get(\"name\", cfg_path.stem))\n",
        "    out_path = OUT_DIR / f\"submission_{name}.csv\"\n",
        "    try:\n",
        "        write_submission_csv(\n",
        "            config_path=cfg_path,\n",
        "            data_root=data_root,\n",
        "            split=SPLIT,  # type: ignore[arg-type]\n",
        "            out_path=out_path,\n",
        "            device=device,\n",
        "            limit=LIMIT,\n",
        "            path_roots=[OUT_DIR, Path.cwd(), CODE_ROOT, CONFIG_ROOT],\n",
        "            amp=True,\n",
        "        )\n",
        "        sub_paths.append(out_path)\n",
        "\n",
        "        ckpt = None\n",
        "        try:\n",
        "            ckpt_rel = cfg.get(\"model\", {}).get(\"checkpoint\")\n",
        "            if isinstance(ckpt_rel, str) and ckpt_rel.strip():\n",
        "                ckpt = resolve_existing_path(ckpt_rel, roots=[OUT_DIR, CODE_ROOT], search_kaggle_input=True)\n",
        "        except Exception:\n",
        "            ckpt = None\n",
        "\n",
        "        if ckpt is not None and ckpt.exists():\n",
        "            try:\n",
        "                d = torch.load(ckpt, map_location=\"cpu\")\n",
        "                v = d.get(\"val_of1\", None)\n",
        "                if isinstance(v, (int, float)):\n",
        "                    sub_scores.append(float(v))\n",
        "                else:\n",
        "                    sub_scores.append(float(\"nan\"))\n",
        "            except Exception:\n",
        "                sub_scores.append(float(\"nan\"))\n",
        "        else:\n",
        "            sub_scores.append(float(\"nan\"))\n",
        "    except FileNotFoundError as e:\n",
        "        if SKIP_MISSING_CONFIGS:\n",
        "            print(f\"[warn] {e} (pulando {cfg_path.name})\")\n",
        "            continue\n",
        "        raise\n",
        "\n",
        "if not sub_paths:\n",
        "    raise RuntimeError(\"Nenhuma submissão foi gerada (verifique configs/checkpoints).\")\n",
        "\n",
        "if DO_ENSEMBLE and len(sub_paths) > 1:\n",
        "    if (\n",
        "        SCORES is None\n",
        "        and WEIGHTS is None\n",
        "        and AUTO_SCORES_FROM_CKPT\n",
        "        and len(sub_scores) == len(sub_paths)\n",
        "        and all(not (s != s) for s in sub_scores)  # not NaN\n",
        "    ):\n",
        "        SCORES = list(sub_scores)\n",
        "        print(f\"[info] Using SCORES from checkpoints: {SCORES}\")\n",
        "\n",
        "    ensemble_submissions_from_csvs(\n",
        "        sub_paths=sub_paths,\n",
        "        data_root=data_root,\n",
        "        split=SPLIT,\n",
        "        out_path=FINAL_OUT,\n",
        "        method=str(ENSEMBLE_METHOD),\n",
        "        weights=WEIGHTS,\n",
        "        scores=SCORES,\n",
        "        threshold=float(ENSEMBLE_THRESHOLD),\n",
        "    )\n",
        "else:\n",
        "    # 1 config => apenas renomeia como submission.csv\n",
        "    FINAL_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "    if sub_paths:\n",
        "        Path(sub_paths[0]).replace(FINAL_OUT)\n",
        "    print(f\"Wrote {FINAL_OUT}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Score / Sanity-check do submission.csv\n",
        "# -------------------------\n",
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "# >>> AJUSTE AQUI <<<\n",
        "EVAL_CSV = FINAL_OUT  # por padrão usa o submission final gerado\n",
        "EVAL_SPLIT = \"test\"  # \"train\" ou \"supplemental\" para calcular score; \"test\" só valida formato\n",
        "# --------------------\n",
        "\n",
        "eval_csv = Path(EVAL_CSV)\n",
        "print(\"data_root =\", data_root)\n",
        "print(\"EVAL_CSV  =\", eval_csv)\n",
        "print(\"EVAL_SPLIT=\", EVAL_SPLIT)\n",
        "\n",
        "fmt = validate_submission_format(eval_csv, data_root=data_root, split=EVAL_SPLIT)  # type: ignore[arg-type]\n",
        "print(\"\\n[Format check]\")\n",
        "print(json.dumps(fmt, indent=2, ensure_ascii=False))\n",
        "\n",
        "if EVAL_SPLIT in (\"train\", \"supplemental\"):\n",
        "    print(\"\\n[Local score]\")\n",
        "    try:\n",
        "        res = score_submission_csv(eval_csv, data_root=data_root, split=EVAL_SPLIT)  # type: ignore[arg-type]\n",
        "        print(json.dumps(res.as_dict(csv_path=eval_csv, split=EVAL_SPLIT), indent=2, ensure_ascii=False))\n",
        "    except ImportError as e:\n",
        "        print(\"\\n[ERRO] Para calcular o oF1, precisa de SciPy (Hungarian matching).\")\n",
        "        print(\"Detalhe:\", e)\n",
        "else:\n",
        "    print(\"\\nSplit=test => não há ground truth; não dá para calcular score real aqui.\")\n",
        "    print(\"Use train/supplemental para score local ou apenas confie no Format check acima.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
