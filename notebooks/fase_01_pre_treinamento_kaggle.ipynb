{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "526080e2",
   "metadata": {},
   "source": [
    "# Fase 01 — Pré-treinamento (Kaggle)\n",
    "\n",
    "Objetivo: **treinar (ou fine-tunar)** os modelos (principalmente **segmentação**),\n",
    "salvar os checkpoints em `/kaggle/working/outputs` e gerar um `.zip` para você\n",
    "**baixar** e anexar depois no notebook de submissão (internet OFF).\n",
    "\n",
    "Recomendação (workflow Kaggle):\n",
    "\n",
    "1) Rode este notebook com **Internet ON** (**obrigatório**) para treinar/baixar pesos pretrained.\n",
    "2) Baixe `outputs_pretrain.zip` gerado em `/kaggle/working/`.\n",
    "3) Crie um **Kaggle Dataset** contendo a pasta `outputs/` (com `models_seg/` e `models_cls/`).\n",
    "4) No notebook de submissão (internet OFF), anexe esse Dataset (ele será detectado automaticamente).\n",
    "\n",
    "Variáveis de ambiente úteis (opcionais):\n",
    "\n",
    "- `FORGERYSEG_DATA_ROOT`: path do dataset (`.../recodai`).\n",
    "- `FORGERYSEG_REPO_ROOT`: path do repo (ex.: `/kaggle/input/<ds>/recodai_bundle`).\n",
    "- `FORGERYSEG_N_FOLDS`: default `5`.\n",
    "- `FORGERYSEG_FOLD`: `0` (um fold) ou `-1` (todos).\n",
    "- `FORGERYSEG_PROFILE`: `quick|sweep|full` (controla budget/épocas padrão).\n",
    "- `FORGERYSEG_SEG_LEVEL`: `base|plus|max` (quantidade de variações de segmentação).\n",
    "- `FORGERYSEG_SEG_FILTER`: filtra nomes (ex.: `convnext,segformer`).\n",
    "- `FORGERYSEG_CLS_ACTIVE`: escolhe 1 classificador (evita sobrescrita em `models_cls/`).\n",
    "\n",
    "Este notebook assume internet ligada sempre; não há modo offline aqui."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5ac114f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c307925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helpers de ambiente\n",
    "\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def env_str(name: str, default: str = \"\") -> str:\n",
    "    value = os.environ.get(name, \"\")\n",
    "    if value == \"\":\n",
    "        return str(default)\n",
    "    return str(value)\n",
    "\n",
    "\n",
    "def env_bool(name: str, default: bool = False) -> bool:\n",
    "    value = env_str(name, \"\").strip()\n",
    "    if value == \"\":\n",
    "        return bool(default)\n",
    "    return value.lower() in {\"1\", \"true\", \"yes\", \"y\", \"on\"}\n",
    "\n",
    "\n",
    "def env_int(name: str, default: int) -> int:\n",
    "    value = env_str(name, \"\").strip()\n",
    "    if value == \"\":\n",
    "        return int(default)\n",
    "    return int(value)\n",
    "\n",
    "\n",
    "def env_path(name: str) -> Path | None:\n",
    "    value = env_str(name, \"\").strip()\n",
    "    if not value:\n",
    "        return None\n",
    "    return Path(value)\n",
    "\n",
    "\n",
    "def run_cmd(cmd: list[str], cwd: Path | None = None) -> None:\n",
    "    cmd_str = \" \".join(str(c) for c in cmd)\n",
    "    print(\"[cmd]\", cmd_str)\n",
    "    subprocess.run(cmd, check=True, cwd=str(cwd) if cwd else None)\n",
    "\n",
    "\n",
    "def find_repo_root() -> Path | None:\n",
    "    explicit = env_path(\"FORGERYSEG_REPO_ROOT\")\n",
    "    if explicit is not None:\n",
    "        return explicit if explicit.exists() else None\n",
    "\n",
    "    here = Path(\".\").resolve()\n",
    "    candidates = [here] + list(here.parents)\n",
    "    for cand in candidates:\n",
    "        if (cand / \"src\" / \"forgeryseg\" / \"__init__.py\").exists() and (cand / \"scripts\").exists():\n",
    "            return cand\n",
    "\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                for base in (ds, ds / \"recodai_bundle\"):\n",
    "                    if (base / \"src\" / \"forgeryseg\" / \"__init__.py\").exists():\n",
    "                        return base\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_data_root() -> Path | None:\n",
    "    explicit = env_path(\"FORGERYSEG_DATA_ROOT\")\n",
    "    if explicit is not None:\n",
    "        return explicit if (explicit / \"train_images\").exists() else None\n",
    "\n",
    "    candidates = [\n",
    "        Path(\"data/recodai\"),\n",
    "        Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection/recodai\"),\n",
    "        Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\"),\n",
    "    ]\n",
    "    for cand in candidates:\n",
    "        if (cand / \"train_images\").exists():\n",
    "            return cand\n",
    "\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                if (ds / \"train_images\").exists():\n",
    "                    return ds\n",
    "                if (ds / \"recodai\" / \"train_images\").exists():\n",
    "                    return ds / \"recodai\"\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_requirements_file(repo_root: Path) -> Path | None:\n",
    "    candidates = [\n",
    "        repo_root / \"requirements.txt\",\n",
    "        repo_root / \"recodai_bundle\" / \"requirements.txt\",\n",
    "    ]\n",
    "    for cand in candidates:\n",
    "        if cand.exists():\n",
    "            return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def find_wheels_root(repo_root: Path) -> Path | None:\n",
    "    explicit = env_path(\"FORGERYSEG_WHEELS_ROOT\")\n",
    "    if explicit is not None:\n",
    "        return explicit if explicit.exists() else None\n",
    "\n",
    "    candidates = [\n",
    "        repo_root / \"recodai_bundle\" / \"wheels\",\n",
    "        repo_root / \"wheels\",\n",
    "    ]\n",
    "    for cand in candidates:\n",
    "        if cand.exists() and any(cand.glob(\"*.whl\")):\n",
    "            return cand\n",
    "\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                for cand in (ds / \"wheels\", ds / \"recodai_bundle\" / \"wheels\"):\n",
    "                    if cand.exists() and any(cand.glob(\"*.whl\")):\n",
    "                        return cand\n",
    "    return None\n",
    "\n",
    "\n",
    "def _missing_modules(mod_names: list[str]) -> list[str]:\n",
    "    missing: list[str] = []\n",
    "    for name in mod_names:\n",
    "        try:\n",
    "            __import__(name)\n",
    "        except Exception:\n",
    "            missing.append(name)\n",
    "    return missing\n",
    "\n",
    "\n",
    "def maybe_install_from_wheels(wheels_root: Path) -> None:\n",
    "    module_to_pip = {\n",
    "        \"segmentation_models_pytorch\": \"segmentation-models-pytorch\",\n",
    "        \"timm\": \"timm\",\n",
    "        \"albumentations\": \"albumentations\",\n",
    "        \"huggingface_hub\": \"huggingface-hub\",\n",
    "        \"safetensors\": \"safetensors\",\n",
    "        \"tqdm\": \"tqdm\",\n",
    "        \"sklearn\": \"scikit-learn\",\n",
    "    }\n",
    "    wanted_modules = list(module_to_pip.keys())\n",
    "    missing = _missing_modules(wanted_modules)\n",
    "    if not missing:\n",
    "        print(\"[wheels] ok (nada a instalar).\")\n",
    "        return\n",
    "\n",
    "    packages = [module_to_pip[m] for m in missing if m in module_to_pip]\n",
    "    if not packages:\n",
    "        return\n",
    "\n",
    "    print(\"[wheels] faltando:\", \", \".join(missing))\n",
    "    run_cmd(\n",
    "        [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheels_root),\n",
    "            *packages,\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4f0db9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Setup base (repo/data/outputs)\n",
    "\n",
    "REPO_ROOT = find_repo_root() or Path(\".\").resolve()\n",
    "if not (REPO_ROOT / \"src\" / \"forgeryseg\" / \"__init__.py\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Não encontrei o código do repo (src/forgeryseg). \"\n",
    "        \"No Kaggle, anexe um Dataset com `recodai_bundle/` e/ou defina `FORGERYSEG_REPO_ROOT`.\"\n",
    "        f\"\\nTentativa: {REPO_ROOT}\"\n",
    "    )\n",
    "\n",
    "DATA_ROOT = find_data_root()\n",
    "if DATA_ROOT is None or not (DATA_ROOT / \"train_images\").exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"Dataset não encontrado. Anexe o dataset da competição e/ou defina `FORGERYSEG_DATA_ROOT`.\"\n",
    "        f\"\\nTentativa: {DATA_ROOT}\"\n",
    "    )\n",
    "\n",
    "OUTPUTS_ROOT = Path(\"/kaggle/working/outputs\") if is_kaggle() else (REPO_ROOT / \"outputs\")\n",
    "OUTPUTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"OUTPUTS_ROOT:\", OUTPUTS_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2990a8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Instalação (opcional)\n",
    "#\n",
    "# Para treino no Kaggle com Internet ON, o ambiente costuma já ter torch/torchvision.\n",
    "# Se faltar algo (ex.: segmentation_models_pytorch), instalamos automaticamente via pip.\n",
    "#\n",
    "# Nota (Kaggle): o ambiente costuma vir com TensorFlow instalado. Se `protobuf>=5`,\n",
    "# alguns imports podem emitir erros do tipo:\n",
    "# `AttributeError: 'MessageFactory' object has no attribute 'GetPrototype'`.\n",
    "# Para evitar isso, fixamos `protobuf<5` aqui.\n",
    "try:\n",
    "    import google.protobuf  # type: ignore\n",
    "\n",
    "    pb_ver = str(getattr(google.protobuf, \"__version__\", \"0.0.0\"))\n",
    "    pb_major = int(pb_ver.split(\".\", maxsplit=1)[0])\n",
    "except Exception:\n",
    "    pb_ver = \"unknown\"\n",
    "    pb_major = 0\n",
    "\n",
    "if pb_major >= 5:\n",
    "    print(f\"[pip] protobuf {pb_ver} detectado; ajustando para protobuf<5 (compat Kaggle).\")\n",
    "    run_cmd([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"protobuf<5\"])\n",
    "\n",
    "required = [\n",
    "    \"torch\",\n",
    "    \"numpy\",\n",
    "    \"cv2\",\n",
    "    \"albumentations\",\n",
    "    \"timm\",\n",
    "    \"segmentation_models_pytorch\",\n",
    "    \"transformers\",\n",
    "    \"sklearn\",\n",
    "]\n",
    "missing = _missing_modules(required)\n",
    "if missing:\n",
    "    module_to_pip = {\n",
    "        \"albumentations\": \"albumentations\",\n",
    "        \"cv2\": \"opencv-python\",\n",
    "        \"segmentation_models_pytorch\": \"segmentation-models-pytorch\",\n",
    "        \"timm\": \"timm\",\n",
    "        \"transformers\": \"transformers\",\n",
    "        \"sklearn\": \"scikit-learn\",\n",
    "    }\n",
    "    pkgs = [module_to_pip[m] for m in missing if m in module_to_pip]\n",
    "    # Mantém TensorFlow/Kaggle compatível ao resolver dependências.\n",
    "    pkgs.append(\"protobuf<5\")\n",
    "    if pkgs:\n",
    "        print(\"[pip] faltando:\", \", \".join(missing))\n",
    "        run_cmd([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", *pkgs])\n",
    "\n",
    "    missing = _missing_modules(required)\n",
    "    if missing:\n",
    "        raise ImportError(\n",
    "            \"Dependências faltando (mesmo após pip install): \"\n",
    "            + \", \".join(missing)\n",
    "            + \"\\nGaranta que o notebook está com Internet=ON no Kaggle e rode novamente.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc22a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laboratório de experimentos (edite esta célula)\n",
    "#\n",
    "# Objetivo: rodar um \"sweep\" local (Kaggle) de vários modelos/configs, gerar OOF e\n",
    "# empacotar checkpoints/caches para uso offline na Fase 00.\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Iterable\n",
    "\n",
    "import shutil\n",
    "\n",
    "\n",
    "def read_json(path: Path) -> dict[str, Any]:\n",
    "    with path.open(\"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def write_json(path: Path, payload: dict[str, Any]) -> Path:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with path.open(\"w\") as f:\n",
    "        json.dump(payload, f, indent=2)\n",
    "    return path\n",
    "\n",
    "\n",
    "def write_config(base_cfg: Path, out_path: Path, overrides: dict[str, Any]) -> Path:\n",
    "    cfg = read_json(base_cfg)\n",
    "    cfg.update(overrides)\n",
    "    return write_json(out_path, cfg)\n",
    "\n",
    "\n",
    "def _cfg_model_id(cfg_path: Path) -> str:\n",
    "    cfg = read_json(cfg_path)\n",
    "    return str(cfg.get(\"model_id\", cfg_path.stem))\n",
    "\n",
    "\n",
    "def _seg_ckpt_path(outputs_root: Path, model_id: str, fold: int) -> Path:\n",
    "    return outputs_root / \"models_seg\" / model_id / f\"fold_{fold}\" / \"best.pt\"\n",
    "\n",
    "\n",
    "def _cls_ckpt_path(outputs_root: Path, fold: int) -> Path:\n",
    "    return outputs_root / \"models_cls\" / f\"fold_{fold}\" / \"best.pt\"\n",
    "\n",
    "\n",
    "def _folds_to_run(n_folds: int, fold: int) -> list[int]:\n",
    "    return [int(fold)] if int(fold) >= 0 else list(range(int(n_folds)))\n",
    "\n",
    "\n",
    "def _all_exist(paths: Iterable[Path]) -> bool:\n",
    "    return all(p.exists() for p in paths)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SegExperiment:\n",
    "    name: str\n",
    "    base_config: Path\n",
    "    overrides: dict[str, Any]\n",
    "    include_supplemental: bool = False\n",
    "\n",
    "    def model_id(self) -> str:\n",
    "        if \"model_id\" in self.overrides:\n",
    "            return str(self.overrides[\"model_id\"])\n",
    "        return _cfg_model_id(self.base_config)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ClsExperiment:\n",
    "    name: str\n",
    "    config: Path\n",
    "    overrides: dict[str, Any]\n",
    "    include_supplemental: bool = False\n",
    "\n",
    "    def model_id(self) -> str:\n",
    "        if \"model_id\" in self.overrides:\n",
    "            return str(self.overrides[\"model_id\"])\n",
    "        cfg = read_json(self.config)\n",
    "        return str(cfg.get(\"model_id\", self.config.stem))\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DinoHeadExperiment:\n",
    "    name: str\n",
    "    dino_path: str\n",
    "    image_size: int\n",
    "    batch_size: int\n",
    "    epochs: int\n",
    "    lr: float\n",
    "    weight_decay: float\n",
    "    decoder_dropout: float\n",
    "    patience: int = 3\n",
    "\n",
    "\n",
    "def _strip_tu_prefix(encoder_name: str) -> str:\n",
    "    text = str(encoder_name).strip()\n",
    "    return text[3:] if text.startswith(\"tu-\") else text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc63a47f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import do projeto (para garantir que o repo está visível)\n",
    "src_root = REPO_ROOT / \"src\"\n",
    "if str(src_root) not in sys.path:\n",
    "    sys.path.insert(0, str(src_root))\n",
    "\n",
    "from forgeryseg.offline import configure_cache_dirs\n",
    "\n",
    "# Cache root (recomendado no Kaggle para empacotar pesos)\n",
    "CACHE_ROOT = env_path(\"FORGERYSEG_CACHE_ROOT\")\n",
    "if CACHE_ROOT is None and is_kaggle():\n",
    "    CACHE_ROOT = Path(\"/kaggle/working/weights_cache\")\n",
    "\n",
    "if CACHE_ROOT is not None:\n",
    "    configure_cache_dirs(CACHE_ROOT)\n",
    "    print(\"[CACHE] using\", CACHE_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6f26b2",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Perfil / budget (edite aqui)\n",
    "PROFILE = env_str(\"FORGERYSEG_PROFILE\", \"quick\").strip().lower()  # quick | sweep | full\n",
    "N_FOLDS = env_int(\"FORGERYSEG_N_FOLDS\", 5)\n",
    "FOLD = env_int(\"FORGERYSEG_FOLD\", 0)  # -1 = todos\n",
    "DRY_RUN = env_bool(\"FORGERYSEG_DRY_RUN\", default=False)\n",
    "SKIP_EXISTING = env_bool(\"FORGERYSEG_SKIP_EXISTING\", default=True)\n",
    "LIMIT = env_int(\"FORGERYSEG_LIMIT\", 0)  # limita OOF/score (debug)\n",
    "\n",
    "RUN_WARMUP_CACHE = env_bool(\"FORGERYSEG_WARMUP_CACHE\", default=False)\n",
    "RUN_SEG_EXPERIMENTS = env_bool(\"FORGERYSEG_RUN_SEG_EXPERIMENTS\", default=True)\n",
    "RUN_CLS_EXPERIMENTS = env_bool(\"FORGERYSEG_RUN_CLS_EXPERIMENTS\", default=False)\n",
    "RUN_DINO_HEAD_EXPERIMENTS = env_bool(\"FORGERYSEG_RUN_DINO_HEAD_EXPERIMENTS\", default=False)\n",
    "\n",
    "RUN_OOF_SEG = env_bool(\"FORGERYSEG_RUN_OOF_SEG\", default=True)\n",
    "RUN_OOF_DINO = env_bool(\"FORGERYSEG_RUN_OOF_DINO\", default=False)\n",
    "RUN_TUNE_THRESHOLDS = env_bool(\"FORGERYSEG_RUN_TUNE_THRESHOLDS\", default=False)\n",
    "RUN_OPTIMIZE_ENSEMBLE = env_bool(\"FORGERYSEG_RUN_OPTIMIZE_ENSEMBLE\", default=False)\n",
    "\n",
    "# Ajustes automáticos por perfil (p/ acelerar o sweep)\n",
    "SEG_EPOCHS_OVERRIDE = env_int(\"FORGERYSEG_SEG_EPOCHS_OVERRIDE\", 0)\n",
    "CLS_EPOCHS_OVERRIDE = env_int(\"FORGERYSEG_CLS_EPOCHS_OVERRIDE\", 0)\n",
    "if PROFILE in {\"quick\", \"sweep\"}:\n",
    "    SEG_EPOCHS_OVERRIDE = int(SEG_EPOCHS_OVERRIDE or 5)\n",
    "    CLS_EPOCHS_OVERRIDE = int(CLS_EPOCHS_OVERRIDE or 3)\n",
    "\n",
    "OOF_TTA = env_str(\"FORGERYSEG_OOF_TTA\", \"none\").strip() if PROFILE in {\"quick\", \"sweep\"} else env_str(\"FORGERYSEG_OOF_TTA\", \"none,hflip,vflip\").strip()\n",
    "OOF_TILE_SIZE = env_int(\"FORGERYSEG_OOF_TILE_SIZE\", 1024)\n",
    "OOF_OVERLAP = env_int(\"FORGERYSEG_OOF_OVERLAP\", 128)\n",
    "\n",
    "print(\"PROFILE:\", PROFILE)\n",
    "print(\"N_FOLDS:\", N_FOLDS)\n",
    "print(\"FOLD:\", FOLD)\n",
    "print(\"DRY_RUN:\", DRY_RUN)\n",
    "print(\"SKIP_EXISTING:\", SKIP_EXISTING)\n",
    "print(\"LIMIT:\", LIMIT)\n",
    "print(\"RUN_WARMUP_CACHE:\", RUN_WARMUP_CACHE)\n",
    "print(\"RUN_SEG_EXPERIMENTS:\", RUN_SEG_EXPERIMENTS)\n",
    "print(\"RUN_CLS_EXPERIMENTS:\", RUN_CLS_EXPERIMENTS)\n",
    "print(\"RUN_DINO_HEAD_EXPERIMENTS:\", RUN_DINO_HEAD_EXPERIMENTS)\n",
    "print(\"RUN_OOF_SEG:\", RUN_OOF_SEG)\n",
    "print(\"RUN_OOF_DINO:\", RUN_OOF_DINO)\n",
    "print(\"RUN_TUNE_THRESHOLDS:\", RUN_TUNE_THRESHOLDS)\n",
    "print(\"RUN_OPTIMIZE_ENSEMBLE:\", RUN_OPTIMIZE_ENSEMBLE)\n",
    "print(\"SEG_EPOCHS_OVERRIDE:\", SEG_EPOCHS_OVERRIDE)\n",
    "print(\"CLS_EPOCHS_OVERRIDE:\", CLS_EPOCHS_OVERRIDE)\n",
    "print(\"OOF_TTA:\", OOF_TTA)\n",
    "print(\"OOF_TILE_SIZE:\", OOF_TILE_SIZE)\n",
    "print(\"OOF_OVERLAP:\", OOF_OVERLAP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ed68bf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Definição de experimentos (máximo útil, mas nada roda sem os RUN_* acima)\n",
    "#\n",
    "# Importante:\n",
    "# - Segmentação: cada `model_id` cria uma pasta própria em `outputs/models_seg/<model_id>/...` (ok treinar vários).\n",
    "# - Classificação: por padrão o treino escreve em `outputs/models_cls/fold_*/...` (uma “família” por vez).\n",
    "#   Neste notebook você seleciona **1** classificador ativo para evitar sobrescrita.\n",
    "\n",
    "SEG_CONFIGS = [\n",
    "    REPO_ROOT / \"configs\" / \"seg_unetpp_tu_convnext_small.json\",\n",
    "    REPO_ROOT / \"configs\" / \"seg_unetpp_tu_swin_tiny.json\",\n",
    "    REPO_ROOT / \"configs\" / \"seg_deeplabv3p_tu_resnest101e.json\",\n",
    "    REPO_ROOT / \"configs\" / \"seg_segformer_mit_b2.json\",\n",
    "    REPO_ROOT / \"configs\" / \"seg_dinov2_base.json\",\n",
    "    REPO_ROOT / \"configs\" / \"seg_dinov2_base_640.json\",\n",
    "]\n",
    "for p in SEG_CONFIGS:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "CLS_CONFIGS = [\n",
    "    REPO_ROOT / \"configs\" / \"cls_effnet_b4.json\",\n",
    "    REPO_ROOT / \"configs\" / \"cls_convnext_small_encoder.json\",\n",
    "    REPO_ROOT / \"configs\" / \"cls_swin_tiny_encoder.json\",\n",
    "    REPO_ROOT / \"configs\" / \"cls_dinov2_base.json\",\n",
    "]\n",
    "for p in CLS_CONFIGS:\n",
    "    if not p.exists():\n",
    "        raise FileNotFoundError(p)\n",
    "\n",
    "SEG_LEVEL = env_str(\"FORGERYSEG_SEG_LEVEL\", \"base\").strip().lower()  # base | plus | max\n",
    "SEG_FILTER = env_str(\"FORGERYSEG_SEG_FILTER\", \"\").strip()  # opcional: \"convnext,segformer\"\n",
    "CLS_ACTIVE = env_str(\"FORGERYSEG_CLS_ACTIVE\", \"cls_effnet_b4\").strip()  # escolha 1\n",
    "\n",
    "seg_experiments_all: list[SegExperiment] = []\n",
    "for base_cfg in SEG_CONFIGS:\n",
    "    base = read_json(base_cfg)\n",
    "    mid = str(base.get(\"model_id\", base_cfg.stem))\n",
    "\n",
    "    # Base\n",
    "    seg_experiments_all.append(SegExperiment(name=mid, base_config=base_cfg, overrides={}))\n",
    "\n",
    "    # + supplemental (pode ajudar recall)\n",
    "    seg_experiments_all.append(\n",
    "        SegExperiment(\n",
    "            name=f\"{mid}_supp\",\n",
    "            base_config=base_cfg,\n",
    "            overrides={\"model_id\": f\"{mid}_supp\", \"include_supplemental\": True},\n",
    "            include_supplemental=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # + FFT channels (4ch) (experimento agressivo: pode melhorar copy-move)\n",
    "    seg_experiments_all.append(\n",
    "        SegExperiment(\n",
    "            name=f\"{mid}_fft\",\n",
    "            base_config=base_cfg,\n",
    "            overrides={\"model_id\": f\"{mid}_fft\", \"use_freq_channels\": True},\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # + BCE+Dice (às vezes melhora componentes pequenas)\n",
    "    if str(base.get(\"backend\", \"smp\")).lower() in {\"smp\", \"\"}:\n",
    "        seg_experiments_all.append(\n",
    "            SegExperiment(\n",
    "                name=f\"{mid}_bce_dice\",\n",
    "                base_config=base_cfg,\n",
    "                overrides={\"model_id\": f\"{mid}_bce_dice\", \"loss\": \"bce_dice\", \"dice_weight\": 1.0},\n",
    "            )\n",
    "        )\n",
    "\n",
    "# Força HF configs a baixarem pesos (internet ON)\n",
    "seg_experiments_all = [\n",
    "    SegExperiment(\n",
    "        name=e.name,\n",
    "        base_config=e.base_config,\n",
    "        overrides={\n",
    "            **e.overrides,\n",
    "            **({\"local_files_only\": False} if str(read_json(e.base_config).get(\"backend\", \"\")).lower() in {\"dinov2\", \"hf\"} else {}),\n",
    "        },\n",
    "        include_supplemental=e.include_supplemental,\n",
    "    )\n",
    "    for e in seg_experiments_all\n",
    "]\n",
    "\n",
    "def _select_seg_experiments(experiments: list[SegExperiment], level: str, flt: str) -> list[SegExperiment]:\n",
    "    level = str(level).strip().lower()\n",
    "    if level not in {\"base\", \"plus\", \"max\"}:\n",
    "        raise ValueError(\"FORGERYSEG_SEG_LEVEL deve ser: base | plus | max\")\n",
    "\n",
    "    if level == \"max\":\n",
    "        selected = list(experiments)\n",
    "    elif level == \"plus\":\n",
    "        selected = [e for e in experiments if not (e.name.endswith(\"_fft\") or e.name.endswith(\"_bce_dice\"))]\n",
    "    else:  # base\n",
    "        selected = [e for e in experiments if not (e.name.endswith(\"_supp\") or e.name.endswith(\"_fft\") or e.name.endswith(\"_bce_dice\"))]\n",
    "\n",
    "    if flt.strip():\n",
    "        tokens = [t.strip().lower() for t in flt.split(\",\") if t.strip()]\n",
    "        selected = [e for e in selected if any(tok in e.name.lower() for tok in tokens)]\n",
    "\n",
    "    # remove duplicates mantendo ordem\n",
    "    seen = set()\n",
    "    out = []\n",
    "    for e in selected:\n",
    "        if e.name in seen:\n",
    "            continue\n",
    "        seen.add(e.name)\n",
    "        out.append(e)\n",
    "    return out\n",
    "\n",
    "\n",
    "seg_experiments = _select_seg_experiments(seg_experiments_all, SEG_LEVEL, SEG_FILTER)\n",
    "\n",
    "cls_experiments_all: list[ClsExperiment] = []\n",
    "for cfg_path in CLS_CONFIGS:\n",
    "    base = read_json(cfg_path)\n",
    "    mid = str(base.get(\"model_id\", cfg_path.stem))\n",
    "    cls_experiments_all.append(ClsExperiment(name=mid, config=cfg_path, overrides={}))\n",
    "    cls_experiments_all.append(\n",
    "        ClsExperiment(\n",
    "            name=f\"{mid}_supp\",\n",
    "            config=cfg_path,\n",
    "            overrides={\"model_id\": f\"{mid}_supp\", \"include_supplemental\": True},\n",
    "            include_supplemental=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Força HF configs a baixarem pesos (internet ON)\n",
    "cls_experiments_all = [\n",
    "    ClsExperiment(\n",
    "        name=e.name,\n",
    "        config=e.config,\n",
    "        overrides={\n",
    "            **e.overrides,\n",
    "            **({\"local_files_only\": False} if str(read_json(e.config).get(\"backend\", \"\")).lower() in {\"dinov2\", \"hf\"} else {}),\n",
    "        },\n",
    "        include_supplemental=e.include_supplemental,\n",
    "    )\n",
    "    for e in cls_experiments_all\n",
    "]\n",
    "\n",
    "cls_experiments = [e for e in cls_experiments_all if e.name == CLS_ACTIVE]\n",
    "if RUN_CLS_EXPERIMENTS and not cls_experiments:\n",
    "    raise ValueError(f\"CLS_ACTIVE inválido: {CLS_ACTIVE}. Opções: {[e.name for e in cls_experiments_all]}\")\n",
    "\n",
    "print(f\"Seg experiments (all): {len(seg_experiments_all)} | active={len(seg_experiments)} | level={SEG_LEVEL} filter={SEG_FILTER!r}\")\n",
    "print(f\"Cls experiments (all): {len(cls_experiments_all)} | active={len(cls_experiments)} | CLS_ACTIVE={CLS_ACTIVE!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14c501",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Experimentos DINO head (opcional) — útil como baseline leve / ablação\n",
    "DINO_HEAD_LEVEL = env_str(\"FORGERYSEG_DINO_HEAD_LEVEL\", \"base\").strip().lower()  # base | max\n",
    "DINO_HEAD_FILTER = env_str(\"FORGERYSEG_DINO_HEAD_FILTER\", \"\").strip()\n",
    "DINO_PATH = env_str(\"FORGERYSEG_DINO_PATH\", \"facebook/dinov2-base\").strip()\n",
    "\n",
    "dino_head_epochs = env_int(\"FORGERYSEG_DINO_HEAD_EPOCHS\", 0)\n",
    "if int(dino_head_epochs) <= 0:\n",
    "    dino_head_epochs = 5 if PROFILE in {\"quick\", \"sweep\"} else 10\n",
    "\n",
    "dino_head_experiments_all: list[DinoHeadExperiment] = [\n",
    "    DinoHeadExperiment(\n",
    "        name=\"dino_head_512\",\n",
    "        dino_path=DINO_PATH,\n",
    "        image_size=512,\n",
    "        batch_size=4,\n",
    "        epochs=int(dino_head_epochs),\n",
    "        lr=3e-4,\n",
    "        weight_decay=1e-2,\n",
    "        decoder_dropout=0.0,\n",
    "        patience=3,\n",
    "    ),\n",
    "    DinoHeadExperiment(\n",
    "        name=\"dino_head_384\",\n",
    "        dino_path=DINO_PATH,\n",
    "        image_size=384,\n",
    "        batch_size=8,\n",
    "        epochs=int(dino_head_epochs),\n",
    "        lr=3e-4,\n",
    "        weight_decay=1e-2,\n",
    "        decoder_dropout=0.0,\n",
    "        patience=3,\n",
    "    ),\n",
    "    DinoHeadExperiment(\n",
    "        name=\"dino_head_640\",\n",
    "        dino_path=DINO_PATH,\n",
    "        image_size=640,\n",
    "        batch_size=2,\n",
    "        epochs=int(dino_head_epochs),\n",
    "        lr=3e-4,\n",
    "        weight_decay=1e-2,\n",
    "        decoder_dropout=0.0,\n",
    "        patience=3,\n",
    "    ),\n",
    "    DinoHeadExperiment(\n",
    "        name=\"dino_head_512_do10\",\n",
    "        dino_path=DINO_PATH,\n",
    "        image_size=512,\n",
    "        batch_size=4,\n",
    "        epochs=int(dino_head_epochs),\n",
    "        lr=3e-4,\n",
    "        weight_decay=1e-2,\n",
    "        decoder_dropout=0.10,\n",
    "        patience=3,\n",
    "    ),\n",
    "]\n",
    "\n",
    "if DINO_HEAD_LEVEL == \"base\":\n",
    "    dino_head_experiments = [e for e in dino_head_experiments_all if e.name in {\"dino_head_512\", \"dino_head_512_do10\"}]\n",
    "elif DINO_HEAD_LEVEL == \"max\":\n",
    "    dino_head_experiments = list(dino_head_experiments_all)\n",
    "else:\n",
    "    raise ValueError(\"FORGERYSEG_DINO_HEAD_LEVEL deve ser: base | max\")\n",
    "\n",
    "if DINO_HEAD_FILTER.strip():\n",
    "    tokens = [t.strip().lower() for t in DINO_HEAD_FILTER.split(\",\") if t.strip()]\n",
    "    dino_head_experiments = [e for e in dino_head_experiments if any(tok in e.name.lower() for tok in tokens)]\n",
    "\n",
    "print(f\"DINO head experiments (all): {len(dino_head_experiments_all)} | active={len(dino_head_experiments)} | level={DINO_HEAD_LEVEL} filter={DINO_HEAD_FILTER!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b08cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warmup de cache (opcional): baixa pesos pretrained para não depender de downloads depois.\n",
    "# Isso ajuda a empacotar `weights_cache/` para uso offline no Kaggle.\n",
    "if RUN_WARMUP_CACHE:\n",
    "    if CACHE_ROOT is None:\n",
    "        raise RuntimeError(\"Defina FORGERYSEG_CACHE_ROOT ou rode no Kaggle (default /kaggle/working/weights_cache).\")\n",
    "\n",
    "    # Timm weights (classificadores e encoders `tu-*` do SMP)\n",
    "    timm_models: set[str] = set()\n",
    "    for cfg_path in CLS_CONFIGS:\n",
    "        cfg = read_json(cfg_path)\n",
    "        backend = str(cfg.get(\"backend\", \"timm\")).lower()\n",
    "        if backend in {\"timm\", \"timm_encoder\"}:\n",
    "            timm_models.add(str(cfg.get(\"model_name\", \"\")))\n",
    "    for cfg_path in SEG_CONFIGS:\n",
    "        cfg = read_json(cfg_path)\n",
    "        encoder = str(cfg.get(\"encoder_name\", \"\"))\n",
    "        if encoder.startswith(\"tu-\"):\n",
    "            timm_models.add(_strip_tu_prefix(encoder))\n",
    "\n",
    "    timm_models = {m for m in timm_models if m}\n",
    "    if timm_models:\n",
    "        dl_script = REPO_ROOT / \"scripts\" / \"download_timm_weights.py\"\n",
    "        if dl_script.exists():\n",
    "            run_cmd(\n",
    "                [\n",
    "                    sys.executable,\n",
    "                    str(dl_script),\n",
    "                    \"--models\",\n",
    "                    \",\".join(sorted(timm_models)),\n",
    "                    \"--out-dir\",\n",
    "                    str(CACHE_ROOT / \"timm_state_dict\"),\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            print(\"[warmup] scripts/download_timm_weights.py não encontrado (pulando).\")\n",
    "\n",
    "    # HuggingFace weights (DINOv2)\n",
    "    try:\n",
    "        from transformers import AutoModel\n",
    "\n",
    "        hf_ids = set()\n",
    "        for cfg_path in SEG_CONFIGS + CLS_CONFIGS:\n",
    "            cfg = read_json(cfg_path)\n",
    "            if str(cfg.get(\"backend\", \"\")).lower() in {\"dinov2\", \"hf\"}:\n",
    "                hf_ids.add(str(cfg.get(\"hf_model_id\", \"\")))\n",
    "        hf_ids = {x for x in hf_ids if x}\n",
    "        for mid in sorted(hf_ids):\n",
    "            print(\"[hf] baixando:\", mid)\n",
    "            AutoModel.from_pretrained(mid)\n",
    "    except Exception as exc:\n",
    "        print(\"[warmup] transformers não disponível ou falhou:\", exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3744fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runner: treino (seg/cls) + OOF + tuning\n",
    "\n",
    "def _maybe_override_epochs(cfg: dict[str, Any], epochs_override: int) -> dict[str, Any]:\n",
    "    if int(epochs_override) <= 0:\n",
    "        return cfg\n",
    "    out = dict(cfg)\n",
    "    out[\"epochs\"] = int(epochs_override)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _train_seg(exp: SegExperiment) -> None:\n",
    "    train_seg_script = REPO_ROOT / \"scripts\" / \"train_seg_smp_cv.py\"\n",
    "    if not train_seg_script.exists():\n",
    "        raise FileNotFoundError(train_seg_script)\n",
    "\n",
    "    model_id = exp.model_id()\n",
    "    out_cfg = OUTPUTS_ROOT / \"configs\" / f\"seg_{exp.name}.json\"\n",
    "    cfg = read_json(exp.base_config)\n",
    "    cfg.update(exp.overrides)\n",
    "    cfg = _maybe_override_epochs(cfg, SEG_EPOCHS_OVERRIDE)\n",
    "    out_cfg = write_json(out_cfg, cfg)\n",
    "\n",
    "    folds_to_run = _folds_to_run(N_FOLDS, FOLD)\n",
    "    if SKIP_EXISTING and _all_exist(_seg_ckpt_path(OUTPUTS_ROOT, model_id, f) for f in folds_to_run):\n",
    "        print(f\"[SEG] {model_id}: ckpt já existe (pulando).\")\n",
    "        return\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(train_seg_script),\n",
    "        \"--config\",\n",
    "        str(out_cfg),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--output-dir\",\n",
    "        str(OUTPUTS_ROOT),\n",
    "        \"--folds\",\n",
    "        str(N_FOLDS),\n",
    "    ]\n",
    "    if int(FOLD) >= 0:\n",
    "        cmd += [\"--fold\", str(FOLD)]\n",
    "    if exp.include_supplemental or bool(cfg.get(\"include_supplemental\", False)):\n",
    "        cmd += [\"--include-supplemental\"]\n",
    "    if CACHE_ROOT is not None:\n",
    "        cmd += [\"--cache-root\", str(CACHE_ROOT)]\n",
    "\n",
    "    if DRY_RUN:\n",
    "        print(\"[dry-run]\", \" \".join(map(str, cmd)))\n",
    "        return\n",
    "    run_cmd(cmd)\n",
    "\n",
    "\n",
    "def _train_cls(exp: ClsExperiment) -> None:\n",
    "    train_cls_script = REPO_ROOT / \"scripts\" / \"train_cls_cv.py\"\n",
    "    if not train_cls_script.exists():\n",
    "        raise FileNotFoundError(train_cls_script)\n",
    "\n",
    "    out_cfg = OUTPUTS_ROOT / \"configs\" / f\"cls_{exp.name}.json\"\n",
    "    cfg = read_json(exp.config)\n",
    "    cfg.update(exp.overrides)\n",
    "    cfg = _maybe_override_epochs(cfg, CLS_EPOCHS_OVERRIDE)\n",
    "    out_cfg = write_json(out_cfg, cfg)\n",
    "\n",
    "    folds_to_run = _folds_to_run(N_FOLDS, FOLD)\n",
    "    if SKIP_EXISTING and _all_exist(_cls_ckpt_path(OUTPUTS_ROOT, f) for f in folds_to_run):\n",
    "        try:\n",
    "            import torch\n",
    "\n",
    "            expected_id = exp.model_id()\n",
    "            same = True\n",
    "            for fold_id in folds_to_run:\n",
    "                ckpt = torch.load(_cls_ckpt_path(OUTPUTS_ROOT, fold_id), map_location=\"cpu\")\n",
    "                ckpt_cfg = ckpt.get(\"config\", {}) if isinstance(ckpt, dict) else {}\n",
    "                if str(ckpt_cfg.get(\"model_id\", \"\")) != str(expected_id):\n",
    "                    same = False\n",
    "                    break\n",
    "            if same:\n",
    "                print(f\"[CLS] ckpt já existe para {expected_id} (pulando).\")\n",
    "                return\n",
    "        except Exception:\n",
    "            print(\"[CLS] ckpt existe, mas não consegui validar model_id; retreinando (para evitar sobrescrita errada).\")\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(train_cls_script),\n",
    "        \"--config\",\n",
    "        str(out_cfg),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--output-dir\",\n",
    "        str(OUTPUTS_ROOT),\n",
    "        \"--folds\",\n",
    "        str(N_FOLDS),\n",
    "    ]\n",
    "    if int(FOLD) >= 0:\n",
    "        cmd += [\"--fold\", str(FOLD)]\n",
    "    if exp.include_supplemental or bool(cfg.get(\"include_supplemental\", False)):\n",
    "        cmd += [\"--include-supplemental\"]\n",
    "    if CACHE_ROOT is not None:\n",
    "        cmd += [\"--cache-root\", str(CACHE_ROOT)]\n",
    "\n",
    "    if DRY_RUN:\n",
    "        print(\"[dry-run]\", \" \".join(map(str, cmd)))\n",
    "        return\n",
    "    run_cmd(cmd)\n",
    "\n",
    "\n",
    "def _oof_seg(model_id: str) -> None:\n",
    "    script = REPO_ROOT / \"scripts\" / \"predict_seg_oof.py\"\n",
    "    if not script.exists():\n",
    "        raise FileNotFoundError(script)\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(script),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--output-dir\",\n",
    "        str(OUTPUTS_ROOT),\n",
    "        \"--model-id\",\n",
    "        str(model_id),\n",
    "        \"--folds\",\n",
    "        str(N_FOLDS),\n",
    "        \"--tta\",\n",
    "        str(OOF_TTA),\n",
    "        \"--tile-size\",\n",
    "        str(OOF_TILE_SIZE),\n",
    "        \"--overlap\",\n",
    "        str(OOF_OVERLAP),\n",
    "    ]\n",
    "    if int(FOLD) >= 0:\n",
    "        cmd += [\"--fold\", str(FOLD)]\n",
    "    if int(LIMIT) > 0:\n",
    "        cmd += [\"--limit\", str(LIMIT)]\n",
    "\n",
    "    if DRY_RUN:\n",
    "        print(\"[dry-run]\", \" \".join(map(str, cmd)))\n",
    "        return\n",
    "    run_cmd(cmd)\n",
    "\n",
    "\n",
    "def _tune_postprocess(model_id: str) -> None:\n",
    "    script = REPO_ROOT / \"scripts\" / \"tune_thresholds.py\"\n",
    "    if not script.exists():\n",
    "        raise FileNotFoundError(script)\n",
    "\n",
    "    preds_root = OUTPUTS_ROOT / \"oof\" / model_id\n",
    "    out_cfg = OUTPUTS_ROOT / \"configs\" / f\"postproc_{model_id}.json\"\n",
    "\n",
    "    # Grid pequeno por padrão (aumente se quiser)\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(script),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--preds-root\",\n",
    "        str(preds_root),\n",
    "        \"--folds\",\n",
    "        str(N_FOLDS),\n",
    "        \"--adaptive-threshold\",\n",
    "        \"--threshold-factors\",\n",
    "        \"0.2,0.3,0.4\",\n",
    "        \"--min-areas\",\n",
    "        \"0,30,64,128\",\n",
    "        \"--min-area-percents\",\n",
    "        \"0.0002,0.0005,0.001\",\n",
    "        \"--min-confidences\",\n",
    "        \"0.30,0.33,0.36,0.40\",\n",
    "        \"--out-config\",\n",
    "        str(out_cfg),\n",
    "    ]\n",
    "    if int(LIMIT) > 0:\n",
    "        cmd += [\"--limit\", str(LIMIT)]\n",
    "\n",
    "    if DRY_RUN:\n",
    "        print(\"[dry-run]\", \" \".join(map(str, cmd)))\n",
    "        return\n",
    "    run_cmd(cmd)\n",
    "\n",
    "\n",
    "def _train_dino_head(exp: DinoHeadExperiment) -> None:\n",
    "    script = REPO_ROOT / \"scripts\" / \"train_dino_head.py\"\n",
    "    if not script.exists():\n",
    "        raise FileNotFoundError(script)\n",
    "\n",
    "    out_root = OUTPUTS_ROOT / \"models_dino\" / exp.name\n",
    "    folds_to_run = _folds_to_run(N_FOLDS, FOLD)\n",
    "    for fold_id in folds_to_run:\n",
    "        ckpt_path = out_root / f\"fold_{fold_id}\" / \"best.pt\"\n",
    "        if SKIP_EXISTING and ckpt_path.exists():\n",
    "            print(f\"[DINO] {exp.name} fold={fold_id}: ckpt já existe (pulando).\")\n",
    "            continue\n",
    "\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            str(script),\n",
    "            \"--data-root\",\n",
    "            str(DATA_ROOT),\n",
    "            \"--output-dir\",\n",
    "            str(out_root),\n",
    "            \"--folds\",\n",
    "            str(N_FOLDS),\n",
    "            \"--fold\",\n",
    "            str(fold_id),\n",
    "            \"--dino-path\",\n",
    "            str(exp.dino_path),\n",
    "            \"--image-size\",\n",
    "            str(int(exp.image_size)),\n",
    "            \"--batch-size\",\n",
    "            str(int(exp.batch_size)),\n",
    "            \"--epochs\",\n",
    "            str(int(exp.epochs)),\n",
    "            \"--lr\",\n",
    "            str(float(exp.lr)),\n",
    "            \"--weight-decay\",\n",
    "            str(float(exp.weight_decay)),\n",
    "            \"--decoder-dropout\",\n",
    "            str(float(exp.decoder_dropout)),\n",
    "            \"--patience\",\n",
    "            str(int(exp.patience)),\n",
    "        ]\n",
    "        if CACHE_ROOT is not None:\n",
    "            cmd += [\"--cache-dir\", str(CACHE_ROOT / \"hf\")]\n",
    "\n",
    "        if DRY_RUN:\n",
    "            print(\"[dry-run]\", \" \".join(map(str, cmd)))\n",
    "            continue\n",
    "        run_cmd(cmd)\n",
    "\n",
    "\n",
    "def _oof_dino(exp: DinoHeadExperiment) -> None:\n",
    "    script = REPO_ROOT / \"scripts\" / \"predict_dino_oof.py\"\n",
    "    if not script.exists():\n",
    "        raise FileNotFoundError(script)\n",
    "\n",
    "    preds_root = OUTPUTS_ROOT / \"oof_dino\" / exp.name\n",
    "    head_dir = OUTPUTS_ROOT / \"models_dino\" / exp.name\n",
    "    run_dir = OUTPUTS_ROOT / \"runs_dino\" / exp.name\n",
    "\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(script),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--preds-root\",\n",
    "        str(preds_root),\n",
    "        \"--folds\",\n",
    "        str(N_FOLDS),\n",
    "        \"--head-ckpt-dir\",\n",
    "        str(head_dir),\n",
    "        \"--tta\",\n",
    "        str(OOF_TTA if PROFILE in {\"quick\", \"sweep\"} else \"none,hflip,vflip,rot90,rot180,rot270\"),\n",
    "        \"--run-dir\",\n",
    "        str(run_dir),\n",
    "    ]\n",
    "    if int(FOLD) >= 0:\n",
    "        cmd += [\"--fold\", str(FOLD)]\n",
    "    if int(LIMIT) > 0:\n",
    "        cmd += [\"--limit\", str(LIMIT)]\n",
    "    if CACHE_ROOT is not None:\n",
    "        cmd += [\"--cache-dir\", str(CACHE_ROOT / \"hf\")]\n",
    "\n",
    "    if DRY_RUN:\n",
    "        print(\"[dry-run]\", \" \".join(map(str, cmd)))\n",
    "        return\n",
    "    run_cmd(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c052a96",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 1) Treino de segmentação (sweep)\n",
    "if RUN_SEG_EXPERIMENTS:\n",
    "    for exp in seg_experiments:\n",
    "        _train_seg(exp)\n",
    "else:\n",
    "    print(\"[SEG] RUN_SEG_EXPERIMENTS=False (pulando).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a822c32",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 2) Treino de classificação (opcional, para gate na submissão)\n",
    "if RUN_CLS_EXPERIMENTS:\n",
    "    for exp in cls_experiments:\n",
    "        _train_cls(exp)\n",
    "else:\n",
    "    print(\"[CLS] RUN_CLS_EXPERIMENTS=False (pulando).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe8bb66",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 2b) Treino DINO head (opcional)\n",
    "if RUN_DINO_HEAD_EXPERIMENTS:\n",
    "    for exp in dino_head_experiments:\n",
    "        _train_dino_head(exp)\n",
    "else:\n",
    "    print(\"[DINO] RUN_DINO_HEAD_EXPERIMENTS=False (pulando).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ab4306",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 2c) OOF + score (DINO head)\n",
    "if RUN_OOF_DINO:\n",
    "    for exp in dino_head_experiments:\n",
    "        _oof_dino(exp)\n",
    "else:\n",
    "    print(\"[DINO OOF] RUN_OOF_DINO=False (pulando).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e99ea5",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 3) OOF + score rápido (segmentação)\n",
    "if RUN_OOF_SEG:\n",
    "    for exp in seg_experiments:\n",
    "        _oof_seg(exp.model_id())\n",
    "else:\n",
    "    print(\"[OOF] RUN_OOF_SEG=False (pulando).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9595597",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# 4) Tuning de pós-processamento (segmentação)\n",
    "if RUN_TUNE_THRESHOLDS:\n",
    "    for exp in seg_experiments:\n",
    "        _tune_postprocess(exp.model_id())\n",
    "else:\n",
    "    print(\"[TUNE] RUN_TUNE_THRESHOLDS=False (pulando).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd4a2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Otimizar pesos do ensemble (proxy) (requer OOF de múltiplos modelos)\n",
    "if RUN_OPTIMIZE_ENSEMBLE:\n",
    "    script = REPO_ROOT / \"scripts\" / \"optimize_ensemble.py\"\n",
    "    if not script.exists():\n",
    "        raise FileNotFoundError(script)\n",
    "\n",
    "    # Edite aqui se quiser restringir o ensemble a poucos modelos base (mais realista)\n",
    "    ensemble_model_ids = sorted({e.model_id() for e in seg_experiments})\n",
    "\n",
    "    out_path = OUTPUTS_ROOT / \"ensemble_weights.json\"\n",
    "    cmd = [\n",
    "        sys.executable,\n",
    "        str(script),\n",
    "        \"--data-root\",\n",
    "        str(DATA_ROOT),\n",
    "        \"--oof-dir\",\n",
    "        str(OUTPUTS_ROOT / \"oof\"),\n",
    "        \"--models\",\n",
    "        \",\".join(ensemble_model_ids),\n",
    "        \"--out\",\n",
    "        str(out_path),\n",
    "    ]\n",
    "    if DRY_RUN:\n",
    "        print(\"[dry-run]\", \" \".join(map(str, cmd)))\n",
    "    else:\n",
    "        run_cmd(cmd)\n",
    "        print(\"[ENSEMBLE] pesos ->\", out_path)\n",
    "else:\n",
    "    print(\"[ENSEMBLE] RUN_OPTIMIZE_ENSEMBLE=False (pulando).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbeab3e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Sumário rápido dos checkpoints de segmentação (dice@0.5 da validação)\n",
    "def summarize_seg_checkpoints(outputs_root: Path) -> None:\n",
    "    import torch\n",
    "\n",
    "    base = outputs_root / \"models_seg\"\n",
    "    if not base.exists():\n",
    "        print(\"[summary] outputs/models_seg não existe.\")\n",
    "        return\n",
    "    rows = []\n",
    "    for model_dir in sorted(base.iterdir()):\n",
    "        if not model_dir.is_dir():\n",
    "            continue\n",
    "        scores = []\n",
    "        for ckpt_path in sorted(model_dir.glob(\"fold_*/best.pt\")):\n",
    "            try:\n",
    "                ckpt = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "                score = ckpt.get(\"score\", None) if isinstance(ckpt, dict) else None\n",
    "                if score is not None:\n",
    "                    scores.append(float(score))\n",
    "            except Exception:\n",
    "                continue\n",
    "        if scores:\n",
    "            rows.append((model_dir.name, float(sum(scores) / len(scores)), len(scores)))\n",
    "    rows = sorted(rows, key=lambda x: x[1], reverse=True)\n",
    "    print(\"Top seg ckpts (mean dice):\")\n",
    "    for name, mean_dice, n in rows[:15]:\n",
    "        print(f\"- {name}: mean_dice={mean_dice:.4f} folds={n}\")\n",
    "\n",
    "\n",
    "summarize_seg_checkpoints(OUTPUTS_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995a5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empacotar artefatos para download\n",
    "#\n",
    "# Saídas (Kaggle):\n",
    "# - `/kaggle/working/outputs_pretrain.zip` (checkpoints + logs + configs gerados)\n",
    "# - `/kaggle/working/weights_cache_pretrain.zip` (opcional; útil para submissão offline sem downloads)\n",
    "\n",
    "zip_base = Path(\"/kaggle/working/outputs_pretrain\") if is_kaggle() else (OUTPUTS_ROOT.parent / \"outputs_pretrain\")\n",
    "zip_path = shutil.make_archive(str(zip_base), \"zip\", root_dir=str(OUTPUTS_ROOT))\n",
    "print(\"ZIP outputs:\", zip_path)\n",
    "print(\"Conteúdo (top-level):\", [p.name for p in OUTPUTS_ROOT.iterdir()])\n",
    "\n",
    "if CACHE_ROOT is not None and CACHE_ROOT.exists():\n",
    "    cache_zip_base = Path(\"/kaggle/working/weights_cache_pretrain\") if is_kaggle() else (CACHE_ROOT.parent / \"weights_cache_pretrain\")\n",
    "    cache_zip = shutil.make_archive(str(cache_zip_base), \"zip\", root_dir=str(CACHE_ROOT))\n",
    "    print(\"ZIP weights_cache:\", cache_zip)\n",
    "else:\n",
    "    print(\"[zip] CACHE_ROOT não definido/existe (pulando weights_cache zip).\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent",
   "notebook_metadata_filter": "language_info"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
