{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f425e8d1",
   "metadata": {},
   "source": [
    "# Fase 1 — Setup offline + checagem (Kaggle)\n",
    "\n",
    "Este notebook existe para **rodar com internet OFF** no Kaggle e deixar claro:\n",
    "- quais pacotes estão disponíveis,\n",
    "- se existe um **bundle offline** (`wheels/*.whl`) anexado,\n",
    "- e se o dataset da competição está montado corretamente.\n",
    "\n",
    "**Sem “código do projeto”**: tudo aqui é auto-contido no notebook.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba473d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1 — Sanidade Kaggle (lembrete)\n",
    "print(\"Kaggle submission constraints (lembrete):\")\n",
    "print(\"- Submissions via Notebook\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF no submit\")\n",
    "print(\"- Output: submission.csv ou submission.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b755aed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2 — Imports + ambiente\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "warnings.simplefilter(\"default\")\n",
    "\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2757ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2b — Instalação offline (opcional): wheels via Kaggle Dataset (sem internet)\n",
    "#\n",
    "# Se você anexar um Dataset que contenha `wheels/*.whl`, esta célula instala os pacotes **offline** via pip.\n",
    "# Estruturas suportadas:\n",
    "# - `/kaggle/input/<dataset>/wheels/*.whl`\n",
    "# - `/kaggle/input/<dataset>/recodai_bundle/wheels/*.whl`\n",
    "#\n",
    "# Se nada for encontrado, nada é instalado (e os imports abaixo vão mostrar o erro explicitamente).\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def _find_offline_bundle() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        return None\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return None\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        for base in (ds, ds / \"recodai_bundle\"):\n",
    "            if (base / \"wheels\").exists():\n",
    "                candidates.append(base)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    if len(candidates) > 1:\n",
    "        print(\"[OFFLINE INSTALL] múltiplos bundles com wheels encontrados; usando o primeiro:\")\n",
    "        for c in candidates:\n",
    "            print(\" -\", c)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "OFFLINE_BUNDLE = _find_offline_bundle()\n",
    "if OFFLINE_BUNDLE is None:\n",
    "    print(\"[OFFLINE INSTALL] nenhum bundle com `wheels/` encontrado em `/kaggle/input`.\")\n",
    "else:\n",
    "    wheel_dir = OFFLINE_BUNDLE / \"wheels\"\n",
    "    whls = sorted(str(p) for p in wheel_dir.glob(\"*.whl\"))\n",
    "    print(\"[OFFLINE INSTALL] bundle:\", OFFLINE_BUNDLE)\n",
    "    print(\"[OFFLINE INSTALL] wheels:\", len(whls))\n",
    "    if not whls:\n",
    "        print(\"[OFFLINE INSTALL] aviso: diretório `wheels/` existe mas não há `.whl`.\")\n",
    "    else:\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheel_dir),\n",
    "            \"--no-deps\",\n",
    "            *whls,\n",
    "        ]\n",
    "        print(\"[OFFLINE INSTALL] executando:\", \" \".join(cmd[:9]), \"...\", f\"(+{len(whls)} wheels)\")\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"[OFFLINE INSTALL] OK.\")\n",
    "\n",
    "\n",
    "def _is_competition_dataset_dir(path: Path) -> bool:\n",
    "    return (path / \"train_images\").exists() or (path / \"test_images\").exists() or (path / \"train_masks\").exists()\n",
    "\n",
    "\n",
    "def _candidate_python_roots(base: Path) -> list[Path]:\n",
    "    roots = [\n",
    "        base,\n",
    "        base / \"src\",\n",
    "        base / \"vendor\",\n",
    "        base / \"third_party\",\n",
    "        base / \"recodai_bundle\",\n",
    "        base / \"recodai_bundle\" / \"src\",\n",
    "        base / \"recodai_bundle\" / \"vendor\",\n",
    "        base / \"recodai_bundle\" / \"third_party\",\n",
    "    ]\n",
    "    return [r for r in roots if r.exists()]\n",
    "\n",
    "\n",
    "def add_local_package_to_syspath(package_dir_name: str) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Procura por `package_dir_name/__init__.py` em `/kaggle/input/*` (exceto o dataset da competição)\n",
    "    e adiciona o root correspondente ao `sys.path`.\n",
    "\n",
    "    Útil quando você importa um repositório GitHub como Dataset do Kaggle contendo libs \"vendorizadas\".\n",
    "    \"\"\"\n",
    "    added: list[Path] = []\n",
    "    if not is_kaggle():\n",
    "        return added\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return added\n",
    "\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        if _is_competition_dataset_dir(ds):\n",
    "            continue\n",
    "        for root in _candidate_python_roots(ds):\n",
    "            pkg = root / package_dir_name\n",
    "            if (pkg / \"__init__.py\").exists():\n",
    "                if str(root) not in sys.path:\n",
    "                    sys.path.insert(0, str(root))\n",
    "                    added.append(root)\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                for child in sorted(p for p in root.glob(\"*\") if p.is_dir()):\n",
    "                    pkg2 = child / package_dir_name\n",
    "                    if (pkg2 / \"__init__.py\").exists():\n",
    "                        if str(child) not in sys.path:\n",
    "                            sys.path.insert(0, str(child))\n",
    "                            added.append(child)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if added:\n",
    "        uniq = []\n",
    "        for p in added:\n",
    "            if p not in uniq:\n",
    "                uniq.append(p)\n",
    "        print(f\"[LOCAL IMPORT] adicionado ao sys.path para '{package_dir_name}':\")\n",
    "        for p in uniq[:10]:\n",
    "            print(\" -\", p)\n",
    "        if len(uniq) > 10:\n",
    "            print(\" ...\")\n",
    "        return uniq\n",
    "\n",
    "    print(f\"[LOCAL IMPORT] não encontrei '{package_dir_name}/__init__.py' em `/kaggle/input/*` (fora do dataset da competição).\")\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9503e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3 — Checagem de dependências (mostra tudo, não esconde erro)\n",
    "\n",
    "\n",
    "def _try_import(module_name: str) -> None:\n",
    "    try:\n",
    "        mod = __import__(module_name)\n",
    "        ver = getattr(mod, \"__version__\", None)\n",
    "        print(f\"[OK] import {module_name}\" + (f\" ({ver})\" if ver else \"\"))\n",
    "    except Exception:\n",
    "        print(f\"[ERRO] falha ao importar: {module_name}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "for pkg in [\n",
    "    \"numpy\",\n",
    "    \"torch\",\n",
    "    \"torchvision\",\n",
    "    \"albumentations\",\n",
    "    \"cv2\",\n",
    "    \"timm\",\n",
    "    \"segmentation_models_pytorch\",\n",
    "    \"scipy\",\n",
    "    \"sklearn\",\n",
    "]:\n",
    "    _try_import(pkg)\n",
    "\n",
    "print(\"\\nDica:\")\n",
    "print(\"- Se faltar `timm` / `segmentation_models_pytorch`, anexe um Dataset com `wheels/*.whl` e rode a Célula 2b.\")\n",
    "print(\"- Se uma lib for puro-Python e não tiver wheel, você pode vendorizá-la em um Dataset GitHub e usar `add_local_package_to_syspath()`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e31cbe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4 — Detectar dataset (Kaggle/local) e imprimir contagens\n",
    "\n",
    "\n",
    "def find_dataset_root() -> Path:\n",
    "    # Kaggle: padrão da competição\n",
    "    if is_kaggle():\n",
    "        base = Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "        if base.exists():\n",
    "            return base\n",
    "        # fallback: procurar algo que tenha a estrutura do dataset\n",
    "        kaggle_input = Path(\"/kaggle/input\")\n",
    "        if kaggle_input.exists():\n",
    "            for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "                if (ds / \"train_images\").exists() and (ds / \"test_images\").exists():\n",
    "                    return ds\n",
    "\n",
    "    # Local: respeita layout do repo (data/)\n",
    "    base = Path(\"data\").resolve()\n",
    "    if (base / \"train_images\").exists() and (base / \"test_images\").exists():\n",
    "        return base\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Não encontrei o dataset. No Kaggle, anexe o dataset da competição. Localmente, espere `data/train_images` etc.\"\n",
    "    )\n",
    "\n",
    "\n",
    "DATA_ROOT = find_dataset_root()\n",
    "TRAIN_IMAGES = DATA_ROOT / \"train_images\"\n",
    "TRAIN_MASKS = DATA_ROOT / \"train_masks\"\n",
    "TEST_IMAGES = DATA_ROOT / \"test_images\"\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"train_images/authentic:\", len(list((TRAIN_IMAGES / \"authentic\").glob(\"*.png\"))))\n",
    "print(\"train_images/forged:\", len(list((TRAIN_IMAGES / \"forged\").glob(\"*.png\"))))\n",
    "print(\"train_masks:\", len(list(TRAIN_MASKS.glob(\"*.npy\"))))\n",
    "print(\"test_images:\", len(list(TEST_IMAGES.glob('*.png'))))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
