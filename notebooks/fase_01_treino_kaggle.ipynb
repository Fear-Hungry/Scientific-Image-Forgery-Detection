{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Recod.ai/LUC — Training (Kaggle, internet ON)\n",
        "\n",
        "Este notebook gera os **pesos (`*.pth`)** necessários para rodar a submissão offline:\n",
        "\n",
        "- Segmentação (DINOv2 + decoder) → `outputs/models/r69.pth`\n",
        "- (Opcional) Classificador FFT → `outputs/models/fft_cls.pth`\n",
        "\n",
        "Fluxo recomendado:\n",
        "\n",
        "1. Kaggle Notebook **com internet ON** + **GPU**.\n",
        "2. Anexe o dataset da competição.\n",
        "3. Anexe um dataset com **este repo** (ou clone).\n",
        "4. Rode as células para treinar e salvar os checkpoints em `/kaggle/working/outputs/models/`.\n",
        "5. Empacote um folder `kaggle_bundle/` para criar um Kaggle Dataset com código + pesos.\n",
        "\n",
        "Observação: por regra do repo, a lógica nasce aqui (`.py`) e é espelhada no `.ipynb`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import platform\n",
        "import shutil\n",
        "import subprocess\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "\n",
        "print(f\"python={sys.version.split()[0]} platform={platform.platform()}\")\n",
        "print(f\"torch={torch.__version__} cuda_available={torch.cuda.is_available()}\")\n",
        "\n",
        "\n",
        "def _find_code_root() -> Path:\n",
        "    cwd = Path.cwd()\n",
        "    for p in [cwd, *cwd.parents]:\n",
        "        if (p / \"src\" / \"forgeryseg\").exists():\n",
        "            return p\n",
        "\n",
        "    kaggle_input = Path(\"/kaggle/input\")\n",
        "    if kaggle_input.exists():\n",
        "        for d in kaggle_input.iterdir():\n",
        "            if not d.is_dir():\n",
        "                continue\n",
        "            if (d / \"src\" / \"forgeryseg\").exists():\n",
        "                return d\n",
        "            # common: dataset root contains a single folder with the repo inside\n",
        "            try:\n",
        "                for child in d.iterdir():\n",
        "                    if child.is_dir() and (child / \"src\" / \"forgeryseg\").exists():\n",
        "                        return child\n",
        "            except PermissionError:\n",
        "                continue\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Não encontrei o código (src/forgeryseg). \"\n",
        "        \"No Kaggle: anexe um Dataset contendo este repo (com pastas src/ e configs/).\"\n",
        "    )\n",
        "\n",
        "\n",
        "CODE_ROOT = _find_code_root()\n",
        "SRC = CODE_ROOT / \"src\"\n",
        "CONFIG_ROOT = CODE_ROOT / \"configs\"\n",
        "print(f\"code_root={CODE_ROOT}\")\n",
        "\n",
        "if str(SRC) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# (Opcional) Instalar deps\n",
        "# -------------------------\n",
        "#\n",
        "# No Kaggle, normalmente já existe torch/torchvision. Se faltar timm/albumentations/etc,\n",
        "# use INSTALL_DEPS=True com internet ON.\n",
        "INSTALL_DEPS = False\n",
        "\n",
        "if INSTALL_DEPS:\n",
        "    req = CODE_ROOT / \"requirements-kaggle.txt\"\n",
        "    print(f\"Installing: {req}\")\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", \"-r\", str(req)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from forgeryseg.kaggle import package_kaggle_dataset\n",
        "from forgeryseg.submission import write_submission_csv\n",
        "from forgeryseg.training.dino_decoder import train_dino_decoder\n",
        "from forgeryseg.training.fft_classifier import train_fft_classifier\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Config (edite aqui)\n",
        "# -------------------------\n",
        "\n",
        "DATA_ROOT: Path | None = None  # None => auto-detect (Kaggle -> local)\n",
        "\n",
        "SEG_TRAIN_CONFIG = CONFIG_ROOT / \"dino_v3_518_r69.json\"\n",
        "FFT_TRAIN_CONFIG = CONFIG_ROOT / \"fft_classifier_logmag_256.json\"\n",
        "\n",
        "TRAIN_SEG = True\n",
        "TRAIN_FFT = True\n",
        "\n",
        "SEG_FOLDS = 1  # use 1 para gerar r69.pth diretamente; >1 cria r69_fold{i}.pth\n",
        "FFT_FOLDS = 1  # use 1 para gerar fft_cls.pth diretamente; >1 cria fft_cls_fold{i}.pth\n",
        "\n",
        "SEG_EPOCHS = 5\n",
        "SEG_BATCH = 4\n",
        "SEG_LR = 1e-3\n",
        "SEG_WD = 1e-4\n",
        "SEG_NUM_WORKERS = 2\n",
        "SEG_AUG = \"robust\"  # none | basic | robust (inclui rot90 + JPEG artifacts)\n",
        "SEG_SCHEDULER = \"cosine\"  # none | cosine | onecycle\n",
        "SEG_PATIENCE = 3  # early stopping em val_of1 (0 desliga)\n",
        "\n",
        "# CutMix (opcional, treino de segmentação)\n",
        "SEG_CUTMIX_PROB = 0.0  # 0 desliga; 0.3–0.7 costuma ser um bom range\n",
        "SEG_CUTMIX_ALPHA = 1.0\n",
        "\n",
        "FFT_EPOCHS = 5\n",
        "FFT_BATCH = 32\n",
        "FFT_LR = 1e-3\n",
        "FFT_WD = 1e-4\n",
        "FFT_NUM_WORKERS = 2\n",
        "FFT_SCHEDULER = \"cosine\"  # none | cosine | onecycle\n",
        "\n",
        "OUT_DIR = Path(\"/kaggle/working\") if Path(\"/kaggle/working\").exists() else Path(\"outputs\")\n",
        "OUT_MODELS = OUT_DIR / \"outputs\" / \"models\"\n",
        "\n",
        "SEG_OUT = OUT_MODELS / \"r69.pth\"\n",
        "FFT_OUT = OUT_MODELS / \"fft_cls.pth\"\n",
        "\n",
        "# (Opcional) checar score local rapidamente após treinar:\n",
        "EVAL_AFTER_TRAIN = True\n",
        "EVAL_SPLIT = \"train\"  # train | supplemental\n",
        "EVAL_LIMIT = 0  # 0 = sem limite (usa tudo)\n",
        "EVAL_CONFIG = CONFIG_ROOT / \"dino_v3_518_r69_fft_gate_tta_plus.json\"  # TTA mais forte (h/vflip + zoom in/out)\n",
        "\n",
        "# (Opcional) tunar pós-processamento (rápido) num subset de validação.\n",
        "#\n",
        "# Meta: aumentar mean_forged (e reduzir forg_pred_as_auth) sem destruir mean_authentic.\n",
        "#\n",
        "# Observação: este sweep roda por padrão em um subset (val_fraction) para ser viável no Kaggle.\n",
        "TUNE_POSTPROCESS = True\n",
        "TUNE_METHOD = \"optuna\"  # \"optuna\" (melhor) | \"grid\" (fallback sem Optuna)\n",
        "TUNE_CONFIG = EVAL_CONFIG\n",
        "TUNE_SPLIT = EVAL_SPLIT\n",
        "TUNE_VAL_FRACTION = 0.10\n",
        "TUNE_SEED = 42\n",
        "TUNE_LIMIT = 0  # 0 = usa todo o subset de validação\n",
        "TUNE_BATCH = 4  # ajuste conforme VRAM\n",
        "TUNE_USE_TTA = True  # mais fiel ao config (tende a dar melhor tuning, mas é mais lento)\n",
        "TUNE_THR_START = 0.20\n",
        "TUNE_THR_STOP = 0.60\n",
        "TUNE_THR_STEP = 0.05\n",
        "TUNE_WRITE_TUNED_CONFIG = True\n",
        "\n",
        "# Optuna (Bayesian Optimization)\n",
        "OPTUNA_TRIALS = 200\n",
        "OPTUNA_TIMEOUT_SEC = None  # ex.: 1800 (30 min)\n",
        "OPTUNA_OBJECTIVE = \"mean_score\"  # mean_score | mean_forged | combo\n",
        "\n",
        "# Empacotar um folder pronto para upload como Kaggle Dataset (offline):\n",
        "DO_PACKAGE = True\n",
        "PKG_OUT = OUT_DIR / \"kaggle_bundle\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def _find_recodai_root() -> Path:\n",
        "    if DATA_ROOT is not None:\n",
        "        return Path(DATA_ROOT)\n",
        "\n",
        "    kaggle_input = Path(\"/kaggle/input\")\n",
        "    if kaggle_input.exists():\n",
        "        for d in kaggle_input.iterdir():\n",
        "            if not d.is_dir():\n",
        "                continue\n",
        "            if (d / \"recodai\" / \"sample_submission.csv\").exists():\n",
        "                return d / \"recodai\"\n",
        "            if (d / \"sample_submission.csv\").exists() and (\n",
        "                (d / \"train_images\").exists() or (d / \"test_images\").exists()\n",
        "            ):\n",
        "                return d\n",
        "\n",
        "    local = Path(\"data/recodai\")\n",
        "    if local.exists():\n",
        "        return local\n",
        "    local2 = CODE_ROOT / \"data\" / \"recodai\"\n",
        "    if local2.exists():\n",
        "        return local2\n",
        "\n",
        "    raise FileNotFoundError(\n",
        "        \"Não encontrei o data root. Defina DATA_ROOT manualmente \"\n",
        "        \"(ex.: /kaggle/input/<dataset>/recodai ou data/recodai).\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Run\n",
        "# -------------------------\n",
        "\n",
        "data_root = _find_recodai_root()\n",
        "print(f\"data_root={data_root}\")\n",
        "\n",
        "device_str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device = torch.device(device_str)\n",
        "print(f\"device={device} (Dica: ative GPU em Settings -> Accelerator)\")\n",
        "\n",
        "OUT_MODELS.mkdir(parents=True, exist_ok=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----\n",
        "# Train (Segmentation)\n",
        "# -----\n",
        "\n",
        "seg_result = None\n",
        "if TRAIN_SEG:\n",
        "    seg_overrides: list[str] = []\n",
        "    if float(SEG_CUTMIX_PROB) > 0:\n",
        "        seg_overrides.extend(\n",
        "            [\n",
        "                f\"train.cutmix_prob={float(SEG_CUTMIX_PROB)}\",\n",
        "                f\"train.cutmix_alpha={float(SEG_CUTMIX_ALPHA)}\",\n",
        "            ]\n",
        "        )\n",
        "    seg_result = train_dino_decoder(\n",
        "        config_path=SEG_TRAIN_CONFIG,\n",
        "        data_root=data_root,\n",
        "        out_path=SEG_OUT,\n",
        "        device=device_str,\n",
        "        split=\"train\",\n",
        "        overrides=seg_overrides if seg_overrides else None,\n",
        "        epochs=int(SEG_EPOCHS),\n",
        "        batch_size=int(SEG_BATCH),\n",
        "        lr=float(SEG_LR),\n",
        "        weight_decay=float(SEG_WD),\n",
        "        num_workers=int(SEG_NUM_WORKERS),\n",
        "        folds=int(SEG_FOLDS),\n",
        "        fold=None,\n",
        "        aug=SEG_AUG,  # type: ignore[arg-type]\n",
        "        scheduler=SEG_SCHEDULER,  # type: ignore[arg-type]\n",
        "        patience=int(SEG_PATIENCE),\n",
        "    )\n",
        "\n",
        "    # Se treinou k-fold, copia o melhor fold para o path \"base\" (r69.pth),\n",
        "    # para facilitar o uso em configs que apontam para outputs/models/r69.pth.\n",
        "    if seg_result is not None and int(SEG_FOLDS) > 1:\n",
        "        best = max(seg_result.fold_results, key=lambda fr: fr.best_val_of1)\n",
        "        if best.checkpoint_path != SEG_OUT:\n",
        "            SEG_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(best.checkpoint_path, SEG_OUT)\n",
        "            print(f\"Copied best fold checkpoint -> {SEG_OUT} (from {best.checkpoint_path})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -----\n",
        "# Train (FFT classifier)\n",
        "# -----\n",
        "\n",
        "fft_saved = None\n",
        "if TRAIN_FFT:\n",
        "    fft_saved = train_fft_classifier(\n",
        "        config_path=FFT_TRAIN_CONFIG,\n",
        "        data_root=data_root,\n",
        "        out_path=FFT_OUT,\n",
        "        device=device,\n",
        "        epochs=int(FFT_EPOCHS),\n",
        "        batch_size=int(FFT_BATCH),\n",
        "        lr=float(FFT_LR),\n",
        "        weight_decay=float(FFT_WD),\n",
        "        num_workers=int(FFT_NUM_WORKERS),\n",
        "        folds=int(FFT_FOLDS),\n",
        "        scheduler=FFT_SCHEDULER,  # type: ignore[arg-type]\n",
        "    )\n",
        "\n",
        "    if fft_saved and int(FFT_FOLDS) > 1:\n",
        "        # escolhe melhor fold por menor val_loss no checkpoint\n",
        "        best_path = min(\n",
        "            fft_saved,\n",
        "            key=lambda p: float(torch.load(p, map_location=\"cpu\").get(\"val_loss\", float(\"inf\"))),\n",
        "        )\n",
        "        if best_path != FFT_OUT:\n",
        "            FFT_OUT.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(best_path, FFT_OUT)\n",
        "            print(f\"Copied best FFT fold -> {FFT_OUT} (from {best_path})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Quick evaluation (local)\n",
        "# -------------------------\n",
        "#\n",
        "# Gera um submission no split train/supplemental e calcula oF1 local.\n",
        "\n",
        "if EVAL_AFTER_TRAIN:\n",
        "    from forgeryseg.eval import score_submission_csv, validate_submission_format\n",
        "\n",
        "    eval_csv = OUT_DIR / f\"submission_{EVAL_SPLIT}.csv\"\n",
        "\n",
        "    stats = write_submission_csv(\n",
        "        config_path=EVAL_CONFIG,\n",
        "        data_root=data_root,\n",
        "        split=EVAL_SPLIT,  # type: ignore[arg-type]\n",
        "        out_path=eval_csv,\n",
        "        device=device,\n",
        "        limit=int(EVAL_LIMIT),\n",
        "        path_roots=[OUT_DIR, CODE_ROOT, CONFIG_ROOT],\n",
        "    )\n",
        "    print(stats)\n",
        "\n",
        "    fmt = validate_submission_format(eval_csv, data_root=data_root, split=EVAL_SPLIT)  # type: ignore[arg-type]\n",
        "    print(\"\\n[Format check]\")\n",
        "    print(json.dumps(fmt, indent=2, ensure_ascii=False))\n",
        "\n",
        "    score = score_submission_csv(eval_csv, data_root=data_root, split=EVAL_SPLIT)  # type: ignore[arg-type]\n",
        "    print(\"\\n[Local score]\")\n",
        "    print(json.dumps(score.as_dict(csv_path=eval_csv, split=EVAL_SPLIT), indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Postprocess tuning (threshold / optuna)\n",
        "# -------------------------\n",
        "#\n",
        "# Faz tuning do pós-processamento em um subset de validação estratificado (authentic vs forged):\n",
        "# - método \"optuna\": otimização bayesiana (melhor, tunando vários parâmetros)\n",
        "# - método \"grid\": sweep simples só em `prob_threshold` (fallback)\n",
        "\n",
        "if TUNE_POSTPROCESS:\n",
        "    import dataclasses\n",
        "    import math\n",
        "    import time\n",
        "\n",
        "    import numpy as np\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    from forgeryseg.config import apply_overrides, load_config_data\n",
        "    from forgeryseg.dataset import list_cases, load_mask_instances\n",
        "    from forgeryseg.eval import ScoreSummary\n",
        "    from forgeryseg.inference import load_rgb\n",
        "    from forgeryseg.inference_engine import InferenceEngine\n",
        "    from forgeryseg.metric import of1_score\n",
        "    from forgeryseg.postprocess import PostprocessParams, postprocess_prob\n",
        "    from forgeryseg.training.utils import stratified_splits\n",
        "\n",
        "    tuned_config_path = None\n",
        "\n",
        "    def _frange(start: float, stop: float, step: float) -> list[float]:\n",
        "        if step <= 0:\n",
        "            raise ValueError(\"step must be > 0\")\n",
        "        out: list[float] = []\n",
        "        x = float(start)\n",
        "        while x <= float(stop) + 1e-12:\n",
        "            out.append(float(x))\n",
        "            x += float(step)\n",
        "        return out\n",
        "\n",
        "    def _select_stratified_subset(cases: list, *, seed: int, limit: int) -> list:\n",
        "        if limit <= 0 or len(cases) <= limit:\n",
        "            return cases\n",
        "        rng = np.random.default_rng(int(seed))\n",
        "        idx_auth = [i for i, c in enumerate(cases) if c.mask_path is None]\n",
        "        idx_forg = [i for i, c in enumerate(cases) if c.mask_path is not None]\n",
        "        p_forg = len(idx_forg) / max(1, len(cases))\n",
        "        n_forg = int(round(limit * p_forg))\n",
        "        n_forg = min(n_forg, len(idx_forg))\n",
        "        n_auth = int(limit - n_forg)\n",
        "        n_auth = min(n_auth, len(idx_auth))\n",
        "        chosen = []\n",
        "        if n_auth > 0:\n",
        "            chosen.extend(rng.choice(idx_auth, size=n_auth, replace=False).tolist())\n",
        "        if n_forg > 0:\n",
        "            chosen.extend(rng.choice(idx_forg, size=n_forg, replace=False).tolist())\n",
        "        rng.shuffle(chosen)\n",
        "        return [cases[i] for i in chosen]\n",
        "\n",
        "    # Overrides sugeridos (relaxar filtros agressivos)\n",
        "    base_overrides = [\n",
        "        \"inference.fft_gate.enabled=false\",\n",
        "        \"inference.postprocess.min_prob_std=0.0\",\n",
        "        \"inference.postprocess.small_area=null\",\n",
        "        \"inference.postprocess.small_min_mean_conf=null\",\n",
        "        \"inference.postprocess.authentic_area_max=null\",\n",
        "        \"inference.postprocess.authentic_conf_max=null\",\n",
        "        \"inference.postprocess.min_area=32\",\n",
        "        \"inference.postprocess.open_kernel=0\",\n",
        "        \"inference.postprocess.close_kernel=0\",\n",
        "        \"inference.postprocess.gaussian_sigma=0.0\",\n",
        "        \"inference.postprocess.sobel_weight=0.0\",\n",
        "    ]\n",
        "\n",
        "    if str(TUNE_METHOD).lower() == \"optuna\":\n",
        "        try:\n",
        "            from forgeryseg.tuning import tune_postprocess_optuna\n",
        "\n",
        "            optuna_out = OUT_DIR / \"optuna\"\n",
        "            res = tune_postprocess_optuna(\n",
        "                config_path=TUNE_CONFIG,\n",
        "                data_root=data_root,\n",
        "                split=TUNE_SPLIT,  # type: ignore[arg-type]\n",
        "                out_dir=optuna_out,\n",
        "                device=device,\n",
        "                base_overrides=base_overrides,\n",
        "                val_fraction=float(TUNE_VAL_FRACTION),\n",
        "                seed=int(TUNE_SEED),\n",
        "                limit=int(TUNE_LIMIT),\n",
        "                use_tta=bool(TUNE_USE_TTA),\n",
        "                batch_size=int(TUNE_BATCH),\n",
        "                n_trials=int(OPTUNA_TRIALS),\n",
        "                timeout_sec=OPTUNA_TIMEOUT_SEC,\n",
        "                objective=OPTUNA_OBJECTIVE,  # type: ignore[arg-type]\n",
        "            )\n",
        "            tuned_config_path = res.tuned_config_path\n",
        "        except ImportError as e:\n",
        "            print(f\"[warn] Optuna não disponível ({type(e).__name__}: {e}). Caindo para grid sweep.\")\n",
        "            TUNE_METHOD = \"grid\"\n",
        "\n",
        "    if str(TUNE_METHOD).lower() == \"grid\":\n",
        "        thresholds = _frange(float(TUNE_THR_START), float(TUNE_THR_STOP), float(TUNE_THR_STEP))\n",
        "        if not thresholds:\n",
        "            raise RuntimeError(\"No thresholds configured\")\n",
        "\n",
        "        print(\n",
        "            f\"[tune:grid] split={TUNE_SPLIT} val_fraction={TUNE_VAL_FRACTION} \"\n",
        "            f\"n_thresholds={len(thresholds)} use_tta={TUNE_USE_TTA} batch={TUNE_BATCH}\"\n",
        "        )\n",
        "\n",
        "        # Load engine once (model + input_size). Postprocess will be replaced per-threshold.\n",
        "        engine = InferenceEngine.from_config(\n",
        "            config_path=TUNE_CONFIG,\n",
        "            device=device,\n",
        "            overrides=base_overrides,\n",
        "            path_roots=[OUT_DIR, CODE_ROOT, CONFIG_ROOT],\n",
        "            amp=True,\n",
        "        )\n",
        "\n",
        "        # Define validation subset (stratified).\n",
        "        all_cases = list_cases(data_root, TUNE_SPLIT, include_authentic=True, include_forged=True)\n",
        "        labels = np.asarray([1 if c.mask_path is not None else 0 for c in all_cases], dtype=np.int64)\n",
        "        split_iter = stratified_splits(\n",
        "            labels,\n",
        "            folds=1,\n",
        "            val_fraction=float(TUNE_VAL_FRACTION),\n",
        "            seed=int(TUNE_SEED),\n",
        "        )\n",
        "        _, _, val_idx = next(iter(split_iter))\n",
        "        val_cases = [all_cases[int(i)] for i in val_idx.tolist()]\n",
        "        val_cases = _select_stratified_subset(val_cases, seed=int(TUNE_SEED), limit=int(TUNE_LIMIT))\n",
        "        n_auth = sum(1 for c in val_cases if c.mask_path is None)\n",
        "        n_forg = sum(1 for c in val_cases if c.mask_path is not None)\n",
        "        print(f\"[tune:grid] val_cases={len(val_cases)} authentic={n_auth} forged={n_forg}\")\n",
        "\n",
        "        # Fixed postprocess (with overrides applied), except prob_threshold.\n",
        "        base_post: PostprocessParams = engine.postprocess\n",
        "\n",
        "        # Accumulators per threshold\n",
        "        acc: dict[float, dict[str, float | int]] = {\n",
        "            thr: {\n",
        "                \"sum_all\": 0.0,\n",
        "                \"sum_auth\": 0.0,\n",
        "                \"sum_forg\": 0.0,\n",
        "                \"n_all\": 0,\n",
        "                \"n_auth\": 0,\n",
        "                \"n_forg\": 0,\n",
        "                \"auth_pred_as_forged\": 0,\n",
        "                \"forg_pred_as_auth\": 0,\n",
        "            }\n",
        "            for thr in thresholds\n",
        "        }\n",
        "\n",
        "        def _predict_prob_maps_no_tta(images: list[np.ndarray]) -> list[np.ndarray]:\n",
        "            from forgeryseg.image import letterbox_reflect, unletterbox\n",
        "\n",
        "            padded = []\n",
        "            metas = []\n",
        "            for img in images:\n",
        "                pad, meta = letterbox_reflect(img, int(engine.input_size))\n",
        "                padded.append(pad)\n",
        "                metas.append(meta)\n",
        "\n",
        "            x = torch.stack(\n",
        "                [torch.from_numpy(im).permute(2, 0, 1).contiguous().float() / 255.0 for im in padded],\n",
        "                dim=0,\n",
        "            ).to(engine.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                if engine.amp and engine.device.type == \"cuda\":\n",
        "                    with torch.autocast(device_type=\"cuda\", dtype=torch.float16):\n",
        "                        logits = engine.model(x)\n",
        "                else:\n",
        "                    logits = engine.model(x)\n",
        "                prob = torch.sigmoid(logits)[:, 0].float()\n",
        "\n",
        "            prob_np = prob.detach().cpu().numpy().astype(np.float32)\n",
        "            return [unletterbox(prob_np[i], metas[i]).astype(np.float32) for i in range(len(metas))]\n",
        "\n",
        "        t0 = time.time()\n",
        "        bs = int(max(1, TUNE_BATCH))\n",
        "        for i in tqdm(range(0, len(val_cases), bs), desc=\"[tune:grid] infer\"):\n",
        "            batch_cases = val_cases[i : i + bs]\n",
        "            images = [load_rgb(c.image_path) for c in batch_cases]\n",
        "\n",
        "            if bool(TUNE_USE_TTA):\n",
        "                probs = engine._predict_prob_maps_batched(images)\n",
        "            else:\n",
        "                probs = _predict_prob_maps_no_tta(images)\n",
        "\n",
        "            for case, prob in zip(batch_cases, probs, strict=True):\n",
        "                gt_instances = [] if case.mask_path is None else load_mask_instances(case.mask_path)\n",
        "                gt_is_auth = case.mask_path is None\n",
        "\n",
        "                for thr in thresholds:\n",
        "                    post = dataclasses.replace(base_post, prob_threshold=float(thr))\n",
        "                    pred_instances = postprocess_prob(prob, post)\n",
        "                    pred_is_auth = len(pred_instances) == 0\n",
        "\n",
        "                    if gt_is_auth:\n",
        "                        s = 1.0 if pred_is_auth else 0.0\n",
        "                        if not pred_is_auth:\n",
        "                            acc[thr][\"auth_pred_as_forged\"] = int(acc[thr][\"auth_pred_as_forged\"]) + 1\n",
        "                        acc[thr][\"sum_auth\"] = float(acc[thr][\"sum_auth\"]) + float(s)\n",
        "                        acc[thr][\"n_auth\"] = int(acc[thr][\"n_auth\"]) + 1\n",
        "                    else:\n",
        "                        if pred_is_auth:\n",
        "                            acc[thr][\"forg_pred_as_auth\"] = int(acc[thr][\"forg_pred_as_auth\"]) + 1\n",
        "                            s = 0.0\n",
        "                        else:\n",
        "                            s = float(of1_score(pred_instances, gt_instances))\n",
        "                        acc[thr][\"sum_forg\"] = float(acc[thr][\"sum_forg\"]) + float(s)\n",
        "                        acc[thr][\"n_forg\"] = int(acc[thr][\"n_forg\"]) + 1\n",
        "\n",
        "                    acc[thr][\"sum_all\"] = float(acc[thr][\"sum_all\"]) + float(s)\n",
        "                    acc[thr][\"n_all\"] = int(acc[thr][\"n_all\"]) + 1\n",
        "\n",
        "        dt = time.time() - t0\n",
        "        print(f\"[tune:grid] done in {dt:.1f}s\")\n",
        "\n",
        "        best_thr = None\n",
        "        best_score = -math.inf\n",
        "        results: list[tuple[float, ScoreSummary]] = []\n",
        "        for thr in thresholds:\n",
        "            a = acc[thr]\n",
        "            n_all = int(a[\"n_all\"])\n",
        "            n_auth = int(a[\"n_auth\"])\n",
        "            n_forg = int(a[\"n_forg\"])\n",
        "            mean_all = float(a[\"sum_all\"]) / max(1, n_all)\n",
        "            mean_auth = float(a[\"sum_auth\"]) / max(1, n_auth)\n",
        "            mean_forg = float(a[\"sum_forg\"]) / max(1, n_forg)\n",
        "            summary = ScoreSummary(\n",
        "                mean_score=mean_all,\n",
        "                mean_authentic=mean_auth,\n",
        "                mean_forged=mean_forg,\n",
        "                n_cases=n_all,\n",
        "                n_authentic=n_auth,\n",
        "                n_forged=n_forg,\n",
        "                auth_pred_as_forged=int(a[\"auth_pred_as_forged\"]),\n",
        "                forg_pred_as_auth=int(a[\"forg_pred_as_auth\"]),\n",
        "                decode_errors_scoring=0,\n",
        "            )\n",
        "            results.append((thr, summary))\n",
        "            if mean_all > best_score:\n",
        "                best_score = mean_all\n",
        "                best_thr = thr\n",
        "\n",
        "        assert best_thr is not None\n",
        "        results.sort(key=lambda x: x[0])\n",
        "        print(\"\\n[tune:grid] Results (val subset):\")\n",
        "        for thr, s in results:\n",
        "            print(\n",
        "                f\"thr={thr:.2f} mean={s.mean_score:.4f} mean_forged={s.mean_forged:.4f} \"\n",
        "                f\"auth_pred_as_forged={s.auth_pred_as_forged} forg_pred_as_auth={s.forg_pred_as_auth}\"\n",
        "            )\n",
        "\n",
        "        best_summary = dict(results)[best_thr]\n",
        "        best_overrides = list(base_overrides) + [f\"inference.postprocess.prob_threshold={best_thr}\"]\n",
        "        print(\n",
        "            f\"\\n[tune:grid] BEST thr={best_thr:.2f} mean={best_summary.mean_score:.4f} \"\n",
        "            f\"mean_forged={best_summary.mean_forged:.4f}\"\n",
        "        )\n",
        "        print(\"[tune:grid] Suggested overrides:\")\n",
        "        print(json.dumps(best_overrides, indent=2, ensure_ascii=False))\n",
        "\n",
        "        tuned_path = OUT_DIR / \"tuned_postprocess.json\"\n",
        "        tuned_path.write_text(\n",
        "            json.dumps(\n",
        "                {\n",
        "                    \"config\": str(TUNE_CONFIG),\n",
        "                    \"split\": str(TUNE_SPLIT),\n",
        "                    \"val_fraction\": float(TUNE_VAL_FRACTION),\n",
        "                    \"seed\": int(TUNE_SEED),\n",
        "                    \"limit\": int(TUNE_LIMIT),\n",
        "                    \"best_threshold\": float(best_thr),\n",
        "                    \"best_summary\": best_summary.as_dict(),\n",
        "                    \"overrides\": best_overrides,\n",
        "                },\n",
        "                indent=2,\n",
        "                ensure_ascii=False,\n",
        "            )\n",
        "            + \"\\n\",\n",
        "            encoding=\"utf-8\",\n",
        "        )\n",
        "        print(f\"[tune:grid] Wrote {tuned_path}\")\n",
        "\n",
        "        if TUNE_WRITE_TUNED_CONFIG:\n",
        "\n",
        "            def _slug_float(x: float) -> str:\n",
        "                s = f\"{float(x):.4f}\".rstrip(\"0\").rstrip(\".\")\n",
        "                return s.replace(\".\", \"p\")\n",
        "\n",
        "            tuned_cfg = load_config_data(TUNE_CONFIG)\n",
        "            tuned_cfg = apply_overrides(tuned_cfg, best_overrides)\n",
        "            tuned_config_path = OUT_DIR / f\"tuned_{Path(TUNE_CONFIG).stem}_thr{_slug_float(best_thr)}.json\"\n",
        "            tuned_config_path.write_text(json.dumps(tuned_cfg, indent=2, ensure_ascii=False) + \"\\n\", encoding=\"utf-8\")\n",
        "            print(f\"[tune:grid] Wrote {tuned_config_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# -------------------------\n",
        "# Package Kaggle Dataset folder\n",
        "# -------------------------\n",
        "#\n",
        "# Cria um folder pronto para upload como Kaggle Dataset:\n",
        "# - código (src/scripts/configs/notebooks/docs)\n",
        "# - + `outputs/models/*.pth` (opcional)\n",
        "#\n",
        "# Depois, anexe esse dataset no notebook de submissão (internet OFF).\n",
        "\n",
        "if DO_PACKAGE:\n",
        "    out_root = package_kaggle_dataset(\n",
        "        out_dir=PKG_OUT,\n",
        "        include_models=True,\n",
        "        models_dir=OUT_MODELS,\n",
        "        repo_root=CODE_ROOT,\n",
        "    )\n",
        "    print(f\"Wrote Kaggle bundle at: {out_root.resolve()}\")\n",
        "\n",
        "    if TUNE_POSTPROCESS and \"tuned_config_path\" in globals():\n",
        "        p = globals().get(\"tuned_config_path\")\n",
        "        if isinstance(p, Path) and p.exists():\n",
        "            dst = out_root / \"configs\" / p.name\n",
        "            dst.parent.mkdir(parents=True, exist_ok=True)\n",
        "            shutil.copy2(p, dst)\n",
        "            print(f\"Copied tuned config into bundle: {dst}\")\n",
        "\n",
        "    print(\"Crie um Kaggle Dataset a partir desse folder e anexe no notebook de submissão offline.\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
