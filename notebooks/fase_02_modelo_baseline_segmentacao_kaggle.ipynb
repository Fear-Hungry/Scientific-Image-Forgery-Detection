{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc1b3fcb",
   "metadata": {},
   "source": [
    "Fase 2 — Baseline de Segmentação (Kaggle / Notebook-only)\n",
    "\n",
    "Objetivo desta etapa:\n",
    "- Preparar o dataset (carregar imagens + máscaras)\n",
    "- Padronizar entrada (resize + pad) e binarizar máscaras\n",
    "- Definir augmentations consistentes (imagem + máscara + área válida)\n",
    "- Criar K folds (K=5) para validação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb22b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 1 — Regras do Kaggle (sanidade)\n",
    "print(\"Kaggle submission constraints (lembrete):\")\n",
    "print(\"- Submissions via Notebook\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF\")\n",
    "print(\"- Output: submission.csv ou submission.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad1e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 2 — Imports + ambiente\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as A\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"opencv:\", cv2.__version__)\n",
    "print(\"albumentations:\", A.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccb5930",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Celula 3 — Paths + seed\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "\n",
    "DATA_ROOT = Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\").resolve()\n",
    "TRAIN_IMAGES = DATA_ROOT / \"train_images\"\n",
    "TRAIN_MASKS = DATA_ROOT / \"train_masks\"\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"Train images dir:\", TRAIN_IMAGES)\n",
    "print(\"Train masks dir:\", TRAIN_MASKS)\n",
    "print(\"Train/authentic:\", len(list((TRAIN_IMAGES / \"authentic\").glob(\"*.png\"))))\n",
    "print(\"Train/forged:\", len(list((TRAIN_IMAGES / \"forged\").glob(\"*.png\"))))\n",
    "print(\"Masks:\", len(list(TRAIN_MASKS.glob(\"*.npy\"))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d10b941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 4 — Index do dataset (DataFrame principal)\n",
    "@dataclass(frozen=True)\n",
    "class Sample:\n",
    "    case_id: str\n",
    "    label: str  # authentic/forged\n",
    "    img_path: Path\n",
    "    mask_path: Path | None\n",
    "\n",
    "\n",
    "def build_train_index(train_images_dir: Path, train_masks_dir: Path) -> pd.DataFrame:\n",
    "    rows: list[dict] = []\n",
    "    for label in [\"authentic\", \"forged\"]:\n",
    "        for img_path in sorted((train_images_dir / label).glob(\"*.png\")):\n",
    "            case_id = img_path.stem\n",
    "            mask_path = train_masks_dir / f\"{case_id}.npy\" if label == \"forged\" else None\n",
    "            file_size = img_path.stat().st_size\n",
    "\n",
    "            width = height = None\n",
    "            mode = None\n",
    "            read_error = None\n",
    "            try:\n",
    "                with Image.open(img_path) as img:\n",
    "                    width, height = img.size\n",
    "                    mode = img.mode\n",
    "            except Exception as exc:\n",
    "                read_error = f\"{type(exc).__name__}: {exc}\"\n",
    "\n",
    "            mask_instances = 0\n",
    "            mask_area = 0\n",
    "            mask_area_frac = 0.0\n",
    "            if mask_path is not None:\n",
    "                mask = np.load(mask_path)\n",
    "                if mask.ndim == 2:\n",
    "                    mask = mask[None, ...]\n",
    "                mask_instances = int(mask.shape[0])\n",
    "                union = mask.max(axis=0)\n",
    "                mask_area = int((union > 0).sum())\n",
    "                if width is not None and height is not None:\n",
    "                    mask_area_frac = mask_area / float(width * height)\n",
    "                else:\n",
    "                    mask_area_frac = np.nan\n",
    "\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"case_id\": case_id,\n",
    "                    \"split\": \"train\",\n",
    "                    \"label\": label,\n",
    "                    \"img_path\": str(img_path),\n",
    "                    \"mask_path\": None if mask_path is None else str(mask_path),\n",
    "                    \"width\": width,\n",
    "                    \"height\": height,\n",
    "                    \"mode\": mode,\n",
    "                    \"file_size\": file_size,\n",
    "                    \"mask_instances\": mask_instances,\n",
    "                    \"mask_area\": mask_area,\n",
    "                    \"mask_area_frac\": mask_area_frac,\n",
    "                    \"read_error\": read_error,\n",
    "                }\n",
    "            )\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "df = build_train_index(TRAIN_IMAGES, TRAIN_MASKS)\n",
    "display(df.head())\n",
    "print(\"Rows:\", len(df))\n",
    "display(df[\"label\"].value_counts())\n",
    "print(\"Read errors:\", int(df[\"read_error\"].notna().sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da31aee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 5 — Estatísticas rápidas (tamanho / área de máscara)\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.scatter(df[\"width\"], df[\"height\"], s=4, alpha=0.3)\n",
    "plt.title(\"Image sizes (width x height)\")\n",
    "plt.xlabel(\"width\")\n",
    "plt.ylabel(\"height\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df[df[\"label\"] == \"forged\"][\"mask_area_frac\"].hist(bins=40)\n",
    "plt.title(\"Mask area fraction (forged)\")\n",
    "plt.xlabel(\"mask_area_frac\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1523fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 6 — Preprocess (resize+pad para 512x512) + máscara binária\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "TARGET_SIZE = 512\n",
    "\n",
    "\n",
    "def load_image_rgb(path: str | Path) -> np.ndarray:\n",
    "    path = Path(path)\n",
    "    with Image.open(path) as img:\n",
    "        img = img.convert(\"RGB\")\n",
    "        return np.array(img)\n",
    "\n",
    "\n",
    "def load_mask_union(path: str | Path | None, shape_hw: tuple[int, int]) -> np.ndarray:\n",
    "    if path is None:\n",
    "        return np.zeros(shape_hw, dtype=np.uint8)\n",
    "    masks = np.load(Path(path))\n",
    "    if masks.ndim == 2:\n",
    "        union = masks\n",
    "    else:\n",
    "        union = masks.max(axis=0)\n",
    "    union = (union > 0).astype(np.uint8)\n",
    "    if union.shape != shape_hw:\n",
    "        raise ValueError(f\"Mask shape {union.shape} does not match image shape {shape_hw}\")\n",
    "    return union\n",
    "\n",
    "\n",
    "def resize_pad_to_square(\n",
    "    image: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    size: int = TARGET_SIZE,\n",
    ") -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    h, w = image.shape[:2]\n",
    "    scale = size / float(max(h, w))\n",
    "    new_w = int(round(w * scale))\n",
    "    new_h = int(round(h * scale))\n",
    "    image_rs = cv2.resize(image, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    mask_rs = cv2.resize(mask, (new_w, new_h), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "    pad_h = size - new_h\n",
    "    pad_w = size - new_w\n",
    "    image_pad = np.pad(image_rs, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"constant\")\n",
    "    mask_pad = np.pad(mask_rs, ((0, pad_h), (0, pad_w)), mode=\"constant\")\n",
    "    valid = np.zeros((size, size), dtype=np.uint8)\n",
    "    valid[:new_h, :new_w] = 1\n",
    "    return image_pad, mask_pad, valid\n",
    "\n",
    "\n",
    "row = df[df[\"label\"] == \"forged\"].iloc[0]\n",
    "img = load_image_rgb(row[\"img_path\"])\n",
    "mask = load_mask_union(row[\"mask_path\"], img.shape[:2])\n",
    "img_p, mask_p, valid = resize_pad_to_square(img, mask, size=TARGET_SIZE)\n",
    "print(\"Original:\", img.shape, \"Processed:\", img_p.shape, \"mask sum:\", int(mask.sum()), \"proc mask sum:\", int(mask_p.sum()))\n",
    "\n",
    "\n",
    "def overlay_mask(image: np.ndarray, mask: np.ndarray, color=(255, 0, 0), alpha: float = 0.45) -> np.ndarray:\n",
    "    mask = mask.astype(bool)\n",
    "    out = image.copy().astype(np.float32)\n",
    "    out[mask] = (1 - alpha) * out[mask] + alpha * np.array(color, dtype=np.float32)\n",
    "    return out.astype(np.uint8)\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"original\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(overlay_mask(img, mask))\n",
    "axes[1].set_title(\"original + mask\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(overlay_mask(img_p, mask_p))\n",
    "axes[2].set_title(\"processed 512 + mask\")\n",
    "axes[2].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841e5f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 7 — Augmentations (imagem + máscara + valid_mask) + preview\n",
    "train_aug = A.Compose(\n",
    "    [\n",
    "        # Random crop + resize (mantem saida 512x512)\n",
    "        A.RandomResizedCrop(\n",
    "            size=(TARGET_SIZE, TARGET_SIZE),\n",
    "            scale=(0.6, 1.0),\n",
    "            ratio=(0.75, 1.3333333333333333),\n",
    "            interpolation=cv2.INTER_AREA,\n",
    "            mask_interpolation=cv2.INTER_NEAREST,\n",
    "            p=0.5,\n",
    "        ),\n",
    "        # Geometria (flips + rotacao 0-360 + zoom in/out)\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.Affine(\n",
    "            scale=(0.75, 1.25),\n",
    "            translate_percent=(-0.1, 0.1),\n",
    "            rotate=(-180, 180),\n",
    "            shear=(-8, 8),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            mask_interpolation=cv2.INTER_NEAREST,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            fill=0,\n",
    "            fill_mask=0,\n",
    "            p=0.8,\n",
    "        ),\n",
    "        # Deformacoes locais leves\n",
    "        A.ElasticTransform(\n",
    "            alpha=1.0,\n",
    "            sigma=30.0,\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            mask_interpolation=cv2.INTER_NEAREST,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            fill=0,\n",
    "            fill_mask=0,\n",
    "            p=0.15,\n",
    "        ),\n",
    "        A.GridDistortion(\n",
    "            num_steps=5,\n",
    "            distort_limit=(-0.03, 0.03),\n",
    "            interpolation=cv2.INTER_LINEAR,\n",
    "            mask_interpolation=cv2.INTER_NEAREST,\n",
    "            border_mode=cv2.BORDER_CONSTANT,\n",
    "            fill=0,\n",
    "            fill_mask=0,\n",
    "            p=0.1,\n",
    "        ),\n",
    "        # Cor/contraste/ruido (robustez a intensidades e artefatos)\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "                A.ColorJitter(\n",
    "                    brightness=(0.8, 1.2),\n",
    "                    contrast=(0.8, 1.2),\n",
    "                    saturation=(0.8, 1.2),\n",
    "                    hue=(-0.05, 0.05),\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "            ],\n",
    "            p=0.5,\n",
    "        ),\n",
    "        A.OneOf(\n",
    "            [\n",
    "                A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
    "                A.MedianBlur(blur_limit=(3, 7), p=1.0),\n",
    "                A.MotionBlur(blur_limit=(3, 7), p=1.0),\n",
    "            ],\n",
    "            p=0.25,\n",
    "        ),\n",
    "        A.GaussNoise(std_range=(0.01, 0.08), p=0.2),\n",
    "        A.ImageCompression(quality_range=(60, 100), p=0.2),\n",
    "        # Cutout: aplica na imagem, mas NAO altera mask/valid (fill_mask=None)\n",
    "        A.CoarseDropout(\n",
    "            num_holes_range=(1, 6),\n",
    "            hole_height_range=(0.05, 0.25),\n",
    "            hole_width_range=(0.05, 0.25),\n",
    "            fill=0,\n",
    "            fill_mask=None,\n",
    "            p=0.3,\n",
    "        ),\n",
    "    ],\n",
    "    additional_targets={\"valid\": \"mask\"},\n",
    ")\n",
    "\n",
    "aug = train_aug(image=img_p, mask=mask_p, valid=valid)\n",
    "img_a = aug[\"image\"]\n",
    "mask_a = aug[\"mask\"]\n",
    "valid_a = aug[\"valid\"]\n",
    "print(\"Augmented shapes:\", img_a.shape, mask_a.shape, valid_a.shape)\n",
    "print(\"Aug config: aggressive geometric + color + local distort + cutout\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(14, 4))\n",
    "axes[0].imshow(overlay_mask(img_p, mask_p))\n",
    "axes[0].set_title(\"processed + mask\")\n",
    "axes[0].axis(\"off\")\n",
    "axes[1].imshow(overlay_mask(img_a, mask_a))\n",
    "axes[1].set_title(\"augmented + mask\")\n",
    "axes[1].axis(\"off\")\n",
    "axes[2].imshow(overlay_mask(img_a, (1 - valid_a).astype(np.uint8), color=(0, 255, 255)))\n",
    "axes[2].set_title(\"augmented padding area\")\n",
    "axes[2].axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8170d83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 8 — K-fold (5 folds) sem sklearn (estratificado por label + bins de area)\n",
    "AREA_BINS = [0.0, 0.001, 0.005, 0.01, 0.05, 0.2, 1.0]\n",
    "\n",
    "df[\"area_bin\"] = pd.cut(df[\"mask_area_frac\"].fillna(0), bins=AREA_BINS, include_lowest=True, labels=False)\n",
    "df[\"area_bin\"] = df[\"area_bin\"].fillna(0).astype(int)\n",
    "df[\"stratify_key\"] = df[\"label\"].astype(str) + \"_b\" + df[\"area_bin\"].astype(str)\n",
    "df[\"fold\"] = -1\n",
    "\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "for key, idxs in df.groupby(\"stratify_key\").indices.items():\n",
    "    idxs = np.array(list(idxs), dtype=int)\n",
    "    rng.shuffle(idxs)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        df.loc[idx, \"fold\"] = int(i % 5)\n",
    "\n",
    "assert int((df[\"fold\"] < 0).sum()) == 0\n",
    "fold_counts = pd.crosstab(df[\"fold\"], df[\"label\"])\n",
    "display(fold_counts)\n",
    "print(\"Fold sizes:\", df[\"fold\"].value_counts().sort_index().to_dict())\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "df[df[\"label\"] == \"forged\"].groupby(\"fold\")[\"mask_area_frac\"].mean().plot(kind=\"bar\")\n",
    "plt.title(\"Mean mask_area_frac (forged) por fold\")\n",
    "plt.ylabel(\"mean mask_area_frac\")\n",
    "plt.show()"
   ]
  }
  ,
  {
   "cell_type": "markdown",
   "id": "d2d8b3a1",
   "metadata": {},
   "source": [
    "## Arquitetura do Modelo de Segmenta\\u00e7\\u00e3o (Ensemble)\\n",
    "\\n",
    "Para buscar Top 3, a estrat\\u00e9gia \\u00e9 usar um **ensemble** de modelos complementares (cada um cobre um aspecto diferente do copy-move):\\n",
    "\\n",
    "- **U-Net com backbone CNN forte**: `Unet` (encoder-decoder) com encoder pr\\u00e9-treinado (ex.: EfficientNet-B7, ResNeXt101) + aten\\u00e7\\u00e3o no decoder (ex.: `scse`).\\n",
    "- **Transformer com aten\\u00e7\\u00e3o global**: `SegFormer` (MiT backbone) para capturar similaridades em regi\\u00f5es distantes da imagem.\\n",
    "- **M\\u00f3dulo de auto-correla\\u00e7\\u00e3o intra-imagem**: bloco (non-local) que computa correla\\u00e7\\u00e3o espacial em features profundas e real\\u00e7a padr\\u00f5es repetidos (inspira\\u00e7\\u00e3o copy-move).\\n",
    "\\n",
    "Cada modelo produz um mapa de **logits** (ou probabilidades) e combinamos via m\\u00e9dia ponderada.\\n",
    "\\n",
    "> Observa\\u00e7\\u00e3o (Kaggle): internet \\u00e9 OFF. Se os pesos `imagenet` n\\u00e3o estiverem em cache, os builders abaixo fazem fallback para `encoder_weights=None`. Para for\\u00e7ar erro (sem fallback), use `strict_weights=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c9f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 9 — Arquitetura: importar builders do repo (src/)\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "# Se este notebook estiver rodando a partir de /notebooks, adiciona ../src ao PYTHONPATH.\n",
    "for root in [Path.cwd(), Path.cwd().parent]:\n",
    "    if (root / \"src\").exists():\n",
    "        sys.path.insert(0, str(root / \"src\"))\n",
    "        break\n",
    "\n",
    "from forgeryseg.models.builders import build_segformer, build_unet\n",
    "from forgeryseg.models.correlation import CorrelationConfig, SmpCorrelationWrapper\n",
    "from forgeryseg.models.ensemble import SegmentationEnsemble\n",
    "\n",
    "print(\"Imported forgeryseg model builders\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e7f1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 10 — Definir os 3 modelos do ensemble\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"DEVICE:\", DEVICE)\n",
    "\n",
    "\n",
    "def build_unet_cnn_strong():\n",
    "    # Ex.: EfficientNet-B7 / ResNeXt101 (mais pesados, maior capacidade)\n",
    "    return build_unet(\n",
    "        encoder_name=\"efficientnet-b7\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        decoder_attention_type=\"scse\",\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_transformer_segformer():\n",
    "    # SegFormer (MiT) para aten\\u00e7\\u00e3o global\n",
    "    return build_segformer(\n",
    "        encoder_name=\"mit_b2\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "    )\n",
    "\n",
    "\n",
    "def build_unet_with_correlation():\n",
    "    # U-Net com um bloco de auto-correla\\u00e7\\u00e3o inserido em features profundas\n",
    "    base = build_unet(\n",
    "        encoder_name=\"resnext101_32x8d\",\n",
    "        encoder_weights=\"imagenet\",\n",
    "        decoder_attention_type=\"scse\",\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "    )\n",
    "    corr_cfg = CorrelationConfig(feature_index=-1, max_tokens=256)\n",
    "    return SmpCorrelationWrapper(base, corr_cfg)\n",
    "\n",
    "\n",
    "models = [\n",
    "    build_unet_cnn_strong().to(DEVICE),\n",
    "    build_transformer_segformer().to(DEVICE),\n",
    "    build_unet_with_correlation().to(DEVICE),\n",
    "]\n",
    "for m in models:\n",
    "    m.eval()\n",
    "\n",
    "print(\"Ensemble models:\")\n",
    "for i, m in enumerate(models):\n",
    "    print(i, type(m).__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3a25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Celula 11 — Combinar modelos (ensemble) e checar shapes\n",
    "SANITY_SIZE = 256\n",
    "x = torch.randn(1, 3, SANITY_SIZE, SANITY_SIZE, device=DEVICE)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outs = [m(x) for m in models]\n",
    "print(\"Single-model outputs:\", [tuple(o.shape) for o in outs])\n",
    "\n",
    "# Para treino: combine logits (sem sigmoid)\n",
    "ensemble_logits = SegmentationEnsemble(models, output=\"logits\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    y_logits = ensemble_logits(x)\n",
    "print(\"Ensemble logits:\", tuple(y_logits.shape))\n",
    "\n",
    "# Para infer\\u00eancia: combine probabilidades (sigmoid)\n",
    "ensemble_probs = SegmentationEnsemble(models, output=\"probs\").to(DEVICE)\n",
    "with torch.no_grad():\n",
    "    y_probs = ensemble_probs(x)\n",
    "print(\"Ensemble probs:\", tuple(y_probs.shape), \"min/max\", float(y_probs.min()), float(y_probs.max()))"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
