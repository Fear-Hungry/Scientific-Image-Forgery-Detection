{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "86da5eda",
   "metadata": {},
   "source": [
    "# Fase 2 — Treino do classificador (authentic vs forged)\n",
    "\n",
    "Objetivo:\n",
    "- Treinar um classificador binário para filtrar imagens provavelmente autênticas.\n",
    "- Salvar checkpoints em `/kaggle/working/outputs/models_cls/` (Kaggle) ou `outputs/models_cls/` (local).\n",
    "\n",
    "**Regras**\n",
    "- Notebook-only (sem importar código do projeto).\n",
    "- Internet pode estar OFF; use wheels offline se necessário.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae945dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1 — Sanidade Kaggle (lembrete)\n",
    "print(\"Kaggle submission constraints (lembrete):\")\n",
    "print(\"- Submissions via Notebook\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF no submit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5fa8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2 — Imports + ambiente\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "warnings.simplefilter(\"default\")\n",
    "\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107bfc48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2b — Instalação offline (opcional): wheels via Kaggle Dataset (sem internet)\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def _find_offline_bundle() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        return None\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return None\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        for base in (ds, ds / \"recodai_bundle\"):\n",
    "            if (base / \"wheels\").exists():\n",
    "                candidates.append(base)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    if len(candidates) > 1:\n",
    "        print(\"[OFFLINE INSTALL] múltiplos bundles com wheels encontrados; usando o primeiro:\")\n",
    "        for c in candidates:\n",
    "            print(\" -\", c)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "OFFLINE_BUNDLE = _find_offline_bundle()\n",
    "if OFFLINE_BUNDLE is None:\n",
    "    print(\"[OFFLINE INSTALL] nenhum bundle com `wheels/` encontrado em `/kaggle/input`.\")\n",
    "else:\n",
    "    wheel_dir = OFFLINE_BUNDLE / \"wheels\"\n",
    "    whls = sorted(str(p) for p in wheel_dir.glob(\"*.whl\"))\n",
    "    print(\"[OFFLINE INSTALL] bundle:\", OFFLINE_BUNDLE)\n",
    "    print(\"[OFFLINE INSTALL] wheels:\", len(whls))\n",
    "    if whls:\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheel_dir),\n",
    "            *whls,\n",
    "        ]\n",
    "        print(\"[OFFLINE INSTALL] executando:\", \" \".join(cmd[:8]), \"...\", f\"(+{len(whls)} wheels)\")\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"[OFFLINE INSTALL] OK.\")\n",
    "    else:\n",
    "        print(\"[OFFLINE INSTALL] aviso: `wheels/` existe, mas está vazio.\")\n",
    "\n",
    "\n",
    "def _is_competition_dataset_dir(path: Path) -> bool:\n",
    "    return (path / \"train_images\").exists() or (path / \"test_images\").exists() or (path / \"train_masks\").exists()\n",
    "\n",
    "\n",
    "def _candidate_python_roots(base: Path) -> list[Path]:\n",
    "    roots = [\n",
    "        base,\n",
    "        base / \"src\",\n",
    "        base / \"vendor\",\n",
    "        base / \"third_party\",\n",
    "        base / \"recodai_bundle\",\n",
    "        base / \"recodai_bundle\" / \"src\",\n",
    "        base / \"recodai_bundle\" / \"vendor\",\n",
    "        base / \"recodai_bundle\" / \"third_party\",\n",
    "    ]\n",
    "    return [r for r in roots if r.exists()]\n",
    "\n",
    "\n",
    "def add_local_package_to_syspath(package_dir_name: str) -> list[Path]:\n",
    "    added: list[Path] = []\n",
    "    if not is_kaggle():\n",
    "        return added\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return added\n",
    "\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        if _is_competition_dataset_dir(ds):\n",
    "            continue\n",
    "        for root in _candidate_python_roots(ds):\n",
    "            pkg = root / package_dir_name\n",
    "            if (pkg / \"__init__.py\").exists():\n",
    "                if str(root) not in sys.path:\n",
    "                    sys.path.insert(0, str(root))\n",
    "                    added.append(root)\n",
    "                continue\n",
    "            try:\n",
    "                for child in sorted(p for p in root.glob(\"*\") if p.is_dir()):\n",
    "                    pkg2 = child / package_dir_name\n",
    "                    if (pkg2 / \"__init__.py\").exists():\n",
    "                        if str(child) not in sys.path:\n",
    "                            sys.path.insert(0, str(child))\n",
    "                            added.append(child)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if added:\n",
    "        uniq = []\n",
    "        for p in added:\n",
    "            if p not in uniq:\n",
    "                uniq.append(p)\n",
    "        print(f\"[LOCAL IMPORT] adicionado ao sys.path para '{package_dir_name}':\")\n",
    "        for p in uniq[:10]:\n",
    "            print(\" -\", p)\n",
    "        if len(uniq) > 10:\n",
    "            print(\" ...\")\n",
    "        return uniq\n",
    "\n",
    "    print(f\"[LOCAL IMPORT] não encontrei '{package_dir_name}/__init__.py' em `/kaggle/input/*` (fora do dataset da competição).\")\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a983ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3 — Dataset paths (Kaggle/local)\n",
    "\n",
    "\n",
    "def find_dataset_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        base = Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "        if base.exists():\n",
    "            return base\n",
    "        kaggle_input = Path(\"/kaggle/input\")\n",
    "        if kaggle_input.exists():\n",
    "            for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "                if (ds / \"train_images\").exists() and (ds / \"test_images\").exists():\n",
    "                    return ds\n",
    "\n",
    "    base = Path(\"data\").resolve()\n",
    "    if (base / \"train_images\").exists() and (base / \"test_images\").exists():\n",
    "        return base\n",
    "\n",
    "    raise FileNotFoundError(\"Dataset não encontrado. No Kaggle: anexe o dataset da competição.\")\n",
    "\n",
    "\n",
    "DATA_ROOT = find_dataset_root()\n",
    "TRAIN_IMAGES = DATA_ROOT / \"train_images\"\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff268db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4 — Index (train) para classificação\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ClsSample:\n",
    "    case_id: str\n",
    "    image_path: Path\n",
    "    label: int  # 0 authentic, 1 forged\n",
    "\n",
    "\n",
    "def build_cls_index(train_images_dir: Path) -> list[ClsSample]:\n",
    "    samples: list[ClsSample] = []\n",
    "    for label_name, y in [(\"authentic\", 0), (\"forged\", 1)]:\n",
    "        for img_path in sorted((train_images_dir / label_name).glob(\"*.png\")):\n",
    "            samples.append(ClsSample(case_id=img_path.stem, image_path=img_path, label=int(y)))\n",
    "    if not samples:\n",
    "        raise FileNotFoundError(f\"Nenhuma imagem encontrada em: {train_images_dir}\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "train_samples = build_cls_index(TRAIN_IMAGES)\n",
    "y = np.array([s.label for s in train_samples], dtype=np.int64)\n",
    "print(\"train samples:\", len(train_samples))\n",
    "print(\"authentic:\", int((y == 0).sum()), \"forged:\", int((y == 1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982bc650",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5 — Split (5-fold estratificado)\n",
    "N_FOLDS = 5\n",
    "FOLD = 0\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    folds = np.zeros(len(train_samples), dtype=np.int64)\n",
    "    for fold_id, (_, val_idx) in enumerate(skf.split(np.zeros(len(train_samples)), y)):\n",
    "        folds[val_idx] = int(fold_id)\n",
    "except Exception:\n",
    "    print(\"[ERRO] Falha ao usar scikit-learn para split; usando split simples (não estratificado).\")\n",
    "    traceback.print_exc()\n",
    "    folds = np.arange(len(train_samples), dtype=np.int64) % int(N_FOLDS)\n",
    "\n",
    "train_idx = np.where(folds != int(FOLD))[0]\n",
    "val_idx = np.where(folds == int(FOLD))[0]\n",
    "\n",
    "print(f\"fold={FOLD}: train={len(train_idx)} val={len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf638fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6 — Dataset/DataLoader + transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "try:\n",
    "    import torchvision.transforms as T\n",
    "except Exception:\n",
    "    print(\"[ERRO] torchvision não disponível (ou falhou no import).\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    print(\"[WARN] tqdm indisponível; usando loop simples.\")\n",
    "\n",
    "    def tqdm(x, **kwargs):  # type: ignore\n",
    "        return x\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "IMAGE_SIZE = 384\n",
    "BATCH_SIZE = 32\n",
    "NUM_WORKERS = 2 if is_kaggle() else 4\n",
    "\n",
    "\n",
    "def build_transform(train: bool) -> T.Compose:\n",
    "    aug = []\n",
    "    if train:\n",
    "        aug += [\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "        ]\n",
    "    aug += [\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ]\n",
    "    return T.Compose(aug)\n",
    "\n",
    "\n",
    "class ClsDataset(Dataset):\n",
    "    def __init__(self, samples: list[ClsSample], transform: T.Compose):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        s = self.samples[int(idx)]\n",
    "        img = Image.open(s.image_path).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        y = torch.tensor([float(s.label)], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "ds_train = ClsDataset([train_samples[i] for i in train_idx.tolist()], build_transform(train=True))\n",
    "ds_val = ClsDataset([train_samples[i] for i in val_idx.tolist()], build_transform(train=False))\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"dl_train batches:\", len(dl_train), \"dl_val batches:\", len(dl_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e569f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7 — Modelo (timm preferencial; fallback torchvision)\n",
    "try:\n",
    "    import timm\n",
    "except Exception:\n",
    "    timm = None\n",
    "    print(\"[WARN] timm indisponível; vou usar fallback do torchvision.\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "def build_cls_model(model_name: str) -> nn.Module:\n",
    "    if timm is not None:\n",
    "        try:\n",
    "            m = timm.create_model(model_name, pretrained=False, num_classes=1)\n",
    "            return m\n",
    "        except Exception:\n",
    "            print(f\"[ERRO] falha ao criar modelo timm='{model_name}'.\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    try:\n",
    "        from torchvision.models import resnet50\n",
    "\n",
    "        m = resnet50(weights=None)\n",
    "        m.fc = nn.Linear(m.fc.in_features, 1)\n",
    "        return m\n",
    "    except Exception:\n",
    "        print(\"[ERRO] falha ao criar fallback torchvision (resnet50).\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "CLS_MODEL_NAME = \"tf_efficientnet_b4_ns\"  # se timm disponível\n",
    "model = build_cls_model(CLS_MODEL_NAME).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d7eb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8 — Treino (com progresso) + checkpoint\n",
    "from time import time\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "except Exception:\n",
    "    roc_auc_score = None\n",
    "    print(\"[WARN] scikit-learn (roc_auc_score) indisponível; vou reportar só loss/accuracy.\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "def _compute_pos_weight(labels: np.ndarray) -> torch.Tensor:\n",
    "    pos = float((labels == 1).sum())\n",
    "    neg = float((labels == 0).sum())\n",
    "    if pos <= 0:\n",
    "        return torch.tensor(1.0)\n",
    "    return torch.tensor(neg / max(pos, 1.0), dtype=torch.float32)\n",
    "\n",
    "\n",
    "pos_weight = _compute_pos_weight(y[train_idx]).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "LR = 3e-4\n",
    "EPOCHS = 10\n",
    "WEIGHT_DECAY = 1e-2\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "\n",
    "def _sigmoid_np(x: np.ndarray) -> np.ndarray:\n",
    "    return 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> dict:\n",
    "    model.eval()\n",
    "    losses: list[float] = []\n",
    "    all_logits: list[np.ndarray] = []\n",
    "    all_targets: list[np.ndarray] = []\n",
    "    for x, yb in tqdm(loader, desc=\"val\", leave=False):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x).view(-1, 1)\n",
    "        loss = criterion(logits, yb)\n",
    "        losses.append(float(loss.item()))\n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_targets.append(yb.detach().cpu().numpy())\n",
    "\n",
    "    logits_np = np.concatenate(all_logits, axis=0).reshape(-1)\n",
    "    targets_np = np.concatenate(all_targets, axis=0).reshape(-1)\n",
    "    probs = _sigmoid_np(logits_np)\n",
    "    pred = (probs >= 0.5).astype(np.int64)\n",
    "    acc = float((pred == targets_np.astype(np.int64)).mean())\n",
    "\n",
    "    out = {\n",
    "        \"loss\": float(np.mean(losses)) if losses else float(\"nan\"),\n",
    "        \"acc@0.5\": acc,\n",
    "    }\n",
    "    if roc_auc_score is not None:\n",
    "        try:\n",
    "            out[\"auc\"] = float(roc_auc_score(targets_np, probs))\n",
    "        except Exception:\n",
    "            print(\"[WARN] falha ao calcular AUC (roc_auc_score).\")\n",
    "            traceback.print_exc()\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.train()\n",
    "    losses: list[float] = []\n",
    "    for x, yb in tqdm(loader, desc=\"train\", leave=False):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "            logits = model(x).view(-1, 1)\n",
    "            loss = criterion(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses.append(float(loss.item()))\n",
    "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "\n",
    "def output_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        return Path(\"/kaggle/working\")\n",
    "    return Path(\".\").resolve()\n",
    "\n",
    "\n",
    "OUTPUT_DIR = output_root() / \"outputs\" / \"models_cls\" / f\"fold_{int(FOLD)}\"\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PATH = OUTPUT_DIR / \"best.pt\"\n",
    "\n",
    "best_score = -1.0\n",
    "\n",
    "for epoch in range(1, int(EPOCHS) + 1):\n",
    "    t0 = time()\n",
    "    train_loss = train_one_epoch(model, dl_train)\n",
    "    val_metrics = evaluate(model, dl_val)\n",
    "    elapsed = time() - t0\n",
    "\n",
    "    score = float(val_metrics.get(\"auc\", -val_metrics[\"loss\"]))\n",
    "    print(\n",
    "        f\"epoch {epoch:02d}/{EPOCHS} | \"\n",
    "        f\"train_loss={train_loss:.4f} | val_loss={val_metrics['loss']:.4f} | \"\n",
    "        f\"acc@0.5={val_metrics['acc@0.5']:.4f} | \"\n",
    "        + (f\"auc={val_metrics.get('auc', float('nan')):.4f} | \" if \"auc\" in val_metrics else \"\")\n",
    "        + f\"time={elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        ckpt = {\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"config\": {\n",
    "                \"backend\": \"timm\" if timm is not None else \"torchvision\",\n",
    "                \"model_name\": CLS_MODEL_NAME,\n",
    "                \"image_size\": int(IMAGE_SIZE),\n",
    "                \"fold\": int(FOLD),\n",
    "                \"seed\": int(SEED),\n",
    "            },\n",
    "            \"score\": float(best_score),\n",
    "        }\n",
    "        torch.save(ckpt, BEST_PATH)\n",
    "        print(\"  saved best ->\", BEST_PATH)\n",
    "\n",
    "print(\"best score:\", best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffb00ba",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Célula 9 — Tuning de limiar (favorecer recall em forged)\n",
    "\n",
    "THRESHOLDS = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50]\n",
    "\n",
    "if roc_auc_score is None:\n",
    "    print(\"[INFO] scikit-learn indisponível; tuning simples vai usar apenas accuracy.\")\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "except Exception:\n",
    "    precision_recall_fscore_support = None\n",
    "    print(\"[WARN] scikit-learn indisponível (precision_recall_fscore_support).\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_val_probs(model: nn.Module, loader: DataLoader) -> tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    all_probs: list[np.ndarray] = []\n",
    "    all_targets: list[np.ndarray] = []\n",
    "    for x, yb in tqdm(loader, desc=\"collect\", leave=False):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x).view(-1).detach().cpu().numpy()\n",
    "        probs = _sigmoid_np(logits)\n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(yb.view(-1).cpu().numpy())\n",
    "    return np.concatenate(all_probs), np.concatenate(all_targets)\n",
    "\n",
    "\n",
    "probs_val, y_val = collect_val_probs(model, dl_val)\n",
    "print(\"val probs shape:\", probs_val.shape)\n",
    "\n",
    "best = None\n",
    "for t in THRESHOLDS:\n",
    "    pred = (probs_val >= float(t)).astype(np.int64)\n",
    "    if precision_recall_fscore_support is None:\n",
    "        acc = float((pred == y_val.astype(np.int64)).mean())\n",
    "        print(f\"t={t:.2f} acc={acc:.4f}\")\n",
    "        continue\n",
    "\n",
    "    # classe 1 (forged)\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val, pred, labels=[1], average=None, zero_division=0)\n",
    "    prec1, rec1, f11 = float(prec[0]), float(rec[0]), float(f1[0])\n",
    "    print(f\"t={t:.2f}  forged: recall={rec1:.4f} precision={prec1:.4f} f1={f11:.4f}\")\n",
    "    cand = (rec1, -prec1, t)\n",
    "    if best is None or cand > best:\n",
    "        best = cand\n",
    "\n",
    "if best is not None:\n",
    "    print(\"\\nSugestão (max recall, desempate por menor precision):\")\n",
    "    print(\"recall=\", best[0], \"precision=-\", -best[1], \"threshold=\", best[2])"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
