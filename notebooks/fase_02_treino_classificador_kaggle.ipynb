{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbb88126",
   "metadata": {},
   "source": [
    "# Fase 2 — Treino do classificador (Kaggle)\n",
    "\n",
    "Objetivo:\n",
    "- Treinar um classificador binário para filtrar imagens provavelmente autênticas.\n",
    "- Salvar checkpoints em `outputs/models_cls/fold_<k>/best.pt`.\n",
    "\n",
    "**Importante**\n",
    "- Este notebook **importa o código do projeto** em `src/forgeryseg/` (modularizado).\n",
    "- Internet pode estar OFF: use wheels offline se necessário.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d763362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1 — Sanidade Kaggle (lembrete)\n",
    "print(\"Kaggle submission constraints (lembrete):\")\n",
    "print(\"- Submissions via Notebook\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF no submit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20a3654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2 — Imports + ambiente\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "warnings.simplefilter(\"default\")\n",
    "\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a436e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2b — Instalação offline (opcional): wheels via Kaggle Dataset (sem internet)\n",
    "#\n",
    "# Instala TODAS as wheels encontradas com `--no-deps` para evitar o pip tentar puxar\n",
    "# dependências do torch (ex.: nvidia-cuda-*) quando o Kaggle está offline.\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def _find_offline_bundle() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        return None\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return None\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        for base in (ds, ds / \"recodai_bundle\"):\n",
    "            if (base / \"wheels\").exists():\n",
    "                candidates.append(base)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    if len(candidates) > 1:\n",
    "        print(\"[OFFLINE INSTALL] múltiplos bundles com wheels encontrados; usando o primeiro:\")\n",
    "        for c in candidates:\n",
    "            print(\" -\", c)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def _is_competition_dataset_dir(path: Path) -> bool:\n",
    "    return (path / \"train_images\").exists() or (path / \"test_images\").exists() or (path / \"train_masks\").exists()\n",
    "\n",
    "\n",
    "def _candidate_python_roots(base: Path) -> list[Path]:\n",
    "    roots = [\n",
    "        base,\n",
    "        base / \"src\",\n",
    "        base / \"vendor\",\n",
    "        base / \"third_party\",\n",
    "        base / \"recodai_bundle\",\n",
    "        base / \"recodai_bundle\" / \"src\",\n",
    "        base / \"recodai_bundle\" / \"vendor\",\n",
    "        base / \"recodai_bundle\" / \"third_party\",\n",
    "    ]\n",
    "    return [r for r in roots if r.exists()]\n",
    "\n",
    "\n",
    "def add_local_package_to_syspath(package_dir_name: str) -> list[Path]:\n",
    "    added: list[Path] = []\n",
    "    if not is_kaggle():\n",
    "        return added\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return added\n",
    "\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        if _is_competition_dataset_dir(ds):\n",
    "            continue\n",
    "        for root in _candidate_python_roots(ds):\n",
    "            pkg = root / package_dir_name\n",
    "            if (pkg / \"__init__.py\").exists():\n",
    "                if str(root) not in sys.path:\n",
    "                    sys.path.insert(0, str(root))\n",
    "                    added.append(root)\n",
    "                continue\n",
    "            try:\n",
    "                for child in sorted(p for p in root.glob(\"*\") if p.is_dir()):\n",
    "                    pkg2 = child / package_dir_name\n",
    "                    if (pkg2 / \"__init__.py\").exists():\n",
    "                        if str(child) not in sys.path:\n",
    "                            sys.path.insert(0, str(child))\n",
    "                            added.append(child)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if added:\n",
    "        uniq = []\n",
    "        for p in added:\n",
    "            if p not in uniq:\n",
    "                uniq.append(p)\n",
    "        print(f\"[LOCAL IMPORT] adicionado ao sys.path para '{package_dir_name}':\")\n",
    "        for p in uniq[:10]:\n",
    "            print(\" -\", p)\n",
    "        if len(uniq) > 10:\n",
    "            print(\" ...\")\n",
    "        return uniq\n",
    "\n",
    "    print(f\"[LOCAL IMPORT] não encontrei '{package_dir_name}/__init__.py' em `/kaggle/input/*` (fora do dataset da competição).\")\n",
    "    return added\n",
    "\n",
    "\n",
    "OFFLINE_BUNDLE = _find_offline_bundle()\n",
    "if OFFLINE_BUNDLE is None:\n",
    "    print(\"[OFFLINE INSTALL] nenhum bundle com `wheels/` encontrado em `/kaggle/input`.\")\n",
    "else:\n",
    "    wheel_dir = OFFLINE_BUNDLE / \"wheels\"\n",
    "    whls = sorted(str(p) for p in wheel_dir.glob(\"*.whl\"))\n",
    "    print(\"[OFFLINE INSTALL] bundle:\", OFFLINE_BUNDLE)\n",
    "    print(\"[OFFLINE INSTALL] wheels:\", len(whls))\n",
    "    if not whls:\n",
    "        print(\"[OFFLINE INSTALL] aviso: `wheels/` existe, mas está vazio.\")\n",
    "    else:\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheel_dir),\n",
    "            \"--no-deps\",\n",
    "            *whls,\n",
    "        ]\n",
    "        print(\"[OFFLINE INSTALL] executando:\", \" \".join(cmd[:9]), \"...\", f\"(+{len(whls)} wheels)\")\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"[OFFLINE INSTALL] OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d4fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2c — Import do projeto (src/forgeryseg)\n",
    "try:\n",
    "    import forgeryseg  # type: ignore\n",
    "except Exception:\n",
    "    local_src = Path(\"src\").resolve()\n",
    "    if (local_src / \"forgeryseg\" / \"__init__.py\").exists() and str(local_src) not in sys.path:\n",
    "        sys.path.insert(0, str(local_src))\n",
    "    if is_kaggle():\n",
    "        add_local_package_to_syspath(\"forgeryseg\")\n",
    "    import forgeryseg  # type: ignore\n",
    "\n",
    "print(\"forgeryseg:\", Path(forgeryseg.__file__).resolve())\n",
    "\n",
    "from PIL import Image  # noqa: E402\n",
    "from torch.utils.data import DataLoader, Dataset  # noqa: E402\n",
    "\n",
    "from forgeryseg.dataset import build_train_index  # noqa: E402\n",
    "from forgeryseg.models.classifier import build_classifier, compute_pos_weight  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f808370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3 — Dataset root + index\n",
    "\n",
    "\n",
    "def find_dataset_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        base = Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "        if base.exists():\n",
    "            return base\n",
    "        kaggle_input = Path(\"/kaggle/input\")\n",
    "        if kaggle_input.exists():\n",
    "            for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "                if (ds / \"train_images\").exists() and (ds / \"test_images\").exists():\n",
    "                    return ds\n",
    "\n",
    "    base = Path(\"data\").resolve()\n",
    "    if (base / \"train_images\").exists() and (base / \"test_images\").exists():\n",
    "        return base\n",
    "\n",
    "    raise FileNotFoundError(\"Dataset não encontrado. No Kaggle: anexe o dataset da competição.\")\n",
    "\n",
    "\n",
    "DATA_ROOT = find_dataset_root()\n",
    "train_samples = build_train_index(DATA_ROOT, strict=False)\n",
    "labels = np.array([0 if s.is_authentic else 1 for s in train_samples], dtype=np.int64)\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"train samples:\", len(train_samples), \"auth:\", int((labels == 0).sum()), \"forged:\", int((labels == 1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64bbe29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4 — Split (5-fold estratificado)\n",
    "N_FOLDS = 5\n",
    "FOLD = 0\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    folds = np.zeros(len(train_samples), dtype=np.int64)\n",
    "    for fold_id, (_, val_idx) in enumerate(skf.split(np.zeros(len(train_samples)), labels)):\n",
    "        folds[val_idx] = int(fold_id)\n",
    "except Exception:\n",
    "    print(\"[ERRO] scikit-learn falhou (StratifiedKFold). Usando split simples.\")\n",
    "    traceback.print_exc()\n",
    "    folds = np.arange(len(train_samples), dtype=np.int64) % int(N_FOLDS)\n",
    "\n",
    "train_idx = np.where(folds != int(FOLD))[0]\n",
    "val_idx = np.where(folds == int(FOLD))[0]\n",
    "print(f\"fold={FOLD}: train={len(train_idx)} val={len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b08de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5 — Dataset/DataLoader + transforms\n",
    "try:\n",
    "    import torchvision.transforms as T\n",
    "except Exception:\n",
    "    print(\"[ERRO] torchvision falhou no import.\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    print(\"[WARN] tqdm indisponível; usando loop simples.\")\n",
    "\n",
    "    def tqdm(x, **kwargs):  # type: ignore\n",
    "        return x\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "IMAGE_SIZE = 384\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "LR = 3e-4\n",
    "WEIGHT_DECAY = 1e-2\n",
    "\n",
    "NUM_WORKERS = 2 if is_kaggle() else 0\n",
    "\n",
    "\n",
    "def build_transform(train: bool) -> T.Compose:\n",
    "    aug = []\n",
    "    if train:\n",
    "        aug += [\n",
    "            T.RandomHorizontalFlip(p=0.5),\n",
    "            T.RandomVerticalFlip(p=0.5),\n",
    "        ]\n",
    "    aug += [\n",
    "        T.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "        T.ToTensor(),\n",
    "        T.Normalize(IMAGENET_MEAN, IMAGENET_STD),\n",
    "    ]\n",
    "    return T.Compose(aug)\n",
    "\n",
    "\n",
    "class ClsDataset(Dataset):\n",
    "    def __init__(self, samples, transform: T.Compose):\n",
    "        self.samples = samples\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        s = self.samples[int(idx)]\n",
    "        img = Image.open(s.image_path).convert(\"RGB\")\n",
    "        x = self.transform(img)\n",
    "        y = torch.tensor([0.0 if s.is_authentic else 1.0], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "ds_train = ClsDataset([train_samples[i] for i in train_idx.tolist()], build_transform(train=True))\n",
    "ds_val = ClsDataset([train_samples[i] for i in val_idx.tolist()], build_transform(train=False))\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"dl_train batches:\", len(dl_train), \"dl_val batches:\", len(dl_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bc60ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6 — Modelo + treino\n",
    "\n",
    "MODEL_NAME = \"tf_efficientnet_b4_ns\"\n",
    "model = build_classifier(model_name=MODEL_NAME, pretrained=False, num_classes=1).to(DEVICE)\n",
    "\n",
    "pos_weight = torch.tensor(compute_pos_weight(labels[train_idx]), dtype=torch.float32, device=DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> dict:\n",
    "    model.eval()\n",
    "    losses: list[float] = []\n",
    "    all_logits: list[np.ndarray] = []\n",
    "    all_targets: list[np.ndarray] = []\n",
    "    for x, yb in tqdm(loader, desc=\"val\", leave=False):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x).view(-1, 1)\n",
    "        loss = criterion(logits, yb)\n",
    "        losses.append(float(loss.item()))\n",
    "        all_logits.append(logits.detach().cpu().numpy())\n",
    "        all_targets.append(yb.detach().cpu().numpy())\n",
    "\n",
    "    logits_np = np.concatenate(all_logits, axis=0).reshape(-1)\n",
    "    targets_np = np.concatenate(all_targets, axis=0).reshape(-1)\n",
    "    probs = 1.0 / (1.0 + np.exp(-logits_np))\n",
    "    pred = (probs >= 0.5).astype(np.int64)\n",
    "    acc = float((pred == targets_np.astype(np.int64)).mean())\n",
    "\n",
    "    out = {\"loss\": float(np.mean(losses)) if losses else float(\"nan\"), \"acc@0.5\": acc}\n",
    "    try:\n",
    "        from sklearn.metrics import roc_auc_score\n",
    "\n",
    "        out[\"auc\"] = float(roc_auc_score(targets_np, probs))\n",
    "    except Exception:\n",
    "        print(\"[WARN] falha ao calcular AUC (roc_auc_score).\")\n",
    "        traceback.print_exc()\n",
    "    return out\n",
    "\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.train()\n",
    "    losses: list[float] = []\n",
    "    for x, yb in tqdm(loader, desc=\"train\", leave=False):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "            logits = model(x).view(-1, 1)\n",
    "            loss = criterion(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses.append(float(loss.item()))\n",
    "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "\n",
    "def output_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        return Path(\"/kaggle/working\")\n",
    "    return Path(\".\").resolve()\n",
    "\n",
    "\n",
    "SAVE_DIR = output_root() / \"outputs\" / \"models_cls\" / f\"fold_{int(FOLD)}\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PATH = SAVE_DIR / \"best.pt\"\n",
    "\n",
    "best_score = -1.0\n",
    "\n",
    "for epoch in range(1, int(EPOCHS) + 1):\n",
    "    tr_loss = train_one_epoch(model, dl_train)\n",
    "    val = evaluate(model, dl_val)\n",
    "    score = float(val.get(\"auc\", -val[\"loss\"]))\n",
    "\n",
    "    print(\n",
    "        f\"epoch {epoch:02d}/{EPOCHS} | train_loss={tr_loss:.4f} | val_loss={val['loss']:.4f} | \"\n",
    "        f\"acc@0.5={val['acc@0.5']:.4f} | \" + (f\"auc={val.get('auc', float('nan')):.4f}\" if \"auc\" in val else \"\")\n",
    "    )\n",
    "\n",
    "    if score > best_score:\n",
    "        best_score = score\n",
    "        ckpt = {\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"config\": {\n",
    "                \"backend\": \"timm\",\n",
    "                \"model_name\": MODEL_NAME,\n",
    "                \"image_size\": int(IMAGE_SIZE),\n",
    "                \"fold\": int(FOLD),\n",
    "                \"seed\": int(SEED),\n",
    "            },\n",
    "            \"score\": float(best_score),\n",
    "        }\n",
    "        torch.save(ckpt, BEST_PATH)\n",
    "        print(\"  saved best ->\", BEST_PATH)\n",
    "\n",
    "print(\"best score:\", best_score)\n",
    "print(\"saved dir:\", SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4dcc13",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Célula 7 — (Opcional) Tuning de limiar (favorecer recall em forged)\n",
    "THRESHOLDS = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50]\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import precision_recall_fscore_support\n",
    "except Exception:\n",
    "    precision_recall_fscore_support = None\n",
    "    print(\"[WARN] scikit-learn indisponível (precision_recall_fscore_support).\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def collect_val_probs(model: nn.Module, loader: DataLoader) -> tuple[np.ndarray, np.ndarray]:\n",
    "    model.eval()\n",
    "    all_probs: list[np.ndarray] = []\n",
    "    all_targets: list[np.ndarray] = []\n",
    "    for x, yb in tqdm(loader, desc=\"collect\", leave=False):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x).view(-1).detach().cpu().numpy()\n",
    "        probs = 1.0 / (1.0 + np.exp(-logits))\n",
    "        all_probs.append(probs)\n",
    "        all_targets.append(yb.view(-1).cpu().numpy())\n",
    "    return np.concatenate(all_probs), np.concatenate(all_targets)\n",
    "\n",
    "\n",
    "probs_val, y_val = collect_val_probs(model, dl_val)\n",
    "print(\"val probs shape:\", probs_val.shape)\n",
    "\n",
    "for t in THRESHOLDS:\n",
    "    pred = (probs_val >= float(t)).astype(np.int64)\n",
    "    if precision_recall_fscore_support is None:\n",
    "        acc = float((pred == y_val.astype(np.int64)).mean())\n",
    "        print(f\"t={t:.2f} acc={acc:.4f}\")\n",
    "        continue\n",
    "\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(y_val, pred, labels=[1], average=None, zero_division=0)\n",
    "    prec1, rec1, f11 = float(prec[0]), float(rec[0]), float(f1[0])\n",
    "    print(f\"t={t:.2f}  forged: recall={rec1:.4f} precision={prec1:.4f} f1={f11:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
