{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8578d8b0",
   "metadata": {},
   "source": [
    "# Fase 3 — Pipeline completo (treino + inferência + submissão)\n",
    "\n",
    "Este notebook é um \"guia executável\" para:\n",
    "1) Treinar um baseline de segmentação (opcional),\n",
    "2) Validar com oF1 (opcional, depende de `scipy`),\n",
    "3) Rodar inferência no `test_images/` e gerar `submission.csv`.\n",
    "\n",
    "**Modo Kaggle (Code Competition)**\n",
    "- Internet: OFF no momento da submissão.\n",
    "- Tempo típico: até 4h.\n",
    "- Saída esperada: `submission.csv` (ou `submission.parquet`).\n",
    "\n",
    "**Importante**\n",
    "- Este notebook é **auto-contido** (não importa módulos do projeto).\n",
    "- No Kaggle, você só precisa do dataset da competição (e opcionalmente um dataset com pesos/outputs).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe5afee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1 — Regras do Kaggle (sanidade)\n",
    "print(\"Kaggle submission constraints (lembrete):\")\n",
    "print(\"- Submissions via Notebook\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF no submit\")\n",
    "print(\"- Output: submission.csv ou submission.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac83b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2 — Imports + ambiente\n",
    "import csv\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Iterable\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "warnings.simplefilter(\"default\")\n",
    "\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b759f3",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Célula 2b — Instalação offline (opcional): wheels via Kaggle Dataset (sem internet)\n",
    "#\n",
    "# Se você anexar um Dataset que contenha `wheels/*.whl`, esta célula instala os pacotes **offline** via pip.\n",
    "# Estruturas suportadas:\n",
    "# - `/kaggle/input/<dataset>/wheels/*.whl`\n",
    "# - `/kaggle/input/<dataset>/recodai_bundle/wheels/*.whl`\n",
    "#\n",
    "# Se nada for encontrado, nada é instalado (e eventuais imports vão falhar com erro explícito mais adiante).\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def _find_offline_bundle() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        return None\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return None\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        for base in (ds, ds / \"recodai_bundle\"):\n",
    "            if (base / \"wheels\").exists():\n",
    "                candidates.append(base)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    if len(candidates) > 1:\n",
    "        print(\"[OFFLINE INSTALL] múltiplos bundles com wheels encontrados; usando o primeiro:\")\n",
    "        for c in candidates:\n",
    "            print(\" -\", c)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "OFFLINE_BUNDLE = _find_offline_bundle()\n",
    "if OFFLINE_BUNDLE is None:\n",
    "    print(\"[OFFLINE INSTALL] nenhum bundle com `wheels/` encontrado em `/kaggle/input`.\")\n",
    "else:\n",
    "    wheel_dir = OFFLINE_BUNDLE / \"wheels\"\n",
    "    whls = sorted(str(p) for p in wheel_dir.glob(\"*.whl\"))\n",
    "    print(\"[OFFLINE INSTALL] bundle:\", OFFLINE_BUNDLE)\n",
    "    print(\"[OFFLINE INSTALL] wheels:\", len(whls))\n",
    "    if not whls:\n",
    "        print(\"[OFFLINE INSTALL] aviso: diretório `wheels/` existe mas não há `.whl`.\")\n",
    "    else:\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheel_dir),\n",
    "            \"--no-deps\",\n",
    "            *whls,\n",
    "        ]\n",
    "        print(\"[OFFLINE INSTALL] executando:\", \" \".join(cmd[:9]), \"...\", f\"(+{len(whls)} wheels)\")\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"[OFFLINE INSTALL] OK.\")\n",
    "\n",
    "\n",
    "def _is_competition_dataset_dir(path: Path) -> bool:\n",
    "    # Evita varrer o dataset grande da competição quando procurando código.\n",
    "    return (path / \"train_images\").exists() or (path / \"test_images\").exists() or (path / \"train_masks\").exists()\n",
    "\n",
    "\n",
    "def _candidate_python_roots(base: Path) -> list[Path]:\n",
    "    roots = [\n",
    "        base,\n",
    "        base / \"src\",\n",
    "        base / \"vendor\",\n",
    "        base / \"third_party\",\n",
    "        base / \"recodai_bundle\",\n",
    "        base / \"recodai_bundle\" / \"src\",\n",
    "        base / \"recodai_bundle\" / \"vendor\",\n",
    "        base / \"recodai_bundle\" / \"third_party\",\n",
    "    ]\n",
    "    return [r for r in roots if r.exists()]\n",
    "\n",
    "\n",
    "def add_local_package_to_syspath(package_dir_name: str) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Procura por `package_dir_name/__init__.py` em `/kaggle/input/*` (exceto o dataset da competição)\n",
    "    e adiciona o *root* correspondente ao `sys.path`.\n",
    "\n",
    "    Isso permite \"vendorizar\" libs puras Python via GitHub Dataset quando não há wheel disponível.\n",
    "    \"\"\"\n",
    "    added: list[Path] = []\n",
    "    if not is_kaggle():\n",
    "        return added\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return added\n",
    "\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        if _is_competition_dataset_dir(ds):\n",
    "            continue\n",
    "        for root in _candidate_python_roots(ds):\n",
    "            # 1) package diretamente no root (root/<pkg>/__init__.py)\n",
    "            pkg = root / package_dir_name\n",
    "            if (pkg / \"__init__.py\").exists():\n",
    "                if str(root) not in sys.path:\n",
    "                    sys.path.insert(0, str(root))\n",
    "                    added.append(root)\n",
    "                continue\n",
    "\n",
    "            # 2) um nível abaixo (root/*/<pkg>/__init__.py) para repositórios dentro de vendor/\n",
    "            try:\n",
    "                for child in sorted(p for p in root.glob(\"*\") if p.is_dir()):\n",
    "                    pkg2 = child / package_dir_name\n",
    "                    if (pkg2 / \"__init__.py\").exists():\n",
    "                        if str(child) not in sys.path:\n",
    "                            sys.path.insert(0, str(child))\n",
    "                            added.append(child)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if added:\n",
    "        uniq = []\n",
    "        for p in added:\n",
    "            if p not in uniq:\n",
    "                uniq.append(p)\n",
    "        print(f\"[LOCAL IMPORT] adicionado ao sys.path para '{package_dir_name}':\")\n",
    "        for p in uniq[:10]:\n",
    "            print(\" -\", p)\n",
    "        if len(uniq) > 10:\n",
    "            print(\" ...\")\n",
    "        return uniq\n",
    "\n",
    "    print(f\"[LOCAL IMPORT] não encontrei '{package_dir_name}/__init__.py' em `/kaggle/input/*` (fora do dataset da competição).\")\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fb167d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Célula 3 — Utilitários (dataset + inferência + RLE + métrica) — auto-contido\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, List, Optional, Sequence\n",
    "\n",
    "os.environ.setdefault(\"NO_ALBUMENTATIONS_UPDATE\", \"1\")\n",
    "\n",
    "try:\n",
    "    import albumentations as A\n",
    "except Exception:\n",
    "    print(\"[IMPORT ERROR] albumentations não importou; augmentations ficarão desativadas.\")\n",
    "    traceback.print_exc()\n",
    "    A = None\n",
    "\n",
    "try:\n",
    "    import cv2\n",
    "except Exception:\n",
    "    print(\"[IMPORT ERROR] opencv-python (cv2) não importou; alguns fallbacks/morfologia podem falhar.\")\n",
    "    traceback.print_exc()\n",
    "    cv2 = None\n",
    "\n",
    "try:\n",
    "    from scipy.ndimage import label as _cc_label\n",
    "except Exception:\n",
    "    print(\"[IMPORT ERROR] scipy.ndimage.label não importou; connected components via SciPy ficará indisponível.\")\n",
    "    traceback.print_exc()\n",
    "    _cc_label = None\n",
    "\n",
    "try:\n",
    "    from scipy.optimize import linear_sum_assignment as _linear_sum_assignment\n",
    "except Exception:\n",
    "    print(\"[IMPORT ERROR] scipy.optimize.linear_sum_assignment não importou; métrica oF1 (Hungarian) ficará indisponível.\")\n",
    "    traceback.print_exc()\n",
    "    _linear_sum_assignment = None\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "\n",
    "def find_project_root() -> Path:\n",
    "    \"\"\"\n",
    "    Heurística para achar a raiz do projeto local (para ler configs e gravar outputs).\n",
    "    No Kaggle, normalmente isso vira o CWD (`/kaggle/working`).\n",
    "    \"\"\"\n",
    "    cwd = Path.cwd().resolve()\n",
    "    for p in [cwd, *cwd.parents]:\n",
    "        if (p / \"notebooks\").exists() and (p / \"README.md\").exists():\n",
    "            return p\n",
    "        if (p / \"configs\").exists() and (p / \"data\").exists():\n",
    "            return p\n",
    "    return cwd\n",
    "\n",
    "\n",
    "PROJECT_ROOT = find_project_root()\n",
    "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Dataset / indexing\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class Sample:\n",
    "    case_id: str\n",
    "    image_path: Path\n",
    "    mask_path: Optional[Path]\n",
    "    is_authentic: Optional[bool]\n",
    "    split: str\n",
    "    label: Optional[str]\n",
    "    rel_path: Path\n",
    "\n",
    "\n",
    "def build_train_index(data_root: str | Path, strict: bool = False) -> List[Sample]:\n",
    "    data_root = Path(data_root)\n",
    "    train_root = data_root / \"train_images\"\n",
    "    mask_root = data_root / \"train_masks\"\n",
    "\n",
    "    samples: List[Sample] = []\n",
    "    for label in (\"authentic\", \"forged\"):\n",
    "        for image_path in sorted((train_root / label).glob(\"*.png\")):\n",
    "            case_id = image_path.stem\n",
    "            mask_path = None\n",
    "            if label == \"forged\":\n",
    "                candidate = mask_root / f\"{case_id}.npy\"\n",
    "                if candidate.exists():\n",
    "                    mask_path = candidate\n",
    "                elif strict:\n",
    "                    raise FileNotFoundError(f\"Missing mask for {case_id}\")\n",
    "\n",
    "            samples.append(\n",
    "                Sample(\n",
    "                    case_id=case_id,\n",
    "                    image_path=image_path,\n",
    "                    mask_path=mask_path,\n",
    "                    is_authentic=(label == \"authentic\"),\n",
    "                    split=\"train\",\n",
    "                    label=label,\n",
    "                    rel_path=image_path.relative_to(data_root),\n",
    "                )\n",
    "            )\n",
    "    return samples\n",
    "\n",
    "\n",
    "def build_supplemental_index(data_root: str | Path, strict: bool = False) -> List[Sample]:\n",
    "    data_root = Path(data_root)\n",
    "    image_root = data_root / \"supplemental_images\"\n",
    "    mask_root = data_root / \"supplemental_masks\"\n",
    "\n",
    "    samples: List[Sample] = []\n",
    "    for image_path in sorted(image_root.glob(\"*.png\")):\n",
    "        case_id = image_path.stem\n",
    "        mask_path = None\n",
    "        candidate = mask_root / f\"{case_id}.npy\"\n",
    "        if candidate.exists():\n",
    "            mask_path = candidate\n",
    "        elif strict:\n",
    "            raise FileNotFoundError(f\"Missing supplemental mask for {case_id}\")\n",
    "\n",
    "        samples.append(\n",
    "            Sample(\n",
    "                case_id=case_id,\n",
    "                image_path=image_path,\n",
    "                mask_path=mask_path,\n",
    "                is_authentic=False if mask_path is not None else None,\n",
    "                split=\"supplemental\",\n",
    "                label=None,\n",
    "                rel_path=image_path.relative_to(data_root),\n",
    "            )\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "\n",
    "def build_test_index(data_root: str | Path) -> List[Sample]:\n",
    "    data_root = Path(data_root)\n",
    "    test_root = data_root / \"test_images\"\n",
    "\n",
    "    samples: List[Sample] = []\n",
    "    for image_path in sorted(test_root.glob(\"*.png\")):\n",
    "        case_id = image_path.stem\n",
    "        samples.append(\n",
    "            Sample(\n",
    "                case_id=case_id,\n",
    "                image_path=image_path,\n",
    "                mask_path=None,\n",
    "                is_authentic=None,\n",
    "                split=\"test\",\n",
    "                label=None,\n",
    "                rel_path=image_path.relative_to(data_root),\n",
    "            )\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "\n",
    "def load_image(image_path: str | Path, as_rgb: bool = True) -> np.ndarray:\n",
    "    from PIL import Image\n",
    "\n",
    "    image_path = Path(image_path)\n",
    "    with Image.open(image_path) as img:\n",
    "        if as_rgb:\n",
    "            img = img.convert(\"RGB\")\n",
    "        return np.array(img)\n",
    "\n",
    "\n",
    "def load_mask_instances(mask_path: str | Path) -> List[np.ndarray]:\n",
    "    mask_path = Path(mask_path)\n",
    "    masks = np.load(mask_path)\n",
    "    if masks.ndim == 2:\n",
    "        masks = masks[None, ...]\n",
    "    return [(m > 0).astype(np.uint8) for m in masks]\n",
    "\n",
    "\n",
    "def _load_union_mask(mask_path: Optional[Path], shape: tuple[int, int]) -> np.ndarray:\n",
    "    if mask_path is None:\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "    masks = np.load(mask_path)\n",
    "    if masks.ndim == 2:\n",
    "        union = masks\n",
    "    else:\n",
    "        union = masks.max(axis=0)\n",
    "    union = (union > 0).astype(np.uint8)\n",
    "    if union.shape != shape:\n",
    "        raise ValueError(f\"Mask shape {union.shape} does not match image shape {shape}\")\n",
    "    return union\n",
    "\n",
    "\n",
    "def _pad_to_size(image: np.ndarray, mask: np.ndarray, target_h: int, target_w: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    h, w = mask.shape\n",
    "    pad_h = max(target_h - h, 0)\n",
    "    pad_w = max(target_w - w, 0)\n",
    "    if pad_h == 0 and pad_w == 0:\n",
    "        return image, mask\n",
    "    image_pad = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"constant\")\n",
    "    mask_pad = np.pad(mask, ((0, pad_h), (0, pad_w)), mode=\"constant\")\n",
    "    return image_pad, mask_pad\n",
    "\n",
    "\n",
    "def _random_crop(\n",
    "    image: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    crop_h: int,\n",
    "    crop_w: int,\n",
    "    rng: np.random.Generator,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    h, w = mask.shape\n",
    "    if h == crop_h and w == crop_w:\n",
    "        return image, mask\n",
    "    top = int(rng.integers(0, h - crop_h + 1))\n",
    "    left = int(rng.integers(0, w - crop_w + 1))\n",
    "    return image[top : top + crop_h, left : left + crop_w], mask[top : top + crop_h, left : left + crop_w]\n",
    "\n",
    "\n",
    "def _center_crop(image: np.ndarray, mask: np.ndarray, crop_h: int, crop_w: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    h, w = mask.shape\n",
    "    top = max((h - crop_h) // 2, 0)\n",
    "    left = max((w - crop_w) // 2, 0)\n",
    "    return image[top : top + crop_h, left : left + crop_w], mask[top : top + crop_h, left : left + crop_w]\n",
    "\n",
    "\n",
    "def _positive_crop(\n",
    "    image: np.ndarray,\n",
    "    mask: np.ndarray,\n",
    "    crop_h: int,\n",
    "    crop_w: int,\n",
    "    rng: np.random.Generator,\n",
    "    max_tries: int,\n",
    "    min_pos_pixels: int,\n",
    ") -> tuple[np.ndarray, np.ndarray]:\n",
    "    ys, xs = np.where(mask > 0)\n",
    "    if len(ys) == 0:\n",
    "        return _random_crop(image, mask, crop_h, crop_w, rng)\n",
    "\n",
    "    h, w = mask.shape\n",
    "    for _ in range(max_tries):\n",
    "        idx = int(rng.integers(0, len(ys)))\n",
    "        center_y = int(ys[idx])\n",
    "        center_x = int(xs[idx])\n",
    "        top = max(min(center_y - crop_h // 2, h - crop_h), 0)\n",
    "        left = max(min(center_x - crop_w // 2, w - crop_w), 0)\n",
    "        crop_mask = mask[top : top + crop_h, left : left + crop_w]\n",
    "        if int(crop_mask.sum()) >= min_pos_pixels:\n",
    "            return image[top : top + crop_h, left : left + crop_w], crop_mask\n",
    "\n",
    "    return _random_crop(image, mask, crop_h, crop_w, rng)\n",
    "\n",
    "\n",
    "class PatchDataset(torch.utils.data.Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        samples: List[Sample],\n",
    "        patch_size: int | tuple[int, int] = 512,\n",
    "        train: bool = True,\n",
    "        augment=None,\n",
    "        positive_prob: float = 0.7,\n",
    "        min_pos_pixels: int = 1,\n",
    "        max_tries: int = 10,\n",
    "        seed: int = 42,\n",
    "        return_meta: bool = False,\n",
    "        mean=IMAGENET_MEAN,\n",
    "        std=IMAGENET_STD,\n",
    "        normalize: bool = True,\n",
    "    ) -> None:\n",
    "        self.samples = samples\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = (patch_size, patch_size)\n",
    "        self.patch_size = patch_size\n",
    "        self.train = train\n",
    "        self.augment = augment\n",
    "        self.positive_prob = float(positive_prob)\n",
    "        self.min_pos_pixels = int(min_pos_pixels)\n",
    "        self.max_tries = int(max_tries)\n",
    "        self.seed = int(seed)\n",
    "        self.return_meta = bool(return_meta)\n",
    "        self.mean = np.array(mean, dtype=np.float32)\n",
    "        self.std = np.array(std, dtype=np.float32)\n",
    "        self.normalize = bool(normalize)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        sample = self.samples[idx]\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        worker_id = worker_info.id if worker_info is not None else 0\n",
    "        rng = np.random.default_rng(self.seed + idx + worker_id * 100000)\n",
    "\n",
    "        image = load_image(sample.image_path)\n",
    "        mask = _load_union_mask(sample.mask_path, image.shape[:2])\n",
    "\n",
    "        crop_h, crop_w = self.patch_size\n",
    "        image, mask = _pad_to_size(image, mask, crop_h, crop_w)\n",
    "        if self.train:\n",
    "            wants_positive = (sample.is_authentic is False) and (rng.random() < self.positive_prob)\n",
    "            if wants_positive:\n",
    "                image, mask = _positive_crop(\n",
    "                    image,\n",
    "                    mask,\n",
    "                    crop_h,\n",
    "                    crop_w,\n",
    "                    rng,\n",
    "                    self.max_tries,\n",
    "                    self.min_pos_pixels,\n",
    "                )\n",
    "            else:\n",
    "                image, mask = _random_crop(image, mask, crop_h, crop_w, rng)\n",
    "        else:\n",
    "            image, mask = _center_crop(image, mask, crop_h, crop_w)\n",
    "\n",
    "        if self.augment is not None:\n",
    "            augmented = self.augment(image=image, mask=mask)\n",
    "            image = augmented[\"image\"]\n",
    "            mask = augmented[\"mask\"]\n",
    "\n",
    "        image = image.astype(np.float32)\n",
    "        if image.max() > 1.0:\n",
    "            image /= 255.0\n",
    "        if self.normalize:\n",
    "            image = (image - self.mean) / self.std\n",
    "\n",
    "        image = np.transpose(image, (2, 0, 1))\n",
    "        mask = mask.astype(np.float32)[None, ...]\n",
    "\n",
    "        x = torch.from_numpy(image)\n",
    "        y = torch.from_numpy(mask)\n",
    "        if self.return_meta:\n",
    "            return x, y, sample\n",
    "        return x, y\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Augmentations (Albumentations + copy-move sintético)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def _seg_as_hw(size: int | tuple[int, int] | None) -> tuple[int, int] | None:\n",
    "    if size is None:\n",
    "        return None\n",
    "    if isinstance(size, int):\n",
    "        return (int(size), int(size))\n",
    "    return int(size[0]), int(size[1])\n",
    "\n",
    "\n",
    "def _seg_has_param(transform_cls, name: str) -> bool:\n",
    "    import inspect\n",
    "\n",
    "    return name in inspect.signature(transform_cls).parameters\n",
    "\n",
    "\n",
    "def _seg_fill_kwargs(transform_cls, fill_value: float | int = 0) -> dict:\n",
    "    kwargs: dict[str, Any] = {}\n",
    "    if _seg_has_param(transform_cls, \"fill\"):\n",
    "        kwargs[\"fill\"] = fill_value\n",
    "    if _seg_has_param(transform_cls, \"fill_mask\"):\n",
    "        kwargs[\"fill_mask\"] = fill_value\n",
    "    if _seg_has_param(transform_cls, \"value\"):\n",
    "        kwargs[\"value\"] = fill_value\n",
    "    if _seg_has_param(transform_cls, \"mask_value\"):\n",
    "        kwargs[\"mask_value\"] = fill_value\n",
    "    if _seg_has_param(transform_cls, \"cval\"):\n",
    "        kwargs[\"cval\"] = fill_value\n",
    "    if _seg_has_param(transform_cls, \"cval_mask\"):\n",
    "        kwargs[\"cval_mask\"] = fill_value\n",
    "    return kwargs\n",
    "\n",
    "\n",
    "def _seg_random_resized_crop(size_hw: tuple[int, int], scale: tuple[float, float], ratio: tuple[float, float], p: float):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for augmentations\")\n",
    "    if _seg_has_param(A.RandomResizedCrop, \"size\"):\n",
    "        return A.RandomResizedCrop(size=size_hw, scale=scale, ratio=ratio, p=p)\n",
    "    return A.RandomResizedCrop(height=size_hw[0], width=size_hw[1], scale=scale, ratio=ratio, p=p)\n",
    "\n",
    "\n",
    "def _seg_image_compression(quality_range: tuple[int, int], p: float):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for augmentations\")\n",
    "    if _seg_has_param(A.ImageCompression, \"quality_range\"):\n",
    "        return A.ImageCompression(quality_range=quality_range, p=p)\n",
    "    return A.ImageCompression(quality_lower=quality_range[0], quality_upper=quality_range[1], p=p)\n",
    "\n",
    "\n",
    "def _seg_coarse_dropout(p: float):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for augmentations\")\n",
    "    if _seg_has_param(A.CoarseDropout, \"num_holes_range\"):\n",
    "        return A.CoarseDropout(\n",
    "            num_holes_range=(1, 8),\n",
    "            hole_height_range=(0.03, 0.20),\n",
    "            hole_width_range=(0.03, 0.20),\n",
    "            fill=0,\n",
    "            p=p,\n",
    "        )\n",
    "    return A.CoarseDropout(\n",
    "        max_holes=8,\n",
    "        max_height=64,\n",
    "        max_width=64,\n",
    "        min_holes=1,\n",
    "        min_height=8,\n",
    "        min_width=8,\n",
    "        fill_value=0,\n",
    "        p=p,\n",
    "    )\n",
    "\n",
    "\n",
    "def _seg_gauss_noise(p: float):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for augmentations\")\n",
    "    if _seg_has_param(A.GaussNoise, \"std_range\"):\n",
    "        # Roughly matches var_limit=(5..50) for uint8 images (std ~= 1..5 px).\n",
    "        return A.GaussNoise(std_range=(0.005, 0.02), p=p)\n",
    "    return A.GaussNoise(var_limit=(5.0, 50.0), p=p)\n",
    "\n",
    "\n",
    "if A is not None:\n",
    "    from albumentations.core.transforms_interface import DualTransform\n",
    "\n",
    "    class CopyMoveTransform(DualTransform):\n",
    "        \"\"\"Copy-move sintético (só quando a máscara chega vazia).\n",
    "\n",
    "        Copia uma região da imagem e cola em outro lugar e marca origem+destino na máscara.\n",
    "        \"\"\"\n",
    "\n",
    "        def __init__(\n",
    "            self,\n",
    "            min_area_frac: float = 0.05,\n",
    "            max_area_frac: float = 0.20,\n",
    "            rotation_limit: float = 15.0,\n",
    "            scale_range: tuple[float, float] = (0.9, 1.1),\n",
    "            irregular_prob: float = 0.5,\n",
    "            max_tries: int = 10,\n",
    "            only_if_empty_mask: bool = True,\n",
    "            p: float = 0.25,\n",
    "        ) -> None:\n",
    "            super().__init__(p=float(p))\n",
    "            self.min_area_frac = float(min_area_frac)\n",
    "            self.max_area_frac = float(max_area_frac)\n",
    "            self.rotation_limit = float(rotation_limit)\n",
    "            self.scale_range = (float(scale_range[0]), float(scale_range[1]))\n",
    "            self.irregular_prob = float(irregular_prob)\n",
    "            self.max_tries = int(max_tries)\n",
    "            self.only_if_empty_mask = bool(only_if_empty_mask)\n",
    "\n",
    "            if not (0.0 < self.min_area_frac <= self.max_area_frac <= 1.0):\n",
    "                raise ValueError(\"Expected 0 < min_area_frac <= max_area_frac <= 1\")\n",
    "            if self.max_tries <= 0:\n",
    "                raise ValueError(\"max_tries must be > 0\")\n",
    "            if not (0.0 <= self.irregular_prob <= 1.0):\n",
    "                raise ValueError(\"irregular_prob must be in [0, 1]\")\n",
    "            if self.scale_range[0] <= 0.0 or self.scale_range[1] <= 0.0:\n",
    "                raise ValueError(\"scale_range values must be > 0\")\n",
    "\n",
    "        @property\n",
    "        def targets_as_params(self):  # type: ignore[override]\n",
    "            return [\"image\", \"mask\"]\n",
    "\n",
    "        def get_params_dependent_on_data(self, params: dict, data: dict) -> dict:\n",
    "            image = data[\"image\"]\n",
    "            mask = data.get(\"mask\")\n",
    "            if mask is None:\n",
    "                return {\"do\": False}\n",
    "\n",
    "            mask = np.asarray(mask)\n",
    "            if mask.ndim != 2:\n",
    "                return {\"do\": False}\n",
    "\n",
    "            if self.only_if_empty_mask and mask.max() > 0:\n",
    "                return {\"do\": False}\n",
    "\n",
    "            h, w = mask.shape\n",
    "            if h < 16 or w < 16:\n",
    "                return {\"do\": False}\n",
    "\n",
    "            rg = self.random_generator\n",
    "\n",
    "            area_frac = float(rg.uniform(self.min_area_frac, self.max_area_frac))\n",
    "            target_area = area_frac * float(h * w)\n",
    "\n",
    "            aspect = float(np.exp(rg.uniform(np.log(0.75), np.log(1.3333333333333333))))\n",
    "            patch_h = int(round(np.sqrt(target_area / aspect)))\n",
    "            patch_w = int(round(patch_h * aspect))\n",
    "            patch_h = int(np.clip(patch_h, 8, h - 1))\n",
    "            patch_w = int(np.clip(patch_w, 8, w - 1))\n",
    "\n",
    "            y_choices = h - patch_h + 1\n",
    "            x_choices = w - patch_w + 1\n",
    "            if y_choices <= 0 or x_choices <= 0:\n",
    "                return {\"do\": False}\n",
    "            if y_choices == 1 and x_choices == 1:\n",
    "                return {\"do\": False}\n",
    "\n",
    "            src_y = int(rg.integers(0, y_choices))\n",
    "            src_x = int(rg.integers(0, x_choices))\n",
    "\n",
    "            chosen: tuple[int, int] | None = None\n",
    "            for _ in range(self.max_tries):\n",
    "                cand_y = int(rg.integers(0, y_choices))\n",
    "                cand_x = int(rg.integers(0, x_choices))\n",
    "                if cand_y == src_y and cand_x == src_x:\n",
    "                    continue\n",
    "\n",
    "                if chosen is None:\n",
    "                    chosen = (cand_y, cand_x)\n",
    "\n",
    "                y_overlap = max(0, min(src_y + patch_h, cand_y + patch_h) - max(src_y, cand_y))\n",
    "                x_overlap = max(0, min(src_x + patch_w, cand_x + patch_w) - max(src_x, cand_x))\n",
    "                if (y_overlap * x_overlap) == 0:\n",
    "                    chosen = (cand_y, cand_x)\n",
    "                    break\n",
    "\n",
    "            if chosen is None:\n",
    "                dst_y = (src_y + 1) % y_choices if y_choices > 1 else src_y\n",
    "                dst_x = (src_x + 1) % x_choices if x_choices > 1 else src_x\n",
    "                if dst_y == src_y and dst_x == src_x:\n",
    "                    return {\"do\": False}\n",
    "            else:\n",
    "                dst_y, dst_x = chosen\n",
    "\n",
    "            if float(rg.random()) < self.irregular_prob:\n",
    "                yy, xx = np.mgrid[0:patch_h, 0:patch_w]\n",
    "                cy = float(patch_h - 1) / 2.0 + float(rg.uniform(-0.15, 0.15)) * patch_h\n",
    "                cx = float(patch_w - 1) / 2.0 + float(rg.uniform(-0.15, 0.15)) * patch_w\n",
    "                ry = max(2.0, float(rg.uniform(0.35, 0.55)) * patch_h)\n",
    "                rx = max(2.0, float(rg.uniform(0.35, 0.55)) * patch_w)\n",
    "                mask_src = (((yy - cy) / ry) ** 2 + ((xx - cx) / rx) ** 2 <= 1.0).astype(np.uint8)\n",
    "            else:\n",
    "                mask_src = np.ones((patch_h, patch_w), dtype=np.uint8)\n",
    "\n",
    "            if int(mask_src.sum()) == 0:\n",
    "                mask_src = np.ones((patch_h, patch_w), dtype=np.uint8)\n",
    "\n",
    "            angle = float(rg.uniform(-self.rotation_limit, self.rotation_limit)) if self.rotation_limit > 0 else 0.0\n",
    "            scale = float(rg.uniform(self.scale_range[0], self.scale_range[1]))\n",
    "\n",
    "            if cv2 is not None and (abs(angle) > 1e-6 or abs(scale - 1.0) > 1e-6):\n",
    "                center = (float(patch_w) / 2.0, float(patch_h) / 2.0)\n",
    "                mat = cv2.getRotationMatrix2D(center, angle, scale)\n",
    "                mask_dst = cv2.warpAffine(\n",
    "                    mask_src,\n",
    "                    mat,\n",
    "                    dsize=(patch_w, patch_h),\n",
    "                    flags=cv2.INTER_NEAREST,\n",
    "                    borderMode=cv2.BORDER_CONSTANT,\n",
    "                    borderValue=0,\n",
    "                ).astype(np.uint8)\n",
    "                if int(mask_dst.sum()) == 0:\n",
    "                    mask_dst = mask_src\n",
    "            else:\n",
    "                mat = None\n",
    "                mask_dst = mask_src\n",
    "\n",
    "            return {\n",
    "                \"do\": True,\n",
    "                \"src_y\": src_y,\n",
    "                \"src_x\": src_x,\n",
    "                \"dst_y\": dst_y,\n",
    "                \"dst_x\": dst_x,\n",
    "                \"patch_h\": patch_h,\n",
    "                \"patch_w\": patch_w,\n",
    "                \"mask_src\": mask_src,\n",
    "                \"mask_dst\": mask_dst,\n",
    "                \"mat\": mat,\n",
    "            }\n",
    "\n",
    "        def apply(self, img: np.ndarray, *args, **params) -> np.ndarray:\n",
    "            if not params.get(\"do\", False):\n",
    "                return img\n",
    "\n",
    "            src_y = int(params[\"src_y\"])\n",
    "            src_x = int(params[\"src_x\"])\n",
    "            dst_y = int(params[\"dst_y\"])\n",
    "            dst_x = int(params[\"dst_x\"])\n",
    "            patch_h = int(params[\"patch_h\"])\n",
    "            patch_w = int(params[\"patch_w\"])\n",
    "            mask_dst = np.asarray(params[\"mask_dst\"]).astype(bool)\n",
    "            mat = params.get(\"mat\", None)\n",
    "\n",
    "            out = img.copy()\n",
    "            src_patch = out[src_y : src_y + patch_h, src_x : src_x + patch_w].copy()\n",
    "            if mat is not None and cv2 is not None:\n",
    "                src_patch = cv2.warpAffine(\n",
    "                    src_patch,\n",
    "                    mat,\n",
    "                    dsize=(patch_w, patch_h),\n",
    "                    flags=cv2.INTER_LINEAR,\n",
    "                    borderMode=cv2.BORDER_REFLECT_101,\n",
    "                )\n",
    "\n",
    "            dst_patch = out[dst_y : dst_y + patch_h, dst_x : dst_x + patch_w]\n",
    "            dst_patch[mask_dst] = src_patch[mask_dst]\n",
    "            out[dst_y : dst_y + patch_h, dst_x : dst_x + patch_w] = dst_patch\n",
    "            return out\n",
    "\n",
    "        def apply_to_mask(self, mask: np.ndarray, *args, **params) -> np.ndarray:\n",
    "            if not params.get(\"do\", False):\n",
    "                return mask\n",
    "\n",
    "            src_y = int(params[\"src_y\"])\n",
    "            src_x = int(params[\"src_x\"])\n",
    "            dst_y = int(params[\"dst_y\"])\n",
    "            dst_x = int(params[\"dst_x\"])\n",
    "            patch_h = int(params[\"patch_h\"])\n",
    "            patch_w = int(params[\"patch_w\"])\n",
    "            mask_src = np.asarray(params[\"mask_src\"]).astype(bool)\n",
    "            mask_dst = np.asarray(params[\"mask_dst\"]).astype(bool)\n",
    "\n",
    "            out = (np.asarray(mask) > 0).astype(np.uint8)\n",
    "            out[src_y : src_y + patch_h, src_x : src_x + patch_w][mask_src] = 1\n",
    "            out[dst_y : dst_y + patch_h, dst_x : dst_x + patch_w][mask_dst] = 1\n",
    "            return out\n",
    "\n",
    "else:\n",
    "    CopyMoveTransform = None\n",
    "\n",
    "\n",
    "def get_train_augment(\n",
    "    patch_size: int | tuple[int, int] | None = None,\n",
    "    mean=IMAGENET_MEAN,\n",
    "    std=IMAGENET_STD,\n",
    "    copy_move_prob: float = 0.0,\n",
    "    copy_move_min_area_frac: float = 0.05,\n",
    "    copy_move_max_area_frac: float = 0.20,\n",
    "    copy_move_rotation_limit: float = 15.0,\n",
    "    copy_move_scale_range: tuple[float, float] = (0.9, 1.1),\n",
    "):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for augmentations\")\n",
    "\n",
    "    hw = _seg_as_hw(patch_size)\n",
    "    border_mode = cv2.BORDER_CONSTANT if cv2 is not None else 0\n",
    "\n",
    "    affine_kwargs: dict[str, Any] = {\n",
    "        \"scale\": (0.8, 1.2),\n",
    "        \"translate_percent\": (-0.05, 0.05),\n",
    "        \"rotate\": (-20, 20),\n",
    "        \"p\": 0.75,\n",
    "        **_seg_fill_kwargs(A.Affine, fill_value=0),\n",
    "    }\n",
    "    if _seg_has_param(A.Affine, \"interpolation\"):\n",
    "        affine_kwargs[\"interpolation\"] = cv2.INTER_LINEAR if cv2 is not None else 1\n",
    "    if _seg_has_param(A.Affine, \"mask_interpolation\"):\n",
    "        affine_kwargs[\"mask_interpolation\"] = cv2.INTER_NEAREST if cv2 is not None else 0\n",
    "    if _seg_has_param(A.Affine, \"border_mode\"):\n",
    "        affine_kwargs[\"border_mode\"] = border_mode\n",
    "    elif _seg_has_param(A.Affine, \"mode\"):\n",
    "        affine_kwargs[\"mode\"] = border_mode\n",
    "\n",
    "    transforms = [\n",
    "        *(\n",
    "            [\n",
    "                CopyMoveTransform(\n",
    "                    min_area_frac=copy_move_min_area_frac,\n",
    "                    max_area_frac=copy_move_max_area_frac,\n",
    "                    rotation_limit=copy_move_rotation_limit,\n",
    "                    scale_range=copy_move_scale_range,\n",
    "                    p=float(copy_move_prob),\n",
    "                )\n",
    "            ]\n",
    "            if CopyMoveTransform is not None and float(copy_move_prob) > 0.0\n",
    "            else []\n",
    "        ),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.25),\n",
    "        A.Affine(**affine_kwargs),\n",
    "    ]\n",
    "\n",
    "    if hw is not None:\n",
    "        crop_h, crop_w = hw\n",
    "        transforms.append(_seg_random_resized_crop((crop_h, crop_w), scale=(0.75, 1.0), ratio=(0.85, 1.15), p=0.35))\n",
    "\n",
    "    transforms.extend(\n",
    "        [\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "                    A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "                    A.CLAHE(clip_limit=(1.0, 3.0), p=1.0),\n",
    "                ],\n",
    "                p=0.5,\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    _seg_gauss_noise(p=1.0),\n",
    "                    A.GaussianBlur(blur_limit=(3, 7), p=1.0),\n",
    "                    A.MotionBlur(blur_limit=(3, 7), p=1.0),\n",
    "                ],\n",
    "                p=0.25,\n",
    "            ),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.ElasticTransform(\n",
    "                        alpha=1.0,\n",
    "                        sigma=40.0,\n",
    "                        border_mode=border_mode,\n",
    "                        p=1.0,\n",
    "                        **_seg_fill_kwargs(A.ElasticTransform, fill_value=0),\n",
    "                    ),\n",
    "                    A.GridDistortion(\n",
    "                        num_steps=5,\n",
    "                        distort_limit=0.05,\n",
    "                        border_mode=border_mode,\n",
    "                        p=1.0,\n",
    "                        **_seg_fill_kwargs(A.GridDistortion, fill_value=0),\n",
    "                    ),\n",
    "                    A.OpticalDistortion(\n",
    "                        distort_limit=0.05,\n",
    "                        border_mode=border_mode,\n",
    "                        p=1.0,\n",
    "                        **_seg_fill_kwargs(A.OpticalDistortion, fill_value=0),\n",
    "                        **({\"shift_limit\": 0.05} if _seg_has_param(A.OpticalDistortion, \"shift_limit\") else {}),\n",
    "                    ),\n",
    "                ],\n",
    "                p=0.10,\n",
    "            ),\n",
    "            _seg_coarse_dropout(p=0.20),\n",
    "            _seg_image_compression(quality_range=(60, 100), p=0.10),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return A.Compose(transforms)\n",
    "\n",
    "\n",
    "def get_val_augment(mean=IMAGENET_MEAN, std=IMAGENET_STD):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for augmentations\")\n",
    "    return A.Compose([])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Inferência (tile + resize)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def normalize_image(image: np.ndarray, mean=IMAGENET_MEAN, std=IMAGENET_STD) -> np.ndarray:\n",
    "    image = image.astype(np.float32)\n",
    "    if image.max() > 1.0:\n",
    "        image /= 255.0\n",
    "    mean = np.array(mean, dtype=np.float32)\n",
    "    std = np.array(std, dtype=np.float32)\n",
    "    return (image - mean) / std\n",
    "\n",
    "\n",
    "def _tile_coords(length: int, tile_size: int, overlap: int) -> list[tuple[int, int]]:\n",
    "    stride = int(tile_size) - int(overlap)\n",
    "    if stride <= 0:\n",
    "        raise ValueError(\"tile_size must be larger than overlap\")\n",
    "    if length <= tile_size:\n",
    "        return [(0, tile_size)]\n",
    "    coords = list(range(0, length - tile_size + 1, stride))\n",
    "    if coords[-1] != length - tile_size:\n",
    "        coords.append(length - tile_size)\n",
    "    return [(int(start), int(start + tile_size)) for start in coords]\n",
    "\n",
    "\n",
    "def _pad_image(image: np.ndarray, target_h: int, target_w: int) -> tuple[np.ndarray, tuple[int, int]]:\n",
    "    h, w = image.shape[:2]\n",
    "    pad_h = max(target_h - h, 0)\n",
    "    pad_w = max(target_w - w, 0)\n",
    "    if pad_h == 0 and pad_w == 0:\n",
    "        return image, (0, 0)\n",
    "    padded = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"constant\")\n",
    "    return padded, (pad_h, pad_w)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _predict_tensor(model, tensor: torch.Tensor, device: str) -> torch.Tensor:\n",
    "    tensor = tensor.to(device)\n",
    "    logits = model(tensor)\n",
    "    return torch.sigmoid(logits)\n",
    "\n",
    "\n",
    "def predict_image(\n",
    "    model,\n",
    "    image: np.ndarray,\n",
    "    device: str,\n",
    "    tile_size: int = 0,\n",
    "    overlap: int = 0,\n",
    "    max_size: int = 0,\n",
    "    mean=IMAGENET_MEAN,\n",
    "    std=IMAGENET_STD,\n",
    ") -> np.ndarray:\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    orig_h, orig_w = image.shape[:2]\n",
    "\n",
    "    if tile_size and int(tile_size) > 0:\n",
    "        padded, _ = _pad_image(image, int(tile_size), int(tile_size))\n",
    "        pad_h, pad_w = padded.shape[0], padded.shape[1]\n",
    "        pred_sum = np.zeros((pad_h, pad_w), dtype=np.float32)\n",
    "        pred_count = np.zeros((pad_h, pad_w), dtype=np.float32)\n",
    "\n",
    "        ys = _tile_coords(padded.shape[0], int(tile_size), int(overlap))\n",
    "        xs = _tile_coords(padded.shape[1], int(tile_size), int(overlap))\n",
    "        for y0, y1 in ys:\n",
    "            for x0, x1 in xs:\n",
    "                tile = padded[y0:y1, x0:x1]\n",
    "                tile_norm = normalize_image(tile, mean=mean, std=std)\n",
    "                tile_tensor = torch.from_numpy(tile_norm).permute(2, 0, 1).unsqueeze(0)\n",
    "                probs = _predict_tensor(model, tile_tensor, device)\n",
    "                prob_tile = probs.squeeze(0).squeeze(0).cpu().numpy()\n",
    "                pred_sum[y0:y1, x0:x1] += prob_tile\n",
    "                pred_count[y0:y1, x0:x1] += 1.0\n",
    "\n",
    "        pred = pred_sum / np.maximum(pred_count, 1.0)\n",
    "        return pred[:orig_h, :orig_w]\n",
    "\n",
    "    image_norm = normalize_image(image, mean=mean, std=std)\n",
    "    tensor = torch.from_numpy(image_norm).permute(2, 0, 1).unsqueeze(0)\n",
    "    if max_size and max(orig_h, orig_w) > int(max_size):\n",
    "        scale = int(max_size) / float(max(orig_h, orig_w))\n",
    "        new_h = int(round(orig_h * scale))\n",
    "        new_w = int(round(orig_w * scale))\n",
    "        tensor = F.interpolate(tensor, size=(new_h, new_w), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    probs = _predict_tensor(model, tensor, device)\n",
    "    if probs.shape[-2:] != (orig_h, orig_w):\n",
    "        probs = F.interpolate(probs, size=(orig_h, orig_w), mode=\"bilinear\", align_corners=False)\n",
    "    return probs.squeeze(0).squeeze(0).cpu().numpy()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Pós-processamento (binário + CC)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def binarize(mask: np.ndarray, threshold: float = 0.5) -> np.ndarray:\n",
    "    return (np.asarray(mask) >= float(threshold)).astype(np.uint8)\n",
    "\n",
    "\n",
    "def extract_components(mask: np.ndarray, min_area: int = 0) -> List[np.ndarray]:\n",
    "    mask = (np.asarray(mask) > 0).astype(np.uint8)\n",
    "    if mask.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D mask, got shape {mask.shape}\")\n",
    "    if mask.max() == 0:\n",
    "        return []\n",
    "\n",
    "    instances: List[np.ndarray] = []\n",
    "    if _cc_label is not None:\n",
    "        labeled, num = _cc_label(mask > 0)\n",
    "        for idx in range(1, int(num) + 1):\n",
    "            comp = (labeled == idx)\n",
    "            if min_area and int(comp.sum()) < int(min_area):\n",
    "                continue\n",
    "            instances.append(comp.astype(np.uint8))\n",
    "        return instances\n",
    "\n",
    "    if cv2 is None:\n",
    "        raise ImportError(\"connected components requires scipy or opencv-python\")\n",
    "\n",
    "    num_labels, labels = cv2.connectedComponents(mask, connectivity=4)\n",
    "    for idx in range(1, int(num_labels)):\n",
    "        comp = (labels == idx)\n",
    "        if min_area and int(comp.sum()) < int(min_area):\n",
    "            continue\n",
    "        instances.append(comp.astype(np.uint8))\n",
    "    return instances\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# RLE (formato oficial)\n",
    "# -----------------------------\n",
    "\n",
    "AUTHENTIC_LABEL = \"authentic\"\n",
    "\n",
    "\n",
    "def _normalize_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    arr = np.asarray(mask)\n",
    "    if arr.ndim != 2:\n",
    "        raise ValueError(f\"Expected 2D mask, got shape {arr.shape}\")\n",
    "    return (arr > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def rle_encode(mask: np.ndarray) -> List[int]:\n",
    "    mask = _normalize_mask(mask)\n",
    "    if mask.max() == 0:\n",
    "        return []\n",
    "\n",
    "    pixels = mask.flatten(order=\"F\")\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    changes = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    changes[1::2] -= changes[::2]\n",
    "    return changes.tolist()\n",
    "\n",
    "\n",
    "def rle_decode(rle: Sequence[int] | str | None, shape: tuple[int, int]) -> np.ndarray:\n",
    "    if rle is None:\n",
    "        return np.zeros(shape, dtype=np.uint8)\n",
    "\n",
    "    if isinstance(rle, str):\n",
    "        text = rle.strip()\n",
    "        if text == \"\" or text.lower() == AUTHENTIC_LABEL:\n",
    "            return np.zeros(shape, dtype=np.uint8)\n",
    "        if text.startswith(\"[\"):\n",
    "            rle = json.loads(text)\n",
    "        else:\n",
    "            rle = [int(x) for x in text.split()]\n",
    "\n",
    "    rle = list(rle)\n",
    "    if len(rle) % 2 != 0:\n",
    "        raise ValueError(\"RLE length must be even (start, length pairs)\")\n",
    "\n",
    "    mask = np.zeros(shape[0] * shape[1], dtype=np.uint8)\n",
    "    for start, length in zip(rle[0::2], rle[1::2]):\n",
    "        if length <= 0:\n",
    "            continue\n",
    "        start_index = int(start) - 1\n",
    "        end_index = start_index + int(length)\n",
    "        mask[start_index:end_index] = 1\n",
    "\n",
    "    return mask.reshape(shape, order=\"F\")\n",
    "\n",
    "\n",
    "def _normalize_instances(masks: Iterable[np.ndarray] | np.ndarray | None) -> List[np.ndarray]:\n",
    "    if masks is None:\n",
    "        return []\n",
    "    if isinstance(masks, np.ndarray):\n",
    "        if masks.ndim == 2:\n",
    "            return [_normalize_mask(masks)]\n",
    "        if masks.ndim == 3:\n",
    "            return [_normalize_mask(m) for m in masks]\n",
    "    if isinstance(masks, (list, tuple)):\n",
    "        return [_normalize_mask(m) for m in masks]\n",
    "    raise ValueError(\"Unsupported mask container for RLE encoding\")\n",
    "\n",
    "\n",
    "def encode_instances(masks: Iterable[np.ndarray] | np.ndarray | None) -> str:\n",
    "    instances = _normalize_instances(masks)\n",
    "    parts: List[str] = []\n",
    "    for mask in instances:\n",
    "        runs = rle_encode(mask)\n",
    "        if runs:\n",
    "            parts.append(json.dumps(runs))\n",
    "    if not parts:\n",
    "        return AUTHENTIC_LABEL\n",
    "    return \";\".join(parts)\n",
    "\n",
    "\n",
    "def decode_annotation(annotation: str | None, shape: tuple[int, int]) -> List[np.ndarray]:\n",
    "    if annotation is None:\n",
    "        return []\n",
    "\n",
    "    text = annotation.strip()\n",
    "    if text == \"\" or text.lower() == AUTHENTIC_LABEL:\n",
    "        return []\n",
    "\n",
    "    masks: List[np.ndarray] = []\n",
    "    for part in text.split(\";\"):\n",
    "        part = part.strip()\n",
    "        if not part:\n",
    "            continue\n",
    "        masks.append(rle_decode(part, shape))\n",
    "    return masks\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Métrica (oF1 por instância)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "def _as_instance_list(masks: Iterable[np.ndarray] | np.ndarray | None) -> List[np.ndarray]:\n",
    "    if masks is None:\n",
    "        return []\n",
    "    if isinstance(masks, np.ndarray):\n",
    "        if masks.ndim == 2:\n",
    "            return extract_components(masks)\n",
    "        if masks.ndim == 3:\n",
    "            return [(masks[i] > 0).astype(np.uint8) for i in range(masks.shape[0])]\n",
    "    if isinstance(masks, (list, tuple)):\n",
    "        return [(np.asarray(m) > 0).astype(np.uint8) for m in masks]\n",
    "    raise ValueError(\"Unsupported mask container\")\n",
    "\n",
    "\n",
    "def _build_f1_matrix(gt_instances: List[np.ndarray], pred_instances: List[np.ndarray]) -> np.ndarray:\n",
    "    gt_count = len(gt_instances)\n",
    "    pred_count = len(pred_instances)\n",
    "    if gt_count == 0 or pred_count == 0:\n",
    "        return np.zeros((gt_count, pred_count), dtype=np.float32)\n",
    "\n",
    "    gt_sums = [int(m.sum()) for m in gt_instances]\n",
    "    pred_sums = [int(m.sum()) for m in pred_instances]\n",
    "\n",
    "    f1_matrix = np.zeros((gt_count, pred_count), dtype=np.float32)\n",
    "    for i, gt_mask in enumerate(gt_instances):\n",
    "        if gt_sums[i] == 0:\n",
    "            continue\n",
    "        for j, pred_mask in enumerate(pred_instances):\n",
    "            if pred_sums[j] == 0:\n",
    "                continue\n",
    "            intersection = np.logical_and(gt_mask, pred_mask).sum()\n",
    "            if intersection == 0:\n",
    "                continue\n",
    "            f1_matrix[i, j] = (2.0 * intersection) / (gt_sums[i] + pred_sums[j])\n",
    "    return f1_matrix\n",
    "\n",
    "\n",
    "def score_image(gt_masks: Iterable[np.ndarray] | np.ndarray | None, pred_masks: Iterable[np.ndarray] | np.ndarray | None) -> float:\n",
    "    gt_instances = _as_instance_list(gt_masks)\n",
    "    pred_instances = _as_instance_list(pred_masks)\n",
    "\n",
    "    gt_count = len(gt_instances)\n",
    "    pred_count = len(pred_instances)\n",
    "\n",
    "    if gt_count == 0 and pred_count == 0:\n",
    "        return 1.0\n",
    "    if gt_count == 0 and pred_count > 0:\n",
    "        return 0.0\n",
    "    if gt_count > 0 and pred_count == 0:\n",
    "        return 0.0\n",
    "\n",
    "    if _linear_sum_assignment is None:\n",
    "        raise ImportError(\"scipy is required for Hungarian matching\")\n",
    "\n",
    "    f1_matrix = _build_f1_matrix(gt_instances, pred_instances)\n",
    "    row_ind, col_ind = _linear_sum_assignment(-f1_matrix)\n",
    "    if row_ind.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    matched = f1_matrix[row_ind, col_ind]\n",
    "    if pred_count < gt_count:\n",
    "        base = float(matched.sum() / gt_count)\n",
    "    else:\n",
    "        base = float(matched.mean())\n",
    "    penalty = gt_count / max(pred_count, gt_count)\n",
    "    return base * penalty\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Treino (segmentação)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TrainStats:\n",
    "    loss: float\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _batch_dice(logits: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5) -> torch.Tensor:\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= float(threshold)).float()\n",
    "    targets = targets.float()\n",
    "    dims = (1, 2, 3)\n",
    "    intersection = (preds * targets).sum(dim=dims)\n",
    "    denom = preds.sum(dim=dims) + targets.sum(dim=dims)\n",
    "    dice = (2.0 * intersection + 1.0) / (denom + 1.0)\n",
    "    return dice.mean()\n",
    "\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer, device: str, use_amp: bool = False) -> TrainStats:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "    for images, masks in loader:\n",
    "        images = images.to(device)\n",
    "        masks = masks.to(device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=use_amp):\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, masks)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        total_loss += float(loss.item()) * images.size(0)\n",
    "\n",
    "    return TrainStats(loss=total_loss / max(len(loader.dataset), 1))\n",
    "\n",
    "\n",
    "def validate(model, loader, criterion, device: str) -> tuple[TrainStats, float]:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    dice_scores: list[float] = []\n",
    "    with torch.no_grad():\n",
    "        for images, masks in loader:\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            logits = model(images)\n",
    "            loss = criterion(logits, masks)\n",
    "            total_loss += float(loss.item()) * images.size(0)\n",
    "            dice_scores.append(float(_batch_dice(logits, masks)))\n",
    "\n",
    "    mean_loss = total_loss / max(len(loader.dataset), 1)\n",
    "    mean_dice = float(sum(dice_scores) / max(len(dice_scores), 1))\n",
    "    return TrainStats(loss=mean_loss), mean_dice\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Losses (segmentação)\n",
    "# -----------------------------\n",
    "\n",
    "\n",
    "class TverskyLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha: float = 0.7, beta: float = 0.3, smooth: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "        self.alpha = float(alpha)\n",
    "        self.beta = float(beta)\n",
    "        self.smooth = float(smooth)\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "        probs = probs.view(probs.size(0), -1)\n",
    "        targets = targets.view(targets.size(0), -1)\n",
    "        tp = (probs * targets).sum(dim=1)\n",
    "        fp = (probs * (1.0 - targets)).sum(dim=1)\n",
    "        fn = ((1.0 - probs) * targets).sum(dim=1)\n",
    "        tversky = (tp + self.smooth) / (tp + self.alpha * fp + self.beta * fn + self.smooth)\n",
    "        return 1.0 - tversky.mean()\n",
    "\n",
    "\n",
    "class BCETverskyLoss(torch.nn.Module):\n",
    "    def __init__(self, alpha: float = 0.7, beta: float = 0.3, tversky_weight: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "        self.bce = torch.nn.BCEWithLogitsLoss()\n",
    "        self.tversky = TverskyLoss(alpha=alpha, beta=beta)\n",
    "        self.tversky_weight = float(tversky_weight)\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        return self.bce(logits, targets) + self.tversky_weight * self.tversky(logits, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250313a9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Célula 4 — Paths do dataset (Kaggle/local) + config\n",
    "\n",
    "KAGGLE_COMP_DATASET = Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "if (KAGGLE_COMP_DATASET / \"train_images\").exists():\n",
    "    DATA_ROOT = KAGGLE_COMP_DATASET\n",
    "elif (PROJECT_ROOT / \"data\" / \"train_images\").exists():\n",
    "    DATA_ROOT = PROJECT_ROOT / \"data\"\n",
    "else:\n",
    "    DATA_ROOT = PROJECT_ROOT / \"data\" / \"recodai\"\n",
    "\n",
    "OUTPUT_ROOT = Path(\"/kaggle/working/outputs\") if is_kaggle() else (PROJECT_ROOT / \"outputs\")\n",
    "OUTPUT_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "CONFIG_PATH = PROJECT_ROOT / \"configs\" / \"baseline_fpn_convnext.json\"\n",
    "cfg: dict = {}\n",
    "if CONFIG_PATH.exists():\n",
    "    cfg = json.loads(CONFIG_PATH.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"OUTPUT_ROOT:\", OUTPUT_ROOT)\n",
    "print(\"Config loaded:\", bool(cfg))\n",
    "\n",
    "\n",
    "def _find_checkpoint_root(default_dir: Path, *, expected_subdir: str) -> Path:\n",
    "    \"\"\"\n",
    "    Procura por checkpoints em locais comuns (incluindo `/kaggle/input/*`) e retorna o primeiro diretório\n",
    "    que contenha algum arquivo `.pt`.\n",
    "    \"\"\"\n",
    "    candidates: list[Path] = [Path(default_dir)]\n",
    "\n",
    "    # comum em repos locais: <repo>/outputs/<expected_subdir>\n",
    "    candidates.append(PROJECT_ROOT / \"outputs\" / expected_subdir)\n",
    "    candidates.append(PROJECT_ROOT / \"weights\" / expected_subdir)\n",
    "\n",
    "    if is_kaggle():\n",
    "        kaggle_input = Path(\"/kaggle/input\")\n",
    "        if kaggle_input.exists():\n",
    "            for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "                candidates.append(ds / \"outputs\" / expected_subdir)\n",
    "                candidates.append(ds / expected_subdir)\n",
    "                candidates.append(ds / \"weights\" / expected_subdir)\n",
    "                candidates.append(ds / \"recodai_bundle\" / \"outputs\" / expected_subdir)\n",
    "                candidates.append(ds / \"recodai_bundle\" / expected_subdir)\n",
    "                candidates.append(ds / \"recodai_bundle\" / \"weights\" / expected_subdir)\n",
    "\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            if p.exists() and any(p.glob(\"**/best.pt\")):\n",
    "                return p\n",
    "        except Exception as exc:\n",
    "            print(f\"[CKPT] erro ao checar diretório: {p}\")\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "    return Path(default_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19fa480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5 — Index (train/test) + contagens\n",
    "\n",
    "train_samples = build_train_index(DATA_ROOT)\n",
    "test_samples = build_test_index(DATA_ROOT)\n",
    "\n",
    "print(\"Train samples:\", len(train_samples))\n",
    "print(\"Test samples:\", len(test_samples))\n",
    "print(\"Train authentic:\", sum(1 for s in train_samples if s.is_authentic))\n",
    "print(\"Train forged:\", sum(1 for s in train_samples if s.is_authentic is False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3017364f",
   "metadata": {},
   "source": [
    "## Análise dos Dados e Pré-processamento\n",
    "\n",
    "Antes de treinar modelos, é útil realizar uma exploração dos dados. Cada imagem possui um identificador `case_id`.\n",
    "No dataset de treino, existe um subconjunto de imagens **autênticas** (sem manipulação) e imagens **forjadas**\n",
    "(copy-move). Para as imagens forjadas, há uma máscara de segmentação indicando os pixels duplicados; para imagens\n",
    "autênticas, não há máscara (equivalente a \"nenhum pixel forjado\").\n",
    "\n",
    "**Observação importante (treino):** no snapshot do Kaggle, o mesmo `case_id` pode aparecer em **`authentic/` e\n",
    "`forged/`** (duas imagens diferentes). Para indexar sem colisões, use o caminho relativo (`rel_path`) ou uma chave\n",
    "composta (ex.: `f\\\"{label}/{case_id}\\\"`).\n",
    "\n",
    "O que precisamos construir:\n",
    "\n",
    "- **Segmentação:** pares `(imagem, máscara)` (aqui usamos a **união** das instâncias como máscara binária, e depois\n",
    "  recuperamos instâncias via componentes conexos na hora do `submission`).\n",
    "- **Classificação (opcional):** rótulo binário `y_cls` para decidir se é `authentic` (0) ou `forged` (1).\n",
    "\n",
    "Pré-processamento (baseline deste repo):\n",
    "\n",
    "- Leitura com PIL e conversão para **RGB**.\n",
    "- Conversão para `float32`, escala para `[0, 1]` e **normalização ImageNet**.\n",
    "- Treino *patch-based* (`PatchDataset`): amostra crops de tamanho `patch_size`, com *oversampling* de regiões positivas\n",
    "  em imagens forjadas (controlado por `positive_prob`/`min_pos_pixels`).\n",
    "- Inferência em imagem inteira via **tiling** (`tile_size`/`overlap`) para lidar com imagens grandes.\n",
    "\n",
    "### Dimensionamento e formato (decisão do baseline)\n",
    "\n",
    "- **Canais:** padronizamos todas as imagens para **3 canais (RGB)**. Se a imagem for originalmente em escala de cinza,\n",
    "  duplicamos o canal (via `PIL.Image.convert(\"RGB\")`), o que funciona bem para *backbones* pré-treinados em ImageNet.\n",
    "- **Tamanho no treino:** ao invés de fazer *downscale* agressivo da figura inteira (que pode apagar falsificações\n",
    "  pequenas), treinamos com **patches 512×512** (crop) e fazemos **padding** quando a imagem é menor. Isso fixa o shape\n",
    "  de entrada e mantém detalhes locais.\n",
    "- **Tamanho na inferência:** rodamos em **tiles** (ex.: `tile_size=1024`, `overlap=128`) para preservar resolução em\n",
    "  imagens grandes. Se o runtime ficar inviável, use `MAX_SIZE` para limitar o lado maior (trade-off controlado).\n",
    "\n",
    "### Normalização (decisão do baseline)\n",
    "\n",
    "- **Escala:** convertemos para `float32` e, quando a imagem vem em `uint8`, reescalamos para **[0, 1]**.\n",
    "- **Padronização por canal:** aplicamos **média/desvio do ImageNet** (o padrão esperado por encoders pré-treinados).\n",
    "- **Sem equalização fixa:** não aplicamos equalização/histogram matching como pré-processamento determinístico para não\n",
    "  correr o risco de mascarar/alterar evidências sutis de copy-move. Em vez disso, lidamos com variações de contraste\n",
    "  via **data augmentation** (ex.: `RandomBrightnessContrast`, `RandomGamma`, `CLAHE`) e pela robustez do modelo.\n",
    "\n",
    "### Divisão de dados (decisão do baseline: 5-fold CV + ensemble)\n",
    "\n",
    "Em *code competitions*, o conjunto de teste real é **oculto** e não existe um \"val set oficial\" fixo. Para\n",
    "desenvolvimento local, precisamos criar uma validação a partir do treino:\n",
    "\n",
    "- **Opção simples:** holdout (ex.: 80/20 estratificado).\n",
    "- **Opção de performance:** **K-fold cross-validation** (ex.: 5-fold), treinando 5 modelos e fazendo **ensemble** na\n",
    "  inferência. Isso melhora o uso do treino e tende a reduzir overfitting, mas custa ~5× mais tempo de treino.\n",
    "\n",
    "**Escolha aqui:** usamos **5 folds** e fazemos **ensemble** dos modelos finais.\n",
    "Para evitar vazamento, fazemos o split **agrupando por `case_id`** (quando existe par `authentic/` e `forged/` com o\n",
    "mesmo id, eles caem no mesmo fold).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6aeae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5b — Exemplo: carregando (imagem, máscara) e label binário\n",
    "sample0 = train_samples[0]\n",
    "image0 = load_image(sample0.image_path)\n",
    "gt_instances0 = load_mask_instances(sample0.mask_path) if sample0.mask_path else []\n",
    "\n",
    "if gt_instances0:\n",
    "    union_mask0 = np.max(np.stack(gt_instances0, axis=0), axis=0).astype(np.uint8)\n",
    "else:\n",
    "    union_mask0 = np.zeros(image0.shape[:2], dtype=np.uint8)\n",
    "\n",
    "y_cls0 = 0 if sample0.is_authentic else 1\n",
    "\n",
    "print(\"case_id:\", sample0.case_id)\n",
    "print(\"label:\", sample0.label, \"| y_cls:\", y_cls0)\n",
    "print(\"image shape:\", image0.shape, \"dtype:\", image0.dtype)\n",
    "print(\"instances:\", len(gt_instances0), \"| union mask sum:\", int(union_mask0.sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de938273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5c — EDA rápida (opcional): tamanhos e áreas de máscara\n",
    "RUN_EDA = False  # deixe False no submit; True para explorar interativamente\n",
    "\n",
    "if RUN_EDA:\n",
    "    import pandas as pd\n",
    "    from IPython.display import display\n",
    "    from PIL import Image\n",
    "\n",
    "    rows = []\n",
    "    for s in train_samples:\n",
    "        width = height = None\n",
    "        mode = None\n",
    "        with Image.open(s.image_path) as img:\n",
    "            width, height = img.size\n",
    "            mode = img.mode\n",
    "\n",
    "        mask_instances = 0\n",
    "        mask_area = 0\n",
    "        mask_area_frac = 0.0\n",
    "        if s.mask_path is not None:\n",
    "            masks = np.load(s.mask_path)\n",
    "            if masks.ndim == 2:\n",
    "                masks = masks[None, ...]\n",
    "            mask_instances = int(masks.shape[0])\n",
    "            union = masks.max(axis=0)\n",
    "            mask_area = int((union > 0).sum())\n",
    "            mask_area_frac = mask_area / float(width * height)\n",
    "\n",
    "        rows.append(\n",
    "            {\n",
    "                \"case_id\": s.case_id,\n",
    "                \"label\": s.label,\n",
    "                \"rel_path\": str(s.rel_path),\n",
    "                \"width\": width,\n",
    "                \"height\": height,\n",
    "                \"mode\": mode,\n",
    "                \"mask_instances\": mask_instances,\n",
    "                \"mask_area\": mask_area,\n",
    "                \"mask_area_frac\": mask_area_frac,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df_train = pd.DataFrame(rows)\n",
    "    display(df_train.head())\n",
    "    display(df_train[\"label\"].value_counts())\n",
    "    display(df_train[\"mode\"].value_counts())\n",
    "    print(\"unique case_id:\", int(df_train[\"case_id\"].nunique()))\n",
    "    print(\"duplicated case_id (train):\", int(df_train.duplicated(\"case_id\").sum()))\n",
    "\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        plt.scatter(df_train[\"width\"], df_train[\"height\"], s=3, alpha=0.25)\n",
    "        plt.title(\"Train image sizes (width x height)\")\n",
    "        plt.xlabel(\"width\")\n",
    "        plt.ylabel(\"height\")\n",
    "        plt.show()\n",
    "\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        df_train[df_train[\"label\"] == \"forged\"][\"mask_area_frac\"].hist(bins=40)\n",
    "        plt.title(\"Mask area fraction (forged)\")\n",
    "        plt.xlabel(\"mask_area_frac\")\n",
    "        plt.show()\n",
    "    except Exception as exc:\n",
    "        print(\"[PLOTS] plots indisponíveis (erro abaixo):\")\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7e124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6 — Split em folds (5-fold) com agrupamento por case_id\n",
    "\n",
    "\n",
    "def _case_id_groups(samples) -> dict[str, list[int]]:\n",
    "    groups: dict[str, list[int]] = {}\n",
    "    for idx, s in enumerate(samples):\n",
    "        groups.setdefault(str(s.case_id), []).append(int(idx))\n",
    "    return groups\n",
    "\n",
    "\n",
    "def iter_case_id_folds(samples, n_splits: int, seed: int) -> Iterable[tuple[np.ndarray, np.ndarray]]:\n",
    "    \"\"\"\n",
    "    Gera folds garantindo que o mesmo case_id não apareça em treino e validação.\n",
    "    A estratificação é feita no nível do case_id por \"tipo de grupo\" (par vs solo),\n",
    "    o que ajuda a manter a proporção authentic/forged por fold no snapshot deste dataset.\n",
    "    \"\"\"\n",
    "    groups = _case_id_groups(samples)\n",
    "    case_ids = sorted(groups.keys())\n",
    "    # 0 = par (authentic+forged), 1 = solo (apenas forged)\n",
    "    y_group = np.array([0 if len(groups[cid]) >= 2 else 1 for cid in case_ids], dtype=int)\n",
    "\n",
    "    try:\n",
    "        from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "        splitter = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)\n",
    "        for train_g, val_g in splitter.split(np.zeros(len(case_ids)), y_group):\n",
    "            train_idx: list[int] = []\n",
    "            val_idx: list[int] = []\n",
    "            for gi in train_g:\n",
    "                train_idx.extend(groups[case_ids[int(gi)]])\n",
    "            for gi in val_g:\n",
    "                val_idx.extend(groups[case_ids[int(gi)]])\n",
    "            yield np.array(sorted(train_idx), dtype=int), np.array(sorted(val_idx), dtype=int)\n",
    "        return\n",
    "    except Exception:\n",
    "        print(\"[FOLDS] sklearn.model_selection.StratifiedKFold indisponível; usando split fallback (não-estratificado).\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    indices = np.arange(len(case_ids))\n",
    "    folds: list[list[int]] = [[] for _ in range(n_splits)]\n",
    "    for label in np.unique(y_group):\n",
    "        label_indices = indices[y_group == label]\n",
    "        rng.shuffle(label_indices)\n",
    "        for i, idx in enumerate(label_indices):\n",
    "            folds[i % n_splits].append(int(idx))\n",
    "\n",
    "    for fold_idx in range(n_splits):\n",
    "        val_g = np.array(sorted(folds[fold_idx]), dtype=int)\n",
    "        val_g_set = set(val_g.tolist())\n",
    "        train_g = np.array([int(i) for i in indices if int(i) not in val_g_set], dtype=int)\n",
    "\n",
    "        train_idx: list[int] = []\n",
    "        val_idx: list[int] = []\n",
    "        for gi in train_g:\n",
    "            train_idx.extend(groups[case_ids[int(gi)]])\n",
    "        for gi in val_g:\n",
    "            val_idx.extend(groups[case_ids[int(gi)]])\n",
    "\n",
    "        yield np.array(sorted(train_idx), dtype=int), np.array(sorted(val_idx), dtype=int)\n",
    "\n",
    "\n",
    "N_FOLDS = int(cfg.get(\"folds\", 5)) if cfg else 5\n",
    "FOLD = 0\n",
    "\n",
    "folds = list(iter_case_id_folds(train_samples, n_splits=N_FOLDS, seed=SEED))\n",
    "train_idx, val_idx = folds[FOLD]\n",
    "\n",
    "train_fold_samples = [train_samples[int(i)] for i in train_idx]\n",
    "val_fold_samples = [train_samples[int(i)] for i in val_idx]\n",
    "\n",
    "print(f\"fold {FOLD}/{N_FOLDS}: train={len(train_fold_samples)} val={len(val_fold_samples)}\")\n",
    "print(\"val forged:\", sum(1 for s in val_fold_samples if s.is_authentic is False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68e741f",
   "metadata": {},
   "source": [
    "## Classificador (authentic vs forged) — opcional\n",
    "\n",
    "Usamos um classificador binário para:\n",
    "\n",
    "1) **Gating**: pular a segmentação em imagens previstas como autênticas (economiza tempo), e\n",
    "2) **Sinal adicional**: combinar a confiança do classificador com a segmentação.\n",
    "\n",
    "Decisão do baseline:\n",
    "\n",
    "- Backbone via **timm** (ex.: EfficientNet-B4 `tf_efficientnet_b4_ns`).\n",
    "- Saída binária com 1 neurônio (logits) + `BCEWithLogitsLoss`.\n",
    "- Split **sem vazamento por `case_id`**: reutilizamos os mesmos folds construídos acima.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5087429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6b — Config + dataset/augs + treino do classificador (opcional)\n",
    "import inspect\n",
    "\n",
    "try:\n",
    "    import albumentations as A\n",
    "except Exception:\n",
    "    print(\"[IMPORT ERROR] albumentations não importou (classificador); treino seguirá sem augmentations.\")\n",
    "    traceback.print_exc()\n",
    "    A = None\n",
    "\n",
    "try:\n",
    "    import timm\n",
    "except Exception:\n",
    "    print(\"[IMPORT ERROR] timm não importou; classificador ficará indisponível.\")\n",
    "    traceback.print_exc()\n",
    "    # Tenta fallback por código \"vendor\" via Kaggle Dataset (GitHub import).\n",
    "    add_local_package_to_syspath(\"timm\")\n",
    "    try:\n",
    "        import timm  # type: ignore[no-redef]\n",
    "    except Exception:\n",
    "        timm = None\n",
    "\n",
    "try:\n",
    "    from sklearn.metrics import f1_score as _sk_f1_score\n",
    "    from sklearn.metrics import roc_auc_score as _sk_roc_auc_score\n",
    "except Exception:\n",
    "    _sk_f1_score = None\n",
    "    _sk_roc_auc_score = None\n",
    "    print(\"[IMPORT ERROR] sklearn.metrics não importou; AUC/F1 do classificador não serão calculados.\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "\n",
    "def _has_param(transform_cls, name: str) -> bool:\n",
    "    return name in inspect.signature(transform_cls).parameters\n",
    "\n",
    "\n",
    "def _random_resized_crop(size: int, scale=(0.7, 1.0), ratio=(0.85, 1.15), p: float = 1.0):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for classifier augmentations\")\n",
    "    if _has_param(A.RandomResizedCrop, \"size\"):\n",
    "        return A.RandomResizedCrop(size=(int(size), int(size)), scale=scale, ratio=ratio, p=p)\n",
    "    return A.RandomResizedCrop(height=int(size), width=int(size), scale=scale, ratio=ratio, p=p)\n",
    "\n",
    "\n",
    "def _gauss_noise(p: float = 1.0):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for classifier augmentations\")\n",
    "    if _has_param(A.GaussNoise, \"std_range\"):\n",
    "        return A.GaussNoise(std_range=(0.005, 0.02), p=p)\n",
    "    return A.GaussNoise(var_limit=(5.0, 50.0), p=p)\n",
    "\n",
    "\n",
    "def _image_compression(quality_range=(60, 100), p: float = 0.10):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for classifier augmentations\")\n",
    "    if _has_param(A.ImageCompression, \"quality_range\"):\n",
    "        return A.ImageCompression(quality_range=quality_range, p=p)\n",
    "    return A.ImageCompression(quality_lower=int(quality_range[0]), quality_upper=int(quality_range[1]), p=p)\n",
    "\n",
    "\n",
    "def build_cls_train_augment(image_size: int):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for classifier augmentations\")\n",
    "    # Por padrão usamos augs \"básicas\" (flips/rotações leves) e evitamos\n",
    "    # copy-move sintético no classificador para não viciar em artefatos.\n",
    "    if CLS_AUG_MODE == \"basic\":\n",
    "        return A.Compose(\n",
    "            [\n",
    "                A.LongestMaxSize(max_size=int(image_size), p=1.0),\n",
    "                A.PadIfNeeded(\n",
    "                    min_height=int(image_size),\n",
    "                    min_width=int(image_size),\n",
    "                    border_mode=0,\n",
    "                    fill=0,\n",
    "                    fill_mask=0,\n",
    "                    p=1.0,\n",
    "                ),\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "                A.RandomRotate90(p=0.25),\n",
    "            ]\n",
    "        )\n",
    "    return A.Compose(\n",
    "        [\n",
    "            # Augs um pouco mais fortes (ainda SEM falsificação sintética).\n",
    "            A.LongestMaxSize(max_size=int(image_size), p=1.0),\n",
    "            A.PadIfNeeded(\n",
    "                min_height=int(image_size),\n",
    "                min_width=int(image_size),\n",
    "                border_mode=0,\n",
    "                fill=0,\n",
    "                fill_mask=0,\n",
    "                p=1.0,\n",
    "            ),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.5),\n",
    "            A.RandomRotate90(p=0.25),\n",
    "            A.Affine(scale=(0.9, 1.1), translate_percent=(-0.05, 0.05), rotate=(-15, 15), p=0.5),\n",
    "            A.OneOf(\n",
    "                [\n",
    "                    A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=1.0),\n",
    "                    A.RandomGamma(gamma_limit=(80, 120), p=1.0),\n",
    "                    A.CLAHE(clip_limit=(1.0, 3.0), p=1.0),\n",
    "                ],\n",
    "                p=0.35,\n",
    "            ),\n",
    "            A.OneOf([_gauss_noise(p=1.0), A.GaussianBlur(blur_limit=(3, 5), p=1.0)], p=0.15),\n",
    "            _image_compression(quality_range=(60, 100), p=0.10),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def build_cls_val_augment(image_size: int):\n",
    "    if A is None:\n",
    "        raise ImportError(\"albumentations is required for classifier augmentations\")\n",
    "    return A.Compose(\n",
    "        [\n",
    "            A.LongestMaxSize(max_size=int(image_size), p=1.0),\n",
    "            A.PadIfNeeded(\n",
    "                min_height=int(image_size),\n",
    "                min_width=int(image_size),\n",
    "                border_mode=0,\n",
    "                fill=0,\n",
    "                fill_mask=0,\n",
    "                p=1.0,\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "CLS_MODEL_NAME = \"tf_efficientnet_b4_ns\"\n",
    "CLS_IMAGE_SIZE = 380\n",
    "CLS_BATCH_SIZE = 16\n",
    "CLS_EPOCHS = 15\n",
    "CLS_LR = 2e-4\n",
    "CLS_WEIGHT_DECAY = 1e-4\n",
    "CLS_USE_AMP = DEVICE.startswith(\"cuda\")\n",
    "CLS_NUM_WORKERS = int(cfg.get(\"num_workers\", 2)) if cfg else 2\n",
    "CLS_AUG_MODE = \"basic\"  # \"basic\" (recomendado) ou \"strong\"\n",
    "# Se `prob_forged` ficar abaixo deste valor, marcamos como `authentic` e pulamos a segmentação.\n",
    "# Deixe baixo para minimizar falsos negativos (forged -> authentic).\n",
    "CLS_SKIP_THRESHOLD = 0.10\n",
    "CLS_EARLY_STOPPING = True\n",
    "CLS_PATIENCE = 3\n",
    "CLS_MIN_DELTA = 1e-4\n",
    "\n",
    "CLS_SAVE_DIR = OUTPUT_ROOT / \"models_cls\"\n",
    "CLS_LOAD_DIR = _find_checkpoint_root(CLS_SAVE_DIR, expected_subdir=\"models_cls\")\n",
    "\n",
    "print(\"classifier checkpoints (load):\", CLS_LOAD_DIR)\n",
    "print(\"classifier checkpoints (save):\", CLS_SAVE_DIR)\n",
    "\n",
    "RUN_CLS_TRAIN = False  # mude para True para treinar o classificador aqui\n",
    "TRAIN_CLS_FOLDS = [FOLD]  # para CV, use: list(range(N_FOLDS))\n",
    "\n",
    "\n",
    "def build_cls_model(model_name: str, pretrained: bool = True) -> torch.nn.Module:\n",
    "    if timm is None:\n",
    "        raise ImportError(\"timm is required for classifier model\")\n",
    "    try:\n",
    "        m = timm.create_model(model_name, pretrained=bool(pretrained), num_classes=1, in_chans=3)\n",
    "    except Exception as exc:\n",
    "        if pretrained:\n",
    "            print(\"[CLS] pretrained weights falharam; seguindo sem pesos. Erro abaixo:\")\n",
    "            traceback.print_exc()\n",
    "            m = timm.create_model(model_name, pretrained=False, num_classes=1, in_chans=3)\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "    reset = getattr(m, \"reset_classifier\", None)\n",
    "    if callable(reset):\n",
    "        try:\n",
    "            reset(num_classes=1)\n",
    "        except TypeError:\n",
    "            reset(1)\n",
    "    return m\n",
    "\n",
    "\n",
    "class ClsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, samples, augment=None) -> None:\n",
    "        self.samples = list(samples)\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        s = self.samples[idx]\n",
    "        img = load_image(s.image_path)\n",
    "        if self.augment is not None:\n",
    "            img = self.augment(image=img)[\"image\"]\n",
    "        img = normalize_image(img)\n",
    "        x = torch.from_numpy(img).permute(2, 0, 1)\n",
    "        y = torch.tensor([0.0 if s.is_authentic else 1.0], dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "\n",
    "def train_one_epoch_cls(model, loader, criterion, optimizer) -> dict:\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=CLS_USE_AMP)\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE).view(-1)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        with torch.cuda.amp.autocast(enabled=CLS_USE_AMP):\n",
    "            logits = model(x).view(-1)\n",
    "            loss = criterion(logits, y)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            prob = torch.sigmoid(logits)\n",
    "            pred = (prob >= 0.5).float()\n",
    "            correct += int((pred == y).sum().item())\n",
    "            total += int(y.numel())\n",
    "            total_loss += float(loss.item()) * int(y.numel())\n",
    "\n",
    "    return {\"loss\": total_loss / float(max(total, 1)), \"acc\": float(correct) / float(max(total, 1))}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_cls(model, loader, criterion) -> dict:\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    probs_all = []\n",
    "    y_all = []\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(DEVICE)\n",
    "        y = y.to(DEVICE).view(-1)\n",
    "        logits = model(x).view(-1)\n",
    "        loss = criterion(logits, y)\n",
    "        prob = torch.sigmoid(logits)\n",
    "        pred = (prob >= 0.5).float()\n",
    "\n",
    "        correct += int((pred == y).sum().item())\n",
    "        total += int(y.numel())\n",
    "        total_loss += float(loss.item()) * int(y.numel())\n",
    "\n",
    "        probs_all.append(prob.detach().cpu().numpy())\n",
    "        y_all.append(y.detach().cpu().numpy())\n",
    "\n",
    "    out = {\"loss\": total_loss / float(max(total, 1)), \"acc\": float(correct) / float(max(total, 1))}\n",
    "    probs_np = np.concatenate(probs_all) if probs_all else np.array([], dtype=np.float32)\n",
    "    y_np = np.concatenate(y_all) if y_all else np.array([], dtype=np.float32)\n",
    "    if probs_np.size:\n",
    "        thr = float(CLS_SKIP_THRESHOLD)\n",
    "        y_pred = (probs_np >= thr).astype(np.uint8)\n",
    "        y_true = y_np.astype(np.uint8)\n",
    "\n",
    "        tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "        fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "        fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "        tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "\n",
    "        out[\"thr\"] = thr\n",
    "        out[\"tp\"] = tp\n",
    "        out[\"fn\"] = fn\n",
    "        out[\"fp\"] = fp\n",
    "        out[\"tn\"] = tn\n",
    "        out[\"recall_forged\"] = float(tp) / float(max(tp + fn, 1))\n",
    "        out[\"fpr_auth\"] = float(fp) / float(max(fp + tn, 1))\n",
    "        out[\"skip_rate_total\"] = float(tn + fn) / float(max(tn + fp + tp + fn, 1))\n",
    "        out[\"skip_rate_auth\"] = float(tn) / float(max(tn + fp, 1))\n",
    "        out[\"skip_rate_forged\"] = float(fn) / float(max(fn + tp, 1))\n",
    "\n",
    "    if _sk_roc_auc_score is not None and probs_np.size and np.unique(y_np).size > 1:\n",
    "        out[\"auc\"] = float(_sk_roc_auc_score(y_np, probs_np))\n",
    "    if _sk_f1_score is not None and probs_np.size:\n",
    "        out[\"f1\"] = float(_sk_f1_score(y_np, probs_np >= 0.5))\n",
    "\n",
    "    return out\n",
    "\n",
    "\n",
    "if RUN_CLS_TRAIN:\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "    for fold_id in TRAIN_CLS_FOLDS:\n",
    "        train_idx, val_idx = folds[int(fold_id)]\n",
    "        cls_train_samples = [train_samples[int(i)] for i in train_idx]\n",
    "        cls_val_samples = [train_samples[int(i)] for i in val_idx]\n",
    "\n",
    "        try:\n",
    "            cls_train_aug = build_cls_train_augment(CLS_IMAGE_SIZE)\n",
    "            cls_val_aug = build_cls_val_augment(CLS_IMAGE_SIZE)\n",
    "        except Exception as exc:\n",
    "            print(\"[CLS] erro ao construir augmentations do classificador; treinando sem augs. Erro abaixo:\")\n",
    "            traceback.print_exc()\n",
    "            cls_train_aug = None\n",
    "            cls_val_aug = None\n",
    "\n",
    "        ds_tr = ClsDataset(cls_train_samples, augment=cls_train_aug)\n",
    "        ds_va = ClsDataset(cls_val_samples, augment=cls_val_aug)\n",
    "\n",
    "        loader_tr = DataLoader(\n",
    "            ds_tr,\n",
    "            batch_size=CLS_BATCH_SIZE,\n",
    "            shuffle=True,\n",
    "            num_workers=CLS_NUM_WORKERS,\n",
    "            pin_memory=DEVICE.startswith(\"cuda\"),\n",
    "        )\n",
    "        loader_va = DataLoader(\n",
    "            ds_va,\n",
    "            batch_size=CLS_BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            num_workers=CLS_NUM_WORKERS,\n",
    "            pin_memory=DEVICE.startswith(\"cuda\"),\n",
    "        )\n",
    "\n",
    "        model_cls = build_cls_model(CLS_MODEL_NAME, pretrained=True).to(DEVICE)\n",
    "\n",
    "        pos = sum(1 for s in cls_train_samples if s.is_authentic is False)\n",
    "        neg = sum(1 for s in cls_train_samples if s.is_authentic)\n",
    "        pos_weight = float(neg) / float(max(pos, 1))\n",
    "\n",
    "        criterion = torch.nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight], device=DEVICE))\n",
    "        optimizer = torch.optim.AdamW(model_cls.parameters(), lr=CLS_LR, weight_decay=CLS_WEIGHT_DECAY)\n",
    "\n",
    "        out_dir = CLS_SAVE_DIR / f\"fold_{int(fold_id)}\"\n",
    "        out_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cls_ckpt = out_dir / \"best.pt\"\n",
    "\n",
    "        best_score = -1e9\n",
    "        bad_epochs = 0\n",
    "        for epoch in range(1, CLS_EPOCHS + 1):\n",
    "            tr = train_one_epoch_cls(model_cls, loader_tr, criterion, optimizer)\n",
    "            va = validate_cls(model_cls, loader_va, criterion)\n",
    "            extra = []\n",
    "            if \"auc\" in va:\n",
    "                extra.append(f\"val_auc={va['auc']:.4f}\")\n",
    "            if \"f1\" in va:\n",
    "                extra.append(f\"val_f1={va['f1']:.4f}\")\n",
    "            if \"recall_forged\" in va:\n",
    "                extra.append(f\"val_recall_forged@{va.get('thr', CLS_SKIP_THRESHOLD):.2f}={va['recall_forged']:.4f}\")\n",
    "                extra.append(f\"FN={int(va.get('fn', 0))}\")\n",
    "            print(\n",
    "                f\"[CLS] fold {int(fold_id)} epoch {epoch:02d}/{CLS_EPOCHS} \"\n",
    "                f\"tr_loss={tr['loss']:.4f} tr_acc={tr['acc']:.4f} \"\n",
    "                f\"va_loss={va['loss']:.4f} va_acc={va['acc']:.4f} {' '.join(extra)}\".strip()\n",
    "            )\n",
    "\n",
    "            score = float(va[\"auc\"]) if \"auc\" in va else (-float(va[\"loss\"]))\n",
    "            if score > best_score + CLS_MIN_DELTA:\n",
    "                best_score = score\n",
    "                bad_epochs = 0\n",
    "                torch.save(\n",
    "                    {\n",
    "                        \"model_state\": model_cls.state_dict(),\n",
    "                        \"epoch\": epoch,\n",
    "                        \"val_auc\": va.get(\"auc\"),\n",
    "                        \"val_loss\": va[\"loss\"],\n",
    "                        \"config\": {\n",
    "                            \"model_name\": CLS_MODEL_NAME,\n",
    "                            \"image_size\": CLS_IMAGE_SIZE,\n",
    "                            \"aug_mode\": CLS_AUG_MODE,\n",
    "                        },\n",
    "                    },\n",
    "                    cls_ckpt,\n",
    "                )\n",
    "                print(\"[CLS] saved:\", cls_ckpt)\n",
    "            else:\n",
    "                bad_epochs += 1\n",
    "\n",
    "            if CLS_EARLY_STOPPING and bad_epochs >= CLS_PATIENCE:\n",
    "                print(f\"[CLS] early stopping: sem melhora por {CLS_PATIENCE} épocas\")\n",
    "                break\n",
    "\n",
    "        del model_cls, optimizer\n",
    "        if DEVICE.startswith(\"cuda\"):\n",
    "            torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c8e2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6c — (Opcional) Tuning do threshold para favorecer recall (evitar falsos negativos)\n",
    "RUN_CLS_THRESHOLD_TUNING = False\n",
    "CLS_TARGET_RECALL = 0.995\n",
    "CLS_SKIP_THRESHOLD_GRID = np.linspace(0.01, 0.80, 80)\n",
    "\n",
    "\n",
    "def _confusion_from_probs(y_true: np.ndarray, probs: np.ndarray, thr: float) -> dict:\n",
    "    y_true = y_true.astype(np.uint8)\n",
    "    y_pred = (probs >= float(thr)).astype(np.uint8)\n",
    "\n",
    "    tp = int(((y_true == 1) & (y_pred == 1)).sum())\n",
    "    fn = int(((y_true == 1) & (y_pred == 0)).sum())\n",
    "    fp = int(((y_true == 0) & (y_pred == 1)).sum())\n",
    "    tn = int(((y_true == 0) & (y_pred == 0)).sum())\n",
    "\n",
    "    recall = float(tp) / float(max(tp + fn, 1))\n",
    "    fpr = float(fp) / float(max(fp + tn, 1))\n",
    "    return {\"thr\": float(thr), \"tp\": tp, \"fn\": fn, \"fp\": fp, \"tn\": tn, \"recall\": recall, \"fpr\": fpr}\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def _collect_probs_cls(model, samples, image_size: int):\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    probs: list[float] = []\n",
    "    y_true: list[int] = []\n",
    "    keys: list[str] = []\n",
    "\n",
    "    try:\n",
    "        aug = build_cls_val_augment(image_size) if A is not None else None\n",
    "    except Exception:\n",
    "        print(\"[CLS] erro ao construir augmentation de validação; seguindo sem augs. Erro abaixo:\")\n",
    "        traceback.print_exc()\n",
    "        aug = None\n",
    "\n",
    "    for s in samples:\n",
    "        img = load_image(s.image_path)\n",
    "        if aug is not None:\n",
    "            img = aug(image=img)[\"image\"]\n",
    "        img = normalize_image(img)\n",
    "        x = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n",
    "        if image_size and x.shape[-2:] != (image_size, image_size):\n",
    "            x = F.interpolate(x, size=(image_size, image_size), mode=\"bilinear\", align_corners=False)\n",
    "        logit = model(x).view(-1)\n",
    "        prob = float(torch.sigmoid(logit)[0].item())\n",
    "        probs.append(prob)\n",
    "        y_true.append(0 if s.is_authentic else 1)\n",
    "        keys.append(str(s.rel_path))\n",
    "\n",
    "    return np.asarray(y_true, dtype=np.uint8), np.asarray(probs, dtype=np.float32), keys\n",
    "\n",
    "\n",
    "if RUN_CLS_THRESHOLD_TUNING:\n",
    "    # Usa o fold atual por padrão (melhor: OOF com todos folds, se você treinou todos).\n",
    "    fold_id = int(FOLD)\n",
    "    cls_ckpt_path = (CLS_LOAD_DIR / f\"fold_{fold_id}\") / \"best.pt\"\n",
    "    if not cls_ckpt_path.exists():\n",
    "        print(\"[CLS] checkpoint não encontrado:\", cls_ckpt_path)\n",
    "    else:\n",
    "        state, cfg_cls = _load_checkpoint(cls_ckpt_path)\n",
    "        model_name = cfg_cls.get(\"model_name\", CLS_MODEL_NAME)\n",
    "        image_size = int(cfg_cls.get(\"image_size\", CLS_IMAGE_SIZE))\n",
    "        model_cls = build_cls_model(model_name, pretrained=False).to(DEVICE)\n",
    "        model_cls.load_state_dict(state)\n",
    "        model_cls.eval()\n",
    "\n",
    "        y_true, probs, keys = _collect_probs_cls(model_cls, val_fold_samples, image_size=image_size)\n",
    "\n",
    "        rows = [_confusion_from_probs(y_true, probs, thr) for thr in CLS_SKIP_THRESHOLD_GRID]\n",
    "        feasible = [r for r in rows if r[\"recall\"] >= float(CLS_TARGET_RECALL)]\n",
    "        if feasible:\n",
    "            best = max(feasible, key=lambda r: r[\"thr\"])  # maior thr mantendo recall => menos falsos positivos\n",
    "        else:\n",
    "            best = max(rows, key=lambda r: (r[\"recall\"], r[\"thr\"]))\n",
    "\n",
    "        CLS_SKIP_THRESHOLD = float(best[\"thr\"])\n",
    "        print(\"[CLS] selected CLS_SKIP_THRESHOLD:\", CLS_SKIP_THRESHOLD)\n",
    "        print(\"[CLS] stats:\", best)\n",
    "\n",
    "        fn_keys = [k for k, yt, p in zip(keys, y_true.tolist(), probs.tolist()) if yt == 1 and p < CLS_SKIP_THRESHOLD]\n",
    "        print(\"[CLS] forged false negatives at threshold:\", len(fn_keys))\n",
    "        if fn_keys:\n",
    "            print(\"[CLS] FN examples (rel_path):\", fn_keys[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceacf04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7 — Config de treino (patch-based)\n",
    "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
    "\n",
    "PATCH_SIZE = int(cfg.get(\"patch_size\", 512)) if cfg else 512\n",
    "BATCH_SIZE = int(cfg.get(\"batch_size\", 8)) if cfg else 8\n",
    "NUM_WORKERS = int(cfg.get(\"num_workers\", 2)) if cfg else 2\n",
    "\n",
    "POSITIVE_PROB = float(cfg.get(\"positive_prob\", 0.7)) if cfg else 0.7\n",
    "MIN_POS_PIXELS = int(cfg.get(\"min_pos_pixels\", 32)) if cfg else 32\n",
    "MAX_TRIES = int(cfg.get(\"max_tries\", 10)) if cfg else 10\n",
    "POS_SAMPLE_WEIGHT = float(cfg.get(\"pos_sample_weight\", 2.0)) if cfg else 2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3a798c",
   "metadata": {},
   "source": [
    "## Data Augmentation (Aumento de Dados)\n",
    "\n",
    "Este baseline usa **Albumentations** para aplicar aumentos **coerentes** entre `image` e `mask` (para geometria),\n",
    "e aumentos **apenas na imagem** (para ruído/cor/blur).\n",
    "\n",
    "Geometria (image + mask):\n",
    "\n",
    "- **Flips** horizontal/vertical\n",
    "- **Rotação 90°** aleatória e **pequenas rotações** (Affine)\n",
    "- **Escala/zoom e translação** (Affine + RandomResizedCrop)\n",
    "\n",
    "Robustez fotométrica (apenas image):\n",
    "\n",
    "- **Brilho/contraste**, **gamma** e **CLAHE** (leve)\n",
    "- **Ruído gaussiano** e **blur** (gauss/motion)\n",
    "- **Compressão** (artefatos tipo JPEG) e **cutout**\n",
    "\n",
    "Copy-move sintético (image + mask):\n",
    "\n",
    "- Para patches com máscara vazia (amostra autêntica), aplicamos um **copy-move on-the-fly**:\n",
    "  copiamos uma região e colamos em outra posição no mesmo patch, marcando **origem e destino** na máscara.\n",
    "  Opcionalmente aplicamos pequena rotação/escala no patch colado.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48abf512",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Célula 7b — Config do augmentation (inclui copy-move sintético)\n",
    "COPY_MOVE_PROB = float(cfg.get(\"copy_move_prob\", 0.25)) if cfg else 0.25\n",
    "COPY_MOVE_MIN_AREA_FRAC = float(cfg.get(\"copy_move_min_area_frac\", 0.05)) if cfg else 0.05\n",
    "COPY_MOVE_MAX_AREA_FRAC = float(cfg.get(\"copy_move_max_area_frac\", 0.20)) if cfg else 0.20\n",
    "COPY_MOVE_ROTATION_LIMIT = float(cfg.get(\"copy_move_rotation_limit\", 15.0)) if cfg else 15.0\n",
    "\n",
    "scale_range = cfg.get(\"copy_move_scale_range\", [0.9, 1.1]) if cfg else [0.9, 1.1]\n",
    "if isinstance(scale_range, (list, tuple)) and len(scale_range) == 2:\n",
    "    COPY_MOVE_SCALE_RANGE = (float(scale_range[0]), float(scale_range[1]))\n",
    "else:\n",
    "    COPY_MOVE_SCALE_RANGE = (0.9, 1.1)\n",
    "\n",
    "try:\n",
    "    train_aug = get_train_augment(\n",
    "        patch_size=PATCH_SIZE,\n",
    "        copy_move_prob=COPY_MOVE_PROB,\n",
    "        copy_move_min_area_frac=COPY_MOVE_MIN_AREA_FRAC,\n",
    "        copy_move_max_area_frac=COPY_MOVE_MAX_AREA_FRAC,\n",
    "        copy_move_rotation_limit=COPY_MOVE_ROTATION_LIMIT,\n",
    "        copy_move_scale_range=COPY_MOVE_SCALE_RANGE,\n",
    "    )\n",
    "    val_aug = get_val_augment()\n",
    "except ImportError as exc:\n",
    "    print(\"[SEG] albumentations indisponível; desativando augmentations de treino. Erro abaixo:\")\n",
    "    traceback.print_exc()\n",
    "    train_aug = None\n",
    "    val_aug = None\n",
    "\n",
    "def make_loaders(train_samples_fold, val_samples_fold, *, train_aug, val_aug, batch_size: int, patch_size: int):\n",
    "    train_ds = PatchDataset(\n",
    "        train_samples_fold,\n",
    "        patch_size=patch_size,\n",
    "        train=True,\n",
    "        augment=train_aug,\n",
    "        positive_prob=POSITIVE_PROB,\n",
    "        min_pos_pixels=MIN_POS_PIXELS,\n",
    "        max_tries=MAX_TRIES,\n",
    "        seed=SEED,\n",
    "    )\n",
    "    val_ds = PatchDataset(\n",
    "        val_samples_fold,\n",
    "        patch_size=patch_size,\n",
    "        train=False,\n",
    "        augment=val_aug,\n",
    "        seed=SEED,\n",
    "    )\n",
    "\n",
    "    weights = [POS_SAMPLE_WEIGHT if s.is_authentic is False else 1.0 for s in train_samples_fold]\n",
    "    sampler = WeightedRandomSampler(weights, num_samples=len(train_samples_fold), replacement=True)\n",
    "\n",
    "    train_loader_fold = DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        sampler=sampler,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=DEVICE.startswith(\"cuda\"),\n",
    "    )\n",
    "    val_loader_fold = DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=NUM_WORKERS,\n",
    "        pin_memory=DEVICE.startswith(\"cuda\"),\n",
    "    )\n",
    "    return train_loader_fold, val_loader_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d180d5f",
   "metadata": {},
   "source": [
    "### Implementação via `segmentation_models_pytorch` (SMP)\n",
    "\n",
    "Neste pipeline usamos o **SMP** para instanciar arquiteturas SOTA rapidamente (U-Net++, DeepLabV3+, SegFormer).\n",
    "\n",
    "**Importante (logits vs sigmoid):** aqui mantemos `activation=None` nos modelos do SMP, e tratamos a saída como\n",
    "**logits**:\n",
    "\n",
    "- Treino: usamos losses com `BCEWithLogitsLoss` (`BCEDiceLoss` / `BCETverskyLoss`), que esperam logits.\n",
    "- Inferência: `predict_image()` aplica `torch.sigmoid(logits)` para gerar probabilidades.\n",
    "\n",
    "Se você configurar `activation=\"sigmoid\"` no SMP, precisa ajustar a loss (sem logits) e remover o `sigmoid` da\n",
    "inferência para não aplicar duas vezes.\n",
    "\n",
    "Sobre encoders:\n",
    "\n",
    "- `efficientnet-b7`, `se_resnet101` e `mit_b*` estão disponíveis no SMP.\n",
    "- `timm-resnest101e` não está disponível como encoder no SMP deste ambiente; por isso usamos **ResNet101+SE**\n",
    "  (`se_resnet101`) como substituto no DeepLabV3+.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c2a91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 8 — Modelos de segmentação (ensemble)\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "def build_fallback_deeplab(pretrained: bool = True) -> nn.Module:\n",
    "    from torchvision.models.segmentation import DeepLabV3_ResNet50_Weights, deeplabv3_resnet50\n",
    "\n",
    "    weights = DeepLabV3_ResNet50_Weights.DEFAULT if pretrained else None\n",
    "    base = deeplabv3_resnet50(weights=weights)\n",
    "    in_ch = int(base.classifier[-1].in_channels)\n",
    "    base.classifier[-1] = nn.Conv2d(in_ch, 1, kernel_size=1)\n",
    "    base.aux_classifier = None\n",
    "\n",
    "    class Wrapper(nn.Module):\n",
    "        def __init__(self, model: nn.Module) -> None:\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "            out = self.model(x)\n",
    "            if isinstance(out, dict):\n",
    "                return out[\"out\"]\n",
    "            return out\n",
    "\n",
    "    return Wrapper(base)\n",
    "\n",
    "\n",
    "try:\n",
    "    import segmentation_models_pytorch as smp\n",
    "except Exception:\n",
    "    print(\"[IMPORT ERROR] segmentation_models_pytorch não importou; não é possível montar os modelos do ensemble.\")\n",
    "    traceback.print_exc()\n",
    "    # Fallback: tenta importar via código \"vendor\" em um Kaggle Dataset (ex.: repo importado do GitHub).\n",
    "    add_local_package_to_syspath(\"segmentation_models_pytorch\")\n",
    "    add_local_package_to_syspath(\"timm\")  # smp pode depender de timm em alguns encoders\n",
    "    try:\n",
    "        import segmentation_models_pytorch as smp  # type: ignore[no-redef]\n",
    "        print(\"[IMPORT] segmentation_models_pytorch importado via fallback local.\")\n",
    "    except Exception:\n",
    "        print(\"[IMPORT ERROR] falha também no fallback local; não há como seguir sem SMP.\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "SEG_MODEL_SPECS = [\n",
    "    # Encoder-decoder CNN (detalhes finos) + dropout no decoder.\n",
    "    {\n",
    "        \"id\": \"unetpp_effb7\",\n",
    "        \"arch\": \"unetpp\",\n",
    "        \"encoder_name\": \"efficientnet-b7\",\n",
    "        \"encoder_weights\": \"imagenet\",\n",
    "        \"decoder_attention_type\": \"scse\",\n",
    "        \"decoder_dropout\": 0.20,\n",
    "        \"batch_size\": 1,\n",
    "    },\n",
    "    # DeepLabV3+ (ASPP / contexto multi-escala) para complementar o U-Net++.\n",
    "    # ResNeSt não está disponível como encoder neste setup; usamos ResNet101+SE como alternativa.\n",
    "    {\n",
    "        \"id\": \"deeplabv3p_se_r101\",\n",
    "        \"arch\": \"deeplabv3p\",\n",
    "        \"encoder_name\": \"se_resnet101\",\n",
    "        \"encoder_weights\": \"imagenet\",\n",
    "        \"encoder_output_stride\": 16,\n",
    "        \"decoder_channels\": 256,\n",
    "        \"decoder_atrous_rates\": [12, 24, 36],\n",
    "        \"decoder_aspp_separable\": False,\n",
    "        \"decoder_aspp_dropout\": 0.10,\n",
    "        \"decoder_dropout\": 0.10,\n",
    "        \"batch_size\": 1,\n",
    "    },\n",
    "    # Transformer-style (autoatenção) para complementar as CNNs.\n",
    "    # Nota: B5 é bem mais custoso; B3 costuma ser um bom trade-off.\n",
    "    {\n",
    "        \"id\": \"segformer_mitb3\",\n",
    "        \"arch\": \"segformer\",\n",
    "        \"encoder_name\": \"mit_b3\",\n",
    "        \"encoder_weights\": \"imagenet\",\n",
    "        \"decoder_segmentation_channels\": 256,\n",
    "        \"decoder_dropout\": 0.10,\n",
    "        \"batch_size\": 1,\n",
    "    },\n",
    "]\n",
    "\n",
    "SEG_SAVE_DIR = OUTPUT_ROOT / \"models_seg\"\n",
    "SEG_LOAD_DIR = _find_checkpoint_root(SEG_SAVE_DIR, expected_subdir=\"models_seg\")\n",
    "\n",
    "print(\"segmentation checkpoints (load):\", SEG_LOAD_DIR)\n",
    "print(\"segmentation checkpoints (save):\", SEG_SAVE_DIR)\n",
    "\n",
    "\n",
    "def _wrap_segmentation_head_dropout(model: nn.Module, p: float) -> nn.Module:\n",
    "    p = float(p)\n",
    "    if p <= 0:\n",
    "        return model\n",
    "    if hasattr(model, \"segmentation_head\"):\n",
    "        model.segmentation_head = nn.Sequential(nn.Dropout2d(p=p), model.segmentation_head)\n",
    "    return model\n",
    "\n",
    "\n",
    "def _inject_unetpp_decoder_dropout(model: nn.Module, p: float) -> nn.Module:\n",
    "    p = float(p)\n",
    "    if p <= 0:\n",
    "        return model\n",
    "    decoder = getattr(model, \"decoder\", None)\n",
    "    blocks = getattr(decoder, \"blocks\", None)\n",
    "    if blocks is None or not hasattr(blocks, \"items\"):\n",
    "        return model\n",
    "\n",
    "    for _, block in blocks.items():\n",
    "        for attr in (\"conv1\", \"conv2\"):\n",
    "            layer = getattr(block, attr, None)\n",
    "            if layer is None:\n",
    "                continue\n",
    "            if isinstance(layer, nn.Sequential) and len(layer) > 0 and isinstance(layer[-1], nn.Dropout2d):\n",
    "                continue\n",
    "            setattr(block, attr, nn.Sequential(layer, nn.Dropout2d(p=p)))\n",
    "    return model\n",
    "\n",
    "\n",
    "def _safe_build_smp(builder, **kwargs):\n",
    "    if smp is None:\n",
    "        raise ImportError(\"segmentation_models_pytorch is required for SMP models\")\n",
    "    try:\n",
    "        return builder(**kwargs)\n",
    "    except Exception as exc:\n",
    "        if kwargs.get(\"encoder_weights\"):\n",
    "            print(\n",
    "                f\"[SEG] encoder_weights falharam ({kwargs.get('encoder_name')}); seguindo sem pesos. Erro abaixo:\",\n",
    "            )\n",
    "            traceback.print_exc()\n",
    "            kwargs[\"encoder_weights\"] = None\n",
    "            return builder(**kwargs)\n",
    "        raise\n",
    "\n",
    "\n",
    "def build_seg_model(spec: dict, pretrained: bool = True) -> nn.Module:\n",
    "    arch = str(spec.get(\"arch\"))\n",
    "    encoder_name = str(spec.get(\"encoder_name\"))\n",
    "    encoder_weights = spec.get(\"encoder_weights\", \"imagenet\") if pretrained else None\n",
    "    if encoder_weights == \"\":\n",
    "        encoder_weights = None\n",
    "\n",
    "    if arch == \"unetpp\":\n",
    "        model = _safe_build_smp(\n",
    "            smp.UnetPlusPlus,\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            decoder_attention_type=spec.get(\"decoder_attention_type\", \"scse\"),\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        )\n",
    "    elif arch == \"segformer\":\n",
    "        model = _safe_build_smp(\n",
    "            smp.Segformer,\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            decoder_segmentation_channels=int(spec.get(\"decoder_segmentation_channels\", 256)),\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        )\n",
    "    elif arch == \"deeplabv3p\":\n",
    "        model = _safe_build_smp(\n",
    "            smp.DeepLabV3Plus,\n",
    "            encoder_name=encoder_name,\n",
    "            encoder_weights=encoder_weights,\n",
    "            encoder_output_stride=int(spec.get(\"encoder_output_stride\", 16)),\n",
    "            decoder_channels=int(spec.get(\"decoder_channels\", 256)),\n",
    "            decoder_atrous_rates=tuple(spec.get(\"decoder_atrous_rates\", (12, 24, 36))),\n",
    "            decoder_aspp_separable=bool(spec.get(\"decoder_aspp_separable\", False)),\n",
    "            decoder_aspp_dropout=float(spec.get(\"decoder_aspp_dropout\", 0.0)),\n",
    "            in_channels=3,\n",
    "            classes=1,\n",
    "            activation=None,\n",
    "        )\n",
    "    elif arch == \"deeplabv3\":\n",
    "        model = build_fallback_deeplab(pretrained=bool(pretrained))\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown seg arch: {arch}\")\n",
    "\n",
    "    p = float(spec.get(\"decoder_dropout\", 0.0))\n",
    "    if arch == \"unetpp\":\n",
    "        model = _inject_unetpp_decoder_dropout(model, p)\n",
    "    else:\n",
    "        model = _wrap_segmentation_head_dropout(model, p)\n",
    "    return model\n",
    "\n",
    "\n",
    "print(\"segmentation model builders ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c8574a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Célula 9 — Loss, otimizador e loop de treino (opcional)\n",
    "\n",
    "LOSS_NAME = (cfg.get(\"loss\", \"bce_dice\") if cfg else \"bce_dice\").lower()\n",
    "LR = float(cfg.get(\"learning_rate\", 1e-4)) if cfg else 1e-4\n",
    "WEIGHT_DECAY = float(cfg.get(\"weight_decay\", 1e-4)) if cfg else 1e-4\n",
    "EPOCHS = int(cfg.get(\"epochs\", 30)) if cfg else 30\n",
    "USE_AMP = bool(cfg.get(\"use_amp\", True)) if cfg else True\n",
    "USE_AMP = USE_AMP and DEVICE.startswith(\"cuda\")\n",
    "\n",
    "RUN_TRAIN = False  # mude para True para treinar aqui\n",
    "TRAIN_FOLDS = [FOLD]  # para CV, use: list(range(N_FOLDS))\n",
    "TRAIN_SEG_MODEL_IDS = [s[\"id\"] for s in SEG_MODEL_SPECS]  # edite para treinar só um dos modelos\n",
    "\n",
    "SEG_LR_ENCODER = float(cfg.get(\"learning_rate_encoder\", LR)) if cfg else LR\n",
    "SEG_LR_DECODER = float(cfg.get(\"learning_rate_decoder\", 1e-3)) if cfg else 1e-3\n",
    "SEG_FREEZE_EPOCHS = int(cfg.get(\"freeze_encoder_epochs\", 3)) if cfg else 3\n",
    "\n",
    "SEG_SCHEDULER = str(cfg.get(\"seg_scheduler\", \"cosine\")) if cfg else \"cosine\"  # \"cosine\" | \"plateau\" | \"none\"\n",
    "SEG_SCHEDULER = SEG_SCHEDULER.lower().strip()\n",
    "SEG_PLATEAU_FACTOR = float(cfg.get(\"seg_plateau_factor\", 0.5)) if cfg else 0.5\n",
    "SEG_PLATEAU_PATIENCE = int(cfg.get(\"seg_plateau_patience\", 2)) if cfg else 2\n",
    "\n",
    "SEG_EARLY_STOPPING = bool(cfg.get(\"seg_early_stopping\", True)) if cfg else True\n",
    "SEG_PATIENCE = int(cfg.get(\"seg_patience\", 5)) if cfg else 5\n",
    "SEG_MIN_DELTA = float(cfg.get(\"seg_min_delta\", 1e-4)) if cfg else 1e-4\n",
    "\n",
    "# Treino em estágios (opcional): começar com patches menores e depois refinar no tamanho final.\n",
    "SEG_STAGED_TRAINING = False\n",
    "SEG_TRAIN_STAGES = [{\"patch_size\": PATCH_SIZE, \"epochs\": EPOCHS}]\n",
    "# Exemplo (mais rápido no começo, mais detalhado no fim):\n",
    "# SEG_STAGED_TRAINING = True\n",
    "# SEG_TRAIN_STAGES = [{\"patch_size\": 256, \"epochs\": 5}, {\"patch_size\": 512, \"epochs\": 25}]\n",
    "\n",
    "SEG_BCE_WEIGHT = float(cfg.get(\"bce_weight\", 1.0)) if cfg else 1.0\n",
    "SEG_DICE_WEIGHT = float(cfg.get(\"dice_weight\", 1.0)) if cfg else 1.0\n",
    "SEG_DICE_SMOOTH = float(cfg.get(\"dice_smooth\", 1.0)) if cfg else 1.0\n",
    "SEG_BCE_POS_WEIGHT = cfg.get(\"bce_pos_weight\", None) if cfg else None  # opcional (float)\n",
    "\n",
    "\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth: float = 1.0) -> None:\n",
    "        super().__init__()\n",
    "        self.smooth = float(smooth)\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        probs = torch.sigmoid(logits)\n",
    "        targets = targets.float()\n",
    "        probs = probs.view(probs.size(0), -1)\n",
    "        targets = targets.view(targets.size(0), -1)\n",
    "        intersection = (probs * targets).sum(dim=1)\n",
    "        denom = probs.sum(dim=1) + targets.sum(dim=1)\n",
    "        dice = (2.0 * intersection + self.smooth) / (denom + self.smooth)\n",
    "        return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "class WeightedBCEDiceLoss(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        bce_weight: float = 1.0,\n",
    "        dice_weight: float = 1.0,\n",
    "        dice_smooth: float = 1.0,\n",
    "        bce_pos_weight: float | None = None,\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.bce_weight = float(bce_weight)\n",
    "        self.dice_weight = float(dice_weight)\n",
    "        self.dice = DiceLoss(smooth=float(dice_smooth))\n",
    "        self.bce_pos_weight = float(bce_pos_weight) if bce_pos_weight is not None else None\n",
    "\n",
    "    def forward(self, logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        import torch.nn.functional as F\n",
    "\n",
    "        if self.bce_pos_weight is None:\n",
    "            bce = F.binary_cross_entropy_with_logits(logits, targets)\n",
    "        else:\n",
    "            pos_weight = torch.tensor([self.bce_pos_weight], device=logits.device, dtype=logits.dtype)\n",
    "            bce = F.binary_cross_entropy_with_logits(logits, targets, pos_weight=pos_weight)\n",
    "\n",
    "        return self.bce_weight * bce + self.dice_weight * self.dice(logits, targets)\n",
    "\n",
    "\n",
    "def _split_encoder_params(model: nn.Module) -> tuple[list[torch.nn.Parameter], list[torch.nn.Parameter]]:\n",
    "    encoder = getattr(model, \"encoder\", None)\n",
    "    if encoder is None:\n",
    "        return [], list(model.parameters())\n",
    "    enc_params = list(encoder.parameters())\n",
    "    enc_ids = {id(p) for p in enc_params}\n",
    "    other_params = [p for p in model.parameters() if id(p) not in enc_ids]\n",
    "    return enc_params, other_params\n",
    "\n",
    "\n",
    "def _set_trainable(params: list[torch.nn.Parameter], trainable: bool) -> None:\n",
    "    for p in params:\n",
    "        p.requires_grad = bool(trainable)\n",
    "\n",
    "\n",
    "def _build_optimizer(model: nn.Module) -> tuple[torch.optim.Optimizer, list[torch.nn.Parameter], list[torch.nn.Parameter]]:\n",
    "    enc_params, other_params = _split_encoder_params(model)\n",
    "    param_groups = []\n",
    "    if enc_params:\n",
    "        param_groups.append({\"params\": enc_params, \"lr\": SEG_LR_ENCODER})\n",
    "    if other_params:\n",
    "        param_groups.append({\"params\": other_params, \"lr\": SEG_LR_DECODER})\n",
    "    optimizer = torch.optim.AdamW(param_groups, weight_decay=WEIGHT_DECAY)\n",
    "    return optimizer, enc_params, other_params\n",
    "\n",
    "\n",
    "def _build_scheduler(optimizer: torch.optim.Optimizer, total_epochs: int):\n",
    "    if SEG_SCHEDULER == \"none\":\n",
    "        return None\n",
    "    if SEG_SCHEDULER == \"plateau\":\n",
    "        return torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode=\"min\",\n",
    "            factor=float(SEG_PLATEAU_FACTOR),\n",
    "            patience=int(SEG_PLATEAU_PATIENCE),\n",
    "            verbose=True,\n",
    "        )\n",
    "    if SEG_SCHEDULER == \"cosine\":\n",
    "        return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=int(max(total_epochs, 1)))\n",
    "    raise ValueError(f\"Unknown SEG_SCHEDULER: {SEG_SCHEDULER}\")\n",
    "\n",
    "\n",
    "def _build_seg_augs(patch_size: int):\n",
    "    try:\n",
    "        train_aug = get_train_augment(\n",
    "            patch_size=int(patch_size),\n",
    "            copy_move_prob=COPY_MOVE_PROB,\n",
    "            copy_move_min_area_frac=COPY_MOVE_MIN_AREA_FRAC,\n",
    "            copy_move_max_area_frac=COPY_MOVE_MAX_AREA_FRAC,\n",
    "            copy_move_rotation_limit=COPY_MOVE_ROTATION_LIMIT,\n",
    "            copy_move_scale_range=COPY_MOVE_SCALE_RANGE,\n",
    "        )\n",
    "        val_aug = get_val_augment()\n",
    "    except Exception as exc:\n",
    "        print(\"[SEG] erro ao construir augmentations; treinando sem augs. Erro abaixo:\")\n",
    "        traceback.print_exc()\n",
    "        train_aug = None\n",
    "        val_aug = None\n",
    "    return train_aug, val_aug\n",
    "\n",
    "\n",
    "if RUN_TRAIN:\n",
    "    for spec in SEG_MODEL_SPECS:\n",
    "        if spec[\"id\"] not in set(TRAIN_SEG_MODEL_IDS):\n",
    "            continue\n",
    "        for fold_id in TRAIN_FOLDS:\n",
    "            train_idx, val_idx = folds[int(fold_id)]\n",
    "            train_samples_fold = [train_samples[int(i)] for i in train_idx]\n",
    "            val_samples_fold = [train_samples[int(i)] for i in val_idx]\n",
    "\n",
    "            fold_dir = SEG_SAVE_DIR / str(spec[\"id\"]) / f\"fold_{int(fold_id)}\"\n",
    "            fold_dir.mkdir(parents=True, exist_ok=True)\n",
    "            ckpt_path = fold_dir / \"best.pt\"\n",
    "\n",
    "            model = build_seg_model(spec, pretrained=True).to(DEVICE)\n",
    "            if LOSS_NAME == \"bce_tversky\":\n",
    "                criterion = BCETverskyLoss(\n",
    "                    alpha=float(cfg.get(\"tversky_alpha\", 0.7)) if cfg else 0.7,\n",
    "                    beta=float(cfg.get(\"tversky_beta\", 0.3)) if cfg else 0.3,\n",
    "                    tversky_weight=float(cfg.get(\"tversky_weight\", 1.0)) if cfg else 1.0,\n",
    "                )\n",
    "            else:\n",
    "                pos_weight = float(SEG_BCE_POS_WEIGHT) if isinstance(SEG_BCE_POS_WEIGHT, (int, float)) else None\n",
    "                criterion = WeightedBCEDiceLoss(\n",
    "                    bce_weight=SEG_BCE_WEIGHT,\n",
    "                    dice_weight=SEG_DICE_WEIGHT,\n",
    "                    dice_smooth=SEG_DICE_SMOOTH,\n",
    "                    bce_pos_weight=pos_weight,\n",
    "                )\n",
    "\n",
    "            optimizer, enc_params, _ = _build_optimizer(model)\n",
    "\n",
    "            # Fase 1: (opcional) congelar encoder por algumas épocas.\n",
    "            if enc_params and SEG_FREEZE_EPOCHS > 0:\n",
    "                _set_trainable(enc_params, False)\n",
    "                print(f\"[SEG:{spec['id']}] freeze encoder for {SEG_FREEZE_EPOCHS} epoch(s)\")\n",
    "\n",
    "            best_dice = -1.0\n",
    "            best_loss = float(\"inf\")\n",
    "            bad_epochs = 0\n",
    "            global_epoch = 0\n",
    "\n",
    "            stages = SEG_TRAIN_STAGES if SEG_STAGED_TRAINING else [{\"patch_size\": PATCH_SIZE, \"epochs\": EPOCHS}]\n",
    "            total_epochs = int(sum(int(s[\"epochs\"]) for s in stages))\n",
    "            scheduler = _build_scheduler(optimizer, total_epochs=total_epochs)\n",
    "\n",
    "            for stage in stages:\n",
    "                patch_size = int(stage[\"patch_size\"])\n",
    "                stage_epochs = int(stage[\"epochs\"])\n",
    "                stage_train_aug, stage_val_aug = _build_seg_augs(patch_size)\n",
    "\n",
    "                batch_size = int(spec.get(\"batch_size\", BATCH_SIZE))\n",
    "                train_loader_fold, val_loader_fold = make_loaders(\n",
    "                    train_samples_fold,\n",
    "                    val_samples_fold,\n",
    "                    train_aug=stage_train_aug,\n",
    "                    val_aug=stage_val_aug,\n",
    "                    batch_size=batch_size,\n",
    "                    patch_size=patch_size,\n",
    "                )\n",
    "\n",
    "                for _ in range(stage_epochs):\n",
    "                    global_epoch += 1\n",
    "\n",
    "                    if enc_params and SEG_FREEZE_EPOCHS > 0 and global_epoch == (SEG_FREEZE_EPOCHS + 1):\n",
    "                        _set_trainable(enc_params, True)\n",
    "                        print(f\"[SEG:{spec['id']}] unfreeze encoder at epoch {global_epoch}\")\n",
    "\n",
    "                    train_stats = train_one_epoch(\n",
    "                        model,\n",
    "                        train_loader_fold,\n",
    "                        criterion,\n",
    "                        optimizer,\n",
    "                        DEVICE,\n",
    "                        use_amp=USE_AMP,\n",
    "                    )\n",
    "                    val_stats, val_dice = validate(model, val_loader_fold, criterion, DEVICE)\n",
    "\n",
    "                    lrs = [float(pg.get(\"lr\", 0.0)) for pg in optimizer.param_groups]\n",
    "                    print(\n",
    "                        f\"[SEG:{spec['id']}] fold {int(fold_id)} epoch {global_epoch:02d}/{total_epochs} \"\n",
    "                        f\"patch={patch_size} lr={lrs} \"\n",
    "                        f\"train_loss={train_stats.loss:.4f} val_loss={val_stats.loss:.4f} val_dice={val_dice:.4f}\"\n",
    "                    )\n",
    "\n",
    "                    if scheduler is not None:\n",
    "                        if SEG_SCHEDULER == \"plateau\":\n",
    "                            scheduler.step(val_stats.loss)\n",
    "                        else:\n",
    "                            scheduler.step()\n",
    "\n",
    "                    val_dice_f = float(val_dice)\n",
    "                    val_loss_f = float(val_stats.loss)\n",
    "\n",
    "                    improved_dice = val_dice_f > best_dice + SEG_MIN_DELTA\n",
    "                    improved_tie = (abs(val_dice_f - best_dice) <= SEG_MIN_DELTA) and (\n",
    "                        val_loss_f < best_loss - SEG_MIN_DELTA\n",
    "                    )\n",
    "\n",
    "                    if improved_dice or improved_tie:\n",
    "                        if improved_dice:\n",
    "                            best_dice = val_dice_f\n",
    "                        best_loss = val_loss_f\n",
    "                        bad_epochs = 0\n",
    "                        torch.save(\n",
    "                            {\n",
    "                                \"model_state\": model.state_dict(),\n",
    "                                \"epoch\": global_epoch,\n",
    "                                \"val_loss\": val_loss_f,\n",
    "                                \"val_dice\": val_dice_f,\n",
    "                                \"patch_size\": patch_size,\n",
    "                                \"spec\": spec,\n",
    "                                \"config\": cfg,\n",
    "                            },\n",
    "                            ckpt_path,\n",
    "                        )\n",
    "                    else:\n",
    "                        bad_epochs += 1\n",
    "\n",
    "                    if SEG_EARLY_STOPPING and bad_epochs >= SEG_PATIENCE:\n",
    "                        print(f\"[SEG:{spec['id']}] early stopping: sem melhora por {SEG_PATIENCE} épocas\")\n",
    "                        break\n",
    "\n",
    "                if SEG_EARLY_STOPPING and bad_epochs >= SEG_PATIENCE:\n",
    "                    break\n",
    "\n",
    "            print(\"[SEG] saved:\", ckpt_path)\n",
    "\n",
    "            del model, optimizer\n",
    "            if DEVICE.startswith(\"cuda\"):\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "print(\"segmentation checkpoints dir (save):\", SEG_SAVE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19bc7c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 10 — Carregar checkpoint (necessário para inferência/submissão)\n",
    "\n",
    "def _load_checkpoint(path: Path) -> tuple[dict, dict]:\n",
    "    ckpt = torch.load(path, map_location=\"cpu\")\n",
    "    if isinstance(ckpt, dict) and \"model_state\" in ckpt:\n",
    "        return ckpt[\"model_state\"], ckpt.get(\"config\", {})\n",
    "    return ckpt, {}\n",
    "\n",
    "\n",
    "def _seg_ckpt_path(model_id: str, fold: int) -> Path:\n",
    "    return (SEG_LOAD_DIR / model_id / f\"fold_{int(fold)}\") / \"best.pt\"\n",
    "\n",
    "\n",
    "SEG_USE_ENSEMBLE = True\n",
    "SEG_INFER_FOLDS = list(range(N_FOLDS)) if SEG_USE_ENSEMBLE else [FOLD]\n",
    "SEG_INFER_MODEL_IDS = [s[\"id\"] for s in SEG_MODEL_SPECS]\n",
    "\n",
    "models_by_id: dict[str, list[nn.Module]] = {}\n",
    "loaded_models: list[str] = []\n",
    "missing_models: list[str] = []\n",
    "loaded_folds_set: set[int] = set()\n",
    "\n",
    "for spec in SEG_MODEL_SPECS:\n",
    "    model_id = str(spec[\"id\"])\n",
    "    if model_id not in set(SEG_INFER_MODEL_IDS):\n",
    "        continue\n",
    "    models_by_id.setdefault(model_id, [])\n",
    "    for fold_id in SEG_INFER_FOLDS:\n",
    "        fold_ckpt = _seg_ckpt_path(model_id, int(fold_id))\n",
    "        tag = f\"{model_id}/fold_{int(fold_id)}\"\n",
    "        if not fold_ckpt.exists():\n",
    "            missing_models.append(tag)\n",
    "            continue\n",
    "\n",
    "        m = build_seg_model(spec, pretrained=False)\n",
    "        state, _ = _load_checkpoint(fold_ckpt)\n",
    "        m.load_state_dict(state)\n",
    "        m.to(DEVICE)\n",
    "        m.eval()\n",
    "        models_by_id[model_id].append(m)\n",
    "        loaded_models.append(tag)\n",
    "        loaded_folds_set.add(int(fold_id))\n",
    "\n",
    "loaded_folds = sorted(loaded_folds_set)\n",
    "print(\"loaded seg models:\", loaded_models)\n",
    "if missing_models:\n",
    "    print(\"missing seg models:\", missing_models[:20], (\"...\" if len(missing_models) > 20 else \"\"))\n",
    "if not loaded_models:\n",
    "    print(f\"[SEG] nenhum checkpoint encontrado em: {SEG_LOAD_DIR}\")\n",
    "    print(\"[SEG] opções:\")\n",
    "    print(\"- Treinar agora: defina `RUN_TRAIN=True` (célula 9) e rode o treino (vai salvar em `SEG_SAVE_DIR`).\")\n",
    "    print(\"- Usar pesos prontos: coloque `outputs/models_seg/<model_id>/fold_*/best.pt` em um dataset do Kaggle;\")\n",
    "    print(\"  o notebook tenta localizar automaticamente em `/kaggle/input/*/outputs/models_seg`.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b74338",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Célula 10b — (Opcional) Carregar classificador para gating na inferência\n",
    "CLS_GATE = True\n",
    "CLS_SKIP_THRESHOLD_INFER = float(CLS_SKIP_THRESHOLD)\n",
    "CLS_USE_ENSEMBLE = False\n",
    "CLS_INFER_FOLDS = loaded_folds if CLS_USE_ENSEMBLE else [FOLD]\n",
    "\n",
    "cls_models: list[torch.nn.Module] = []\n",
    "cls_loaded_folds: list[int] = []\n",
    "CLS_INFER_IMAGE_SIZE = CLS_IMAGE_SIZE\n",
    "\n",
    "if CLS_GATE and timm is None:\n",
    "    print(\"[CLS] timm indisponível; gating desativado.\")\n",
    "    CLS_GATE = False\n",
    "\n",
    "if CLS_GATE:\n",
    "    for fold_id in CLS_INFER_FOLDS:\n",
    "        cls_ckpt_path = (CLS_LOAD_DIR / f\"fold_{int(fold_id)}\") / \"best.pt\"\n",
    "        if not cls_ckpt_path.exists():\n",
    "            continue\n",
    "\n",
    "        state, cfg_cls = _load_checkpoint(cls_ckpt_path)\n",
    "        model_name = cfg_cls.get(\"model_name\", CLS_MODEL_NAME)\n",
    "        CLS_INFER_IMAGE_SIZE = int(cfg_cls.get(\"image_size\", CLS_INFER_IMAGE_SIZE))\n",
    "\n",
    "        m_cls = build_cls_model(model_name, pretrained=False)\n",
    "        m_cls.load_state_dict(state)\n",
    "        m_cls.to(DEVICE)\n",
    "        m_cls.eval()\n",
    "        cls_models.append(m_cls)\n",
    "        cls_loaded_folds.append(int(fold_id))\n",
    "\n",
    "    print(\"[CLS] loaded folds:\", cls_loaded_folds)\n",
    "    if not cls_models:\n",
    "        print(f\"[CLS] nenhum checkpoint encontrado em {CLS_LOAD_DIR}/fold_*/best.pt; gating desativado.\")\n",
    "        CLS_GATE = False\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_prob_forged(image: np.ndarray) -> float:\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    if not cls_models:\n",
    "        raise RuntimeError(\"Classifier models not loaded\")\n",
    "\n",
    "    img = normalize_image(image)\n",
    "    x = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n",
    "    if CLS_INFER_IMAGE_SIZE and x.shape[-2:] != (CLS_INFER_IMAGE_SIZE, CLS_INFER_IMAGE_SIZE):\n",
    "        x = F.interpolate(x, size=(CLS_INFER_IMAGE_SIZE, CLS_INFER_IMAGE_SIZE), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    probs = []\n",
    "    for m in cls_models:\n",
    "        logits = m(x).view(-1)\n",
    "        probs.append(float(torch.sigmoid(logits)[0].item()))\n",
    "    return float(np.mean(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ff2b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 11 — Pós-processamento (componentes conexos)\n",
    "_cc_backend = \"scipy\" if _cc_label is not None else (\"opencv\" if cv2 is not None else \"none\")\n",
    "\n",
    "\n",
    "def extract_components_safe(mask: np.ndarray, min_area: int = 0) -> list[np.ndarray]:\n",
    "    return extract_components(mask, min_area=min_area)\n",
    "\n",
    "\n",
    "print(\"connected components backend:\", _cc_backend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f4f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 12 — Validação em imagem inteira (oF1) + tuning simples de threshold (opcional)\n",
    "\n",
    "RUN_VAL_FULL = False  # mude para True para validar com oF1 em imagem inteira\n",
    "\n",
    "TILE_SIZE = 1024\n",
    "OVERLAP = 128\n",
    "MAX_SIZE = 0  # se quiser, defina ex.: 2048 para reduzir custo\n",
    "\n",
    "THRESHOLD = 0.5\n",
    "MIN_AREA = 32\n",
    "VAL_LIMIT = 200  # limite para não ficar gigante\n",
    "\n",
    "\n",
    "SEG_USE_TTA = True\n",
    "# Flips simples costumam dar bom ganho/custo no Kaggle.\n",
    "# Obs: usamos `.copy()` ao flipar a imagem para evitar strides negativos (torch.from_numpy não aceita).\n",
    "SEG_TTA_MODES = (\"none\", \"hflip\", \"vflip\")\n",
    "\n",
    "# Pesos do ensemble por arquitetura (média por-fold dentro de cada `model_id`, depois média ponderada entre ids).\n",
    "# Comece com pesos iguais; ajuste via validação se quiser.\n",
    "SEG_MODEL_WEIGHTS = {str(s[\"id\"]): 1.0 for s in SEG_MODEL_SPECS}\n",
    "# Exemplo:\n",
    "# SEG_MODEL_WEIGHTS.update({\"unetpp_effb7\": 0.4, \"deeplabv3p_se_r101\": 0.4, \"segformer_mitb3\": 0.2})\n",
    "\n",
    "# Pós-processamento (binário)\n",
    "PP_FILL_SMALL_HOLES = True\n",
    "PP_MAX_HOLE_AREA = 64  # só preenche \"buracos\" bem pequenos (em pixels)\n",
    "PP_MORPH_CLOSE_KERNEL = 0  # 0 desativa; ex.: 3 para suavizar/fechar micro falhas\n",
    "PP_MORPH_OPEN_KERNEL = 0  # 0 desativa; cuidado: pode apagar regiões pequenas reais\n",
    "PP_MORPH_ITERS = 1\n",
    "\n",
    "\n",
    "def _tta_apply_image(image: np.ndarray, mode: str) -> np.ndarray:\n",
    "    mode = str(mode)\n",
    "    if mode == \"none\":\n",
    "        return image\n",
    "    if mode == \"hflip\":\n",
    "        return image[:, ::-1, :].copy()\n",
    "    if mode == \"vflip\":\n",
    "        return image[::-1, :, :].copy()\n",
    "    if mode in {\"hvflip\", \"vhflip\"}:\n",
    "        return image[::-1, ::-1, :].copy()\n",
    "    raise ValueError(f\"Unknown TTA mode: {mode}\")\n",
    "\n",
    "\n",
    "def _tta_invert_mask(mask: np.ndarray, mode: str) -> np.ndarray:\n",
    "    mode = str(mode)\n",
    "    if mode == \"none\":\n",
    "        return mask\n",
    "    if mode == \"hflip\":\n",
    "        return mask[:, ::-1]\n",
    "    if mode == \"vflip\":\n",
    "        return mask[::-1, :]\n",
    "    if mode in {\"hvflip\", \"vhflip\"}:\n",
    "        return mask[::-1, ::-1]\n",
    "    raise ValueError(f\"Unknown TTA mode: {mode}\")\n",
    "\n",
    "\n",
    "def _predict_prob_single_model(model: nn.Module, image: np.ndarray) -> np.ndarray:\n",
    "    if not SEG_USE_TTA or not SEG_TTA_MODES:\n",
    "        return predict_image(model, image, DEVICE, tile_size=TILE_SIZE, overlap=OVERLAP, max_size=MAX_SIZE)\n",
    "\n",
    "    prob_sum: np.ndarray | None = None\n",
    "    for mode in SEG_TTA_MODES:\n",
    "        img_t = _tta_apply_image(image, mode)\n",
    "        prob = predict_image(model, img_t, DEVICE, tile_size=TILE_SIZE, overlap=OVERLAP, max_size=MAX_SIZE)\n",
    "        prob = _tta_invert_mask(prob, mode)\n",
    "        if prob_sum is None:\n",
    "            prob_sum = prob.astype(np.float32, copy=False)\n",
    "        else:\n",
    "            prob_sum += prob.astype(np.float32, copy=False)\n",
    "    return prob_sum / np.float32(len(SEG_TTA_MODES))\n",
    "\n",
    "\n",
    "def _fill_small_holes_cv2(mask: np.ndarray, max_area: int) -> np.ndarray:\n",
    "    import cv2\n",
    "\n",
    "    m = (np.asarray(mask) > 0).astype(np.uint8)\n",
    "    if max_area <= 0 or m.max() == 0:\n",
    "        return m\n",
    "\n",
    "    inv = (1 - m).astype(np.uint8)  # 1 = fundo/buracos\n",
    "    if inv.max() == 0:\n",
    "        return m\n",
    "\n",
    "    # Flood-fill no \"fundo externo\" para sobrar apenas buracos internos.\n",
    "    inv_pad = np.pad(inv, 1, mode=\"constant\", constant_values=1)\n",
    "    flood = inv_pad.copy()\n",
    "    ff_mask = np.zeros((flood.shape[0] + 2, flood.shape[1] + 2), dtype=np.uint8)\n",
    "    cv2.floodFill(flood, ff_mask, seedPoint=(0, 0), newVal=0)\n",
    "\n",
    "    holes = (flood == 1).astype(np.uint8)[1:-1, 1:-1]\n",
    "    if holes.max() == 0:\n",
    "        return m\n",
    "\n",
    "    n, labels, stats, _ = cv2.connectedComponentsWithStats(holes, connectivity=4)\n",
    "    for idx in range(1, int(n)):\n",
    "        area = int(stats[idx, cv2.CC_STAT_AREA])\n",
    "        if area <= int(max_area):\n",
    "            m[labels == idx] = 1\n",
    "    return m\n",
    "\n",
    "\n",
    "def _morphology_cv2(mask: np.ndarray, close_kernel: int, open_kernel: int, iters: int) -> np.ndarray:\n",
    "    import cv2\n",
    "\n",
    "    m = (np.asarray(mask) > 0).astype(np.uint8)\n",
    "    iters = int(max(iters, 1))\n",
    "\n",
    "    if int(close_kernel) and int(close_kernel) > 1:\n",
    "        k = int(close_kernel)\n",
    "        kernel = np.ones((k, k), dtype=np.uint8)\n",
    "        m = cv2.morphologyEx(m, cv2.MORPH_CLOSE, kernel, iterations=iters)\n",
    "\n",
    "    if int(open_kernel) and int(open_kernel) > 1:\n",
    "        k = int(open_kernel)\n",
    "        kernel = np.ones((k, k), dtype=np.uint8)\n",
    "        m = cv2.morphologyEx(m, cv2.MORPH_OPEN, kernel, iterations=iters)\n",
    "\n",
    "    return (m > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def postprocess_binary_mask(mask: np.ndarray) -> np.ndarray:\n",
    "    m = (np.asarray(mask) > 0).astype(np.uint8)\n",
    "    if m.max() == 0:\n",
    "        return m\n",
    "\n",
    "    try:\n",
    "        if PP_FILL_SMALL_HOLES:\n",
    "            m = _fill_small_holes_cv2(m, max_area=int(PP_MAX_HOLE_AREA))\n",
    "        if (int(PP_MORPH_CLOSE_KERNEL) and int(PP_MORPH_CLOSE_KERNEL) > 1) or (\n",
    "            int(PP_MORPH_OPEN_KERNEL) and int(PP_MORPH_OPEN_KERNEL) > 1\n",
    "        ):\n",
    "            m = _morphology_cv2(\n",
    "                m,\n",
    "                close_kernel=int(PP_MORPH_CLOSE_KERNEL),\n",
    "                open_kernel=int(PP_MORPH_OPEN_KERNEL),\n",
    "                iters=int(PP_MORPH_ITERS),\n",
    "            )\n",
    "    except Exception as exc:\n",
    "        print(\"[PP] erro no pós-processamento; seguindo sem. Erro abaixo:\")\n",
    "        traceback.print_exc()\n",
    "    return (m > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def predict_prob_ensemble(image: np.ndarray) -> np.ndarray:\n",
    "    if not models_by_id or not any(models_by_id.values()):\n",
    "        raise RuntimeError(\n",
    "            f\"Sem modelos carregados (nenhum checkpoint encontrado em {SEG_LOAD_DIR}/<model_id>/fold_*/best.pt).\"\n",
    "        )\n",
    "\n",
    "    total: np.ndarray | None = None\n",
    "    weight_sum = 0.0\n",
    "\n",
    "    for model_id, models in models_by_id.items():\n",
    "        if not models:\n",
    "            continue\n",
    "\n",
    "        # Média dentro do model_id (ex.: folds)\n",
    "        prob_id_sum: np.ndarray | None = None\n",
    "        for m in models:\n",
    "            prob = _predict_prob_single_model(m, image)\n",
    "            if prob_id_sum is None:\n",
    "                prob_id_sum = prob.astype(np.float32, copy=False)\n",
    "            else:\n",
    "                prob_id_sum += prob.astype(np.float32, copy=False)\n",
    "        prob_id = prob_id_sum / np.float32(len(models))\n",
    "\n",
    "        w = np.float32(SEG_MODEL_WEIGHTS.get(str(model_id), 1.0))\n",
    "        if float(w) <= 0.0:\n",
    "            continue\n",
    "\n",
    "        if total is None:\n",
    "            total = prob_id * w\n",
    "        else:\n",
    "            total += prob_id * w\n",
    "        weight_sum += float(w)\n",
    "\n",
    "    if total is None or weight_sum <= 0:\n",
    "        raise RuntimeError(\"Ensemble inválido: nenhum modelo com peso > 0 foi usado.\")\n",
    "\n",
    "    return total / np.float32(max(weight_sum, 1e-8))\n",
    "\n",
    "\n",
    "def predict_instances_for_image(image: np.ndarray, threshold: float, min_area: int) -> list[np.ndarray]:\n",
    "    prob = predict_prob_ensemble(image)\n",
    "    bin_mask = binarize(prob, threshold=threshold)\n",
    "    bin_mask = postprocess_binary_mask(bin_mask)\n",
    "    return extract_components_safe(bin_mask, min_area=min_area)\n",
    "\n",
    "\n",
    "def predict_instances_for_sample(sample, threshold: float, min_area: int) -> list[np.ndarray]:\n",
    "    image = load_image(sample.image_path)\n",
    "    return predict_instances_for_image(image, threshold=threshold, min_area=min_area)\n",
    "\n",
    "\n",
    "def mean_of1(samples, threshold: float, min_area: int, limit: int = 0) -> float | None:\n",
    "    scores: list[float] = []\n",
    "    for i, sample in enumerate(samples):\n",
    "        if limit and i >= limit:\n",
    "            break\n",
    "        gt_instances = load_mask_instances(sample.mask_path) if sample.mask_path else []\n",
    "        pred_instances = predict_instances_for_sample(sample, threshold=threshold, min_area=min_area)\n",
    "        try:\n",
    "            scores.append(float(score_image(gt_instances, pred_instances)))\n",
    "        except Exception as exc:\n",
    "            print(\"[METRIC] erro ao calcular métrica (provável falta de SciPy). Erro abaixo:\")\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "    if not scores:\n",
    "        return None\n",
    "    return float(np.mean(scores))\n",
    "\n",
    "\n",
    "if RUN_VAL_FULL and any(models_by_id.values()):\n",
    "    score = mean_of1(val_fold_samples, threshold=THRESHOLD, min_area=MIN_AREA, limit=VAL_LIMIT)\n",
    "    print(\"val mean oF1:\", score)\n",
    "\n",
    "    # grid simples (faixa curta; ajuste conforme custo)\n",
    "    thresholds = [0.3, 0.4, 0.5, 0.6]\n",
    "    min_areas = [0, 16, 32, 64, 128]\n",
    "    best = (None, None, -1.0)\n",
    "    for t in thresholds:\n",
    "        for a in min_areas:\n",
    "            s = mean_of1(val_fold_samples, threshold=t, min_area=a, limit=VAL_LIMIT)\n",
    "            if s is None:\n",
    "                break\n",
    "            print(f\"t={t:.2f} a={a:4d} -> {s:.5f}\")\n",
    "            if s > best[2]:\n",
    "                best = (t, a, s)\n",
    "    print(\"best:\", best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4259cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 13 — Gerar submission.csv (test)\n",
    "\n",
    "SUBMISSION_PATH = Path(\"/kaggle/working/submission.csv\") if is_kaggle() else (OUTPUT_ROOT / \"submission.csv\")\n",
    "\n",
    "RUN_SUBMISSION = True  # deixe True no Kaggle submit\n",
    "\n",
    "if RUN_SUBMISSION:\n",
    "    if not any(models_by_id.values()):\n",
    "        print(f\"Sem checkpoint(s) para inferência. Treine e salve em {SEG_SAVE_DIR}/<model_id>/fold_*/best.pt.\")\n",
    "    else:\n",
    "        SUBMISSION_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with SUBMISSION_PATH.open(\"w\", newline=\"\") as f:\n",
    "            # Formato oficial do `annotation` nesta competição:\n",
    "            # - \"authentic\" OU\n",
    "            # - 1+ instâncias em RLE (coluna-major/F-order, 1-based), cada uma serializada via `json.dumps([...])`,\n",
    "            #   concatenadas por ';'. `csv.DictWriter` cuida das aspas automaticamente (há vírgulas/colchetes).\n",
    "            writer = csv.DictWriter(f, fieldnames=[\"case_id\", \"annotation\"])\n",
    "            writer.writeheader()\n",
    "            for sample in test_samples:\n",
    "                image = load_image(sample.image_path)\n",
    "                if CLS_GATE:\n",
    "                    prob_forged = predict_prob_forged(image)\n",
    "                    if prob_forged < CLS_SKIP_THRESHOLD_INFER:\n",
    "                        writer.writerow({\"case_id\": sample.case_id, \"annotation\": \"authentic\"})\n",
    "                        continue\n",
    "\n",
    "                instances = predict_instances_for_image(image, threshold=THRESHOLD, min_area=MIN_AREA)\n",
    "                writer.writerow({\"case_id\": sample.case_id, \"annotation\": encode_instances(instances)})\n",
    "\n",
    "        print(\"wrote:\", SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e616080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 14 — Preview do CSV\n",
    "try:\n",
    "    import pandas as pd\n",
    "\n",
    "    from IPython.display import display\n",
    "\n",
    "    if not SUBMISSION_PATH.exists():\n",
    "        print(\"submission ainda não foi gerada:\", SUBMISSION_PATH)\n",
    "    else:\n",
    "        df_sub = pd.read_csv(SUBMISSION_PATH)\n",
    "        display(df_sub.head())\n",
    "        print(\"rows:\", len(df_sub))\n",
    "except Exception as exc:\n",
    "    print(\"[PREVIEW] erro ao mostrar preview do CSV. Erro abaixo:\")\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
