{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b18b8737",
   "metadata": {},
   "source": [
    "# Fase 3 — Treino de segmentação (Kaggle)\n",
    "\n",
    "Objetivo:\n",
    "- Treinar um modelo de segmentação para localizar regiões duplicadas.\n",
    "- Salvar checkpoints em `outputs/models_seg/<model_id>/fold_<k>/best.pt`.\n",
    "\n",
    "**Importante**\n",
    "- Este notebook **importa o código do projeto** em `src/forgeryseg/` (modularizado).\n",
    "- Internet pode estar OFF: use wheels offline se necessário.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69fec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1 — Sanidade Kaggle (lembrete)\n",
    "print(\"Kaggle submission constraints (lembrete):\")\n",
    "print(\"- Submissions via Notebook\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF no submit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8374b58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2 — Imports + ambiente\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "warnings.simplefilter(\"default\")\n",
    "os.environ.setdefault(\"NO_ALBUMENTATIONS_UPDATE\", \"1\")\n",
    "\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9dbb3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2b — Instalação offline (opcional): wheels via Kaggle Dataset (sem internet)\n",
    "#\n",
    "# Instala TODAS as wheels encontradas com `--no-deps` para evitar o pip tentar puxar\n",
    "# dependências do torch (ex.: nvidia-cuda-*) quando o Kaggle está offline.\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def _find_offline_bundle() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        return None\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return None\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        for base in (ds, ds / \"recodai_bundle\"):\n",
    "            if (base / \"wheels\").exists():\n",
    "                candidates.append(base)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    if len(candidates) > 1:\n",
    "        print(\"[OFFLINE INSTALL] múltiplos bundles com wheels encontrados; usando o primeiro:\")\n",
    "        for c in candidates:\n",
    "            print(\" -\", c)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def _is_competition_dataset_dir(path: Path) -> bool:\n",
    "    return (path / \"train_images\").exists() or (path / \"test_images\").exists() or (path / \"train_masks\").exists()\n",
    "\n",
    "\n",
    "def _candidate_python_roots(base: Path) -> list[Path]:\n",
    "    roots = [\n",
    "        base,\n",
    "        base / \"src\",\n",
    "        base / \"vendor\",\n",
    "        base / \"third_party\",\n",
    "        base / \"recodai_bundle\",\n",
    "        base / \"recodai_bundle\" / \"src\",\n",
    "        base / \"recodai_bundle\" / \"vendor\",\n",
    "        base / \"recodai_bundle\" / \"third_party\",\n",
    "    ]\n",
    "    return [r for r in roots if r.exists()]\n",
    "\n",
    "\n",
    "def add_local_package_to_syspath(package_dir_name: str) -> list[Path]:\n",
    "    added: list[Path] = []\n",
    "    if not is_kaggle():\n",
    "        return added\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return added\n",
    "\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        if _is_competition_dataset_dir(ds):\n",
    "            continue\n",
    "        for root in _candidate_python_roots(ds):\n",
    "            pkg = root / package_dir_name\n",
    "            if (pkg / \"__init__.py\").exists():\n",
    "                if str(root) not in sys.path:\n",
    "                    sys.path.insert(0, str(root))\n",
    "                    added.append(root)\n",
    "                continue\n",
    "            try:\n",
    "                for child in sorted(p for p in root.glob(\"*\") if p.is_dir()):\n",
    "                    pkg2 = child / package_dir_name\n",
    "                    if (pkg2 / \"__init__.py\").exists():\n",
    "                        if str(child) not in sys.path:\n",
    "                            sys.path.insert(0, str(child))\n",
    "                            added.append(child)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if added:\n",
    "        uniq = []\n",
    "        for p in added:\n",
    "            if p not in uniq:\n",
    "                uniq.append(p)\n",
    "        print(f\"[LOCAL IMPORT] adicionado ao sys.path para '{package_dir_name}':\")\n",
    "        for p in uniq[:10]:\n",
    "            print(\" -\", p)\n",
    "        if len(uniq) > 10:\n",
    "            print(\" ...\")\n",
    "        return uniq\n",
    "\n",
    "    print(f\"[LOCAL IMPORT] não encontrei '{package_dir_name}/__init__.py' em `/kaggle/input/*` (fora do dataset da competição).\")\n",
    "    return added\n",
    "\n",
    "\n",
    "OFFLINE_BUNDLE = _find_offline_bundle()\n",
    "if OFFLINE_BUNDLE is None:\n",
    "    print(\"[OFFLINE INSTALL] nenhum bundle com `wheels/` encontrado em `/kaggle/input`.\")\n",
    "else:\n",
    "    wheel_dir = OFFLINE_BUNDLE / \"wheels\"\n",
    "    whls = sorted(str(p) for p in wheel_dir.glob(\"*.whl\"))\n",
    "    print(\"[OFFLINE INSTALL] bundle:\", OFFLINE_BUNDLE)\n",
    "    print(\"[OFFLINE INSTALL] wheels:\", len(whls))\n",
    "    if not whls:\n",
    "        print(\"[OFFLINE INSTALL] aviso: `wheels/` existe, mas está vazio.\")\n",
    "    else:\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheel_dir),\n",
    "            \"--no-deps\",\n",
    "            *whls,\n",
    "        ]\n",
    "        print(\"[OFFLINE INSTALL] executando:\", \" \".join(cmd[:9]), \"...\", f\"(+{len(whls)} wheels)\")\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"[OFFLINE INSTALL] OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d4b1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2c — Import do projeto (src/forgeryseg)\n",
    "try:\n",
    "    import forgeryseg  # type: ignore\n",
    "except Exception:\n",
    "    local_src = Path(\"src\").resolve()\n",
    "    if (local_src / \"forgeryseg\" / \"__init__.py\").exists() and str(local_src) not in sys.path:\n",
    "        sys.path.insert(0, str(local_src))\n",
    "    if is_kaggle():\n",
    "        add_local_package_to_syspath(\"forgeryseg\")\n",
    "    import forgeryseg  # type: ignore\n",
    "\n",
    "print(\"forgeryseg:\", Path(forgeryseg.__file__).resolve())\n",
    "\n",
    "from torch.utils.data import DataLoader  # noqa: E402\n",
    "\n",
    "from forgeryseg.augment import get_train_augment, get_val_augment  # noqa: E402\n",
    "from forgeryseg.dataset import PatchDataset, build_train_index  # noqa: E402\n",
    "from forgeryseg.losses import BCETverskyLoss  # noqa: E402\n",
    "from forgeryseg.models import builders  # noqa: E402\n",
    "from forgeryseg.train import train_one_epoch, validate  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0889c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3 — Dataset root + index\n",
    "\n",
    "\n",
    "def find_dataset_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        base = Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "        if base.exists():\n",
    "            return base\n",
    "        kaggle_input = Path(\"/kaggle/input\")\n",
    "        if kaggle_input.exists():\n",
    "            for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "                if (ds / \"train_images\").exists() and (ds / \"test_images\").exists():\n",
    "                    return ds\n",
    "\n",
    "    base = Path(\"data\").resolve()\n",
    "    if (base / \"train_images\").exists() and (base / \"test_images\").exists():\n",
    "        return base\n",
    "\n",
    "    raise FileNotFoundError(\"Dataset não encontrado. No Kaggle: anexe o dataset da competição.\")\n",
    "\n",
    "\n",
    "DATA_ROOT = find_dataset_root()\n",
    "train_samples = build_train_index(DATA_ROOT, strict=True)\n",
    "labels = np.array([0 if s.is_authentic else 1 for s in train_samples], dtype=np.int64)\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"train samples:\", len(train_samples), \"auth:\", int((labels == 0).sum()), \"forged:\", int((labels == 1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15812ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4 — Split (5-fold estratificado)\n",
    "N_FOLDS = 5\n",
    "FOLD = 0\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    folds = np.zeros(len(train_samples), dtype=np.int64)\n",
    "    for fold_id, (_, val_idx) in enumerate(skf.split(np.zeros(len(train_samples)), labels)):\n",
    "        folds[val_idx] = int(fold_id)\n",
    "except Exception:\n",
    "    print(\"[ERRO] scikit-learn falhou (StratifiedKFold). Usando split simples.\")\n",
    "    traceback.print_exc()\n",
    "    folds = np.arange(len(train_samples), dtype=np.int64) % int(N_FOLDS)\n",
    "\n",
    "train_idx = np.where(folds != int(FOLD))[0]\n",
    "val_idx = np.where(folds == int(FOLD))[0]\n",
    "print(f\"fold={FOLD}: train={len(train_idx)} val={len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26909201",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5 — Config (patch + augs + dataloader)\n",
    "PATCH_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "EPOCHS = 15\n",
    "LR = 1e-3\n",
    "WEIGHT_DECAY = 1e-2\n",
    "\n",
    "POSITIVE_PROB = 0.70\n",
    "MIN_POS_PIXELS = 1\n",
    "\n",
    "NUM_WORKERS = 2 if is_kaggle() else 0\n",
    "\n",
    "train_aug = get_train_augment(patch_size=PATCH_SIZE, copy_move_prob=0.20)\n",
    "val_aug = get_val_augment()\n",
    "\n",
    "ds_train = PatchDataset(\n",
    "    [train_samples[i] for i in train_idx.tolist()],\n",
    "    patch_size=PATCH_SIZE,\n",
    "    train=True,\n",
    "    augment=train_aug,\n",
    "    positive_prob=POSITIVE_PROB,\n",
    "    min_pos_pixels=MIN_POS_PIXELS,\n",
    "    seed=SEED,\n",
    ")\n",
    "ds_val = PatchDataset(\n",
    "    [train_samples[i] for i in val_idx.tolist()],\n",
    "    patch_size=PATCH_SIZE,\n",
    "    train=False,\n",
    "    augment=val_aug,\n",
    "    seed=SEED,\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"dl_train batches:\", len(dl_train), \"dl_val batches:\", len(dl_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf2f3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6 — Modelo (escolha aqui)\n",
    "#\n",
    "# Importante (offline): use `encoder_weights=None`.\n",
    "MODEL_ID = \"unetpp_effb7\"\n",
    "ENCODER_NAME = \"efficientnet-b7\"\n",
    "\n",
    "try:\n",
    "    model = builders.build_unetplusplus(encoder_name=ENCODER_NAME, encoder_weights=None, classes=1, strict_weights=True)\n",
    "except Exception:\n",
    "    print(\"[ERRO] falha ao construir modelo (provável falta de segmentation_models_pytorch ou encoder).\")\n",
    "    traceback.print_exc()\n",
    "    raise\n",
    "\n",
    "model = model.to(DEVICE)\n",
    "print(\"model:\", MODEL_ID, \"encoder:\", ENCODER_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad9f401",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Célula 7 — Treino + checkpoint\n",
    "\n",
    "\n",
    "def output_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        return Path(\"/kaggle/working\")\n",
    "    return Path(\".\").resolve()\n",
    "\n",
    "\n",
    "SAVE_DIR = output_root() / \"outputs\" / \"models_seg\" / MODEL_ID / f\"fold_{int(FOLD)}\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PATH = SAVE_DIR / \"best.pt\"\n",
    "\n",
    "criterion = BCETverskyLoss(alpha=0.7, beta=0.3, tversky_weight=1.0)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "\n",
    "use_amp = (DEVICE == \"cuda\")\n",
    "best_dice = -1.0\n",
    "\n",
    "for epoch in range(1, int(EPOCHS) + 1):\n",
    "    tr = train_one_epoch(model, dl_train, criterion, optimizer, DEVICE, use_amp=use_amp, progress=True, desc=\"seg train\")\n",
    "    val_stats, val_dice = validate(model, dl_val, criterion, DEVICE, progress=True, desc=\"seg val\")\n",
    "    print(\n",
    "        f\"epoch {epoch:02d}/{EPOCHS} | train_loss={tr.loss:.4f} | val_loss={val_stats.loss:.4f} | dice@0.5={val_dice:.4f}\"\n",
    "    )\n",
    "\n",
    "    if float(val_dice) > best_dice:\n",
    "        best_dice = float(val_dice)\n",
    "        ckpt = {\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"config\": {\n",
    "                \"backend\": \"smp\",\n",
    "                \"arch\": \"unetplusplus\",\n",
    "                \"encoder_name\": ENCODER_NAME,\n",
    "                \"encoder_weights\": None,\n",
    "                \"classes\": 1,\n",
    "                \"model_id\": MODEL_ID,\n",
    "                \"patch_size\": int(PATCH_SIZE),\n",
    "                \"fold\": int(FOLD),\n",
    "                \"seed\": int(SEED),\n",
    "            },\n",
    "            \"score\": float(best_dice),\n",
    "        }\n",
    "        torch.save(ckpt, BEST_PATH)\n",
    "        print(\"  saved best ->\", BEST_PATH)\n",
    "\n",
    "print(\"best dice:\", best_dice)\n",
    "print(\"saved dir:\", SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
