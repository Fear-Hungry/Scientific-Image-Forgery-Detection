{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3af98b2a",
   "metadata": {},
   "source": [
    "# Fase 3 — Treino do segmentador (máscara de duplicação)\n",
    "\n",
    "Objetivo:\n",
    "- Treinar um segmentador binário (1 canal) para localizar regiões duplicadas.\n",
    "- Salvar checkpoints em `/kaggle/working/outputs/models_seg/` (Kaggle) ou `outputs/models_seg/` (local).\n",
    "\n",
    "**Regras**\n",
    "- Notebook-only (sem importar código do projeto).\n",
    "- Internet pode estar OFF; use wheels offline se necessário.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675cb9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1 — Sanidade Kaggle (lembrete)\n",
    "print(\"Kaggle submission constraints (lembrete):\")\n",
    "print(\"- Submissions via Notebook\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF no submit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16d4a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2 — Imports + ambiente\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "warnings.simplefilter(\"default\")\n",
    "\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec663a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2b — Instalação offline (opcional): wheels via Kaggle Dataset (sem internet)\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def _find_offline_bundle() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        return None\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return None\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        for base in (ds, ds / \"recodai_bundle\"):\n",
    "            if (base / \"wheels\").exists():\n",
    "                candidates.append(base)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    if len(candidates) > 1:\n",
    "        print(\"[OFFLINE INSTALL] múltiplos bundles com wheels encontrados; usando o primeiro:\")\n",
    "        for c in candidates:\n",
    "            print(\" -\", c)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "OFFLINE_BUNDLE = _find_offline_bundle()\n",
    "if OFFLINE_BUNDLE is None:\n",
    "    print(\"[OFFLINE INSTALL] nenhum bundle com `wheels/` encontrado em `/kaggle/input`.\")\n",
    "else:\n",
    "    wheel_dir = OFFLINE_BUNDLE / \"wheels\"\n",
    "    whls = sorted(str(p) for p in wheel_dir.glob(\"*.whl\"))\n",
    "    print(\"[OFFLINE INSTALL] bundle:\", OFFLINE_BUNDLE)\n",
    "    print(\"[OFFLINE INSTALL] wheels:\", len(whls))\n",
    "    if whls:\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheel_dir),\n",
    "            *whls,\n",
    "        ]\n",
    "        print(\"[OFFLINE INSTALL] executando:\", \" \".join(cmd[:8]), \"...\", f\"(+{len(whls)} wheels)\")\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"[OFFLINE INSTALL] OK.\")\n",
    "    else:\n",
    "        print(\"[OFFLINE INSTALL] aviso: `wheels/` existe, mas está vazio.\")\n",
    "\n",
    "\n",
    "def _is_competition_dataset_dir(path: Path) -> bool:\n",
    "    return (path / \"train_images\").exists() or (path / \"test_images\").exists() or (path / \"train_masks\").exists()\n",
    "\n",
    "\n",
    "def _candidate_python_roots(base: Path) -> list[Path]:\n",
    "    roots = [\n",
    "        base,\n",
    "        base / \"src\",\n",
    "        base / \"vendor\",\n",
    "        base / \"third_party\",\n",
    "        base / \"recodai_bundle\",\n",
    "        base / \"recodai_bundle\" / \"src\",\n",
    "        base / \"recodai_bundle\" / \"vendor\",\n",
    "        base / \"recodai_bundle\" / \"third_party\",\n",
    "    ]\n",
    "    return [r for r in roots if r.exists()]\n",
    "\n",
    "\n",
    "def add_local_package_to_syspath(package_dir_name: str) -> list[Path]:\n",
    "    added: list[Path] = []\n",
    "    if not is_kaggle():\n",
    "        return added\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return added\n",
    "\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        if _is_competition_dataset_dir(ds):\n",
    "            continue\n",
    "        for root in _candidate_python_roots(ds):\n",
    "            pkg = root / package_dir_name\n",
    "            if (pkg / \"__init__.py\").exists():\n",
    "                if str(root) not in sys.path:\n",
    "                    sys.path.insert(0, str(root))\n",
    "                    added.append(root)\n",
    "                continue\n",
    "            try:\n",
    "                for child in sorted(p for p in root.glob(\"*\") if p.is_dir()):\n",
    "                    pkg2 = child / package_dir_name\n",
    "                    if (pkg2 / \"__init__.py\").exists():\n",
    "                        if str(child) not in sys.path:\n",
    "                            sys.path.insert(0, str(child))\n",
    "                            added.append(child)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if added:\n",
    "        uniq = []\n",
    "        for p in added:\n",
    "            if p not in uniq:\n",
    "                uniq.append(p)\n",
    "        print(f\"[LOCAL IMPORT] adicionado ao sys.path para '{package_dir_name}':\")\n",
    "        for p in uniq[:10]:\n",
    "            print(\" -\", p)\n",
    "        if len(uniq) > 10:\n",
    "            print(\" ...\")\n",
    "        return uniq\n",
    "\n",
    "    print(f\"[LOCAL IMPORT] não encontrei '{package_dir_name}/__init__.py' em `/kaggle/input/*` (fora do dataset da competição).\")\n",
    "    return added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f10975a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3 — Dataset paths (Kaggle/local)\n",
    "\n",
    "\n",
    "def find_dataset_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        base = Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "        if base.exists():\n",
    "            return base\n",
    "        kaggle_input = Path(\"/kaggle/input\")\n",
    "        if kaggle_input.exists():\n",
    "            for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "                if (ds / \"train_images\").exists() and (ds / \"test_images\").exists():\n",
    "                    return ds\n",
    "\n",
    "    base = Path(\"data\").resolve()\n",
    "    if (base / \"train_images\").exists() and (base / \"test_images\").exists():\n",
    "        return base\n",
    "\n",
    "    raise FileNotFoundError(\"Dataset não encontrado. No Kaggle: anexe o dataset da competição.\")\n",
    "\n",
    "\n",
    "DATA_ROOT = find_dataset_root()\n",
    "TRAIN_IMAGES = DATA_ROOT / \"train_images\"\n",
    "TRAIN_MASKS = DATA_ROOT / \"train_masks\"\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44eab6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4 — Index (train) para segmentação (máscara binária)\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class SegSample:\n",
    "    case_id: str\n",
    "    image_path: Path\n",
    "    mask_path: Path | None\n",
    "    label: int  # 0 authentic, 1 forged\n",
    "\n",
    "\n",
    "def build_seg_index(train_images_dir: Path, train_masks_dir: Path) -> list[SegSample]:\n",
    "    samples: list[SegSample] = []\n",
    "    for label_name, y in [(\"authentic\", 0), (\"forged\", 1)]:\n",
    "        for img_path in sorted((train_images_dir / label_name).glob(\"*.png\")):\n",
    "            case_id = img_path.stem\n",
    "            mask_path = train_masks_dir / f\"{case_id}.npy\" if label_name == \"forged\" else None\n",
    "            samples.append(SegSample(case_id=case_id, image_path=img_path, mask_path=mask_path, label=int(y)))\n",
    "    if not samples:\n",
    "        raise FileNotFoundError(f\"Nenhuma imagem encontrada em: {train_images_dir}\")\n",
    "    return samples\n",
    "\n",
    "\n",
    "train_samples = build_seg_index(TRAIN_IMAGES, TRAIN_MASKS)\n",
    "y = np.array([s.label for s in train_samples], dtype=np.int64)\n",
    "print(\"train samples:\", len(train_samples))\n",
    "print(\"authentic:\", int((y == 0).sum()), \"forged:\", int((y == 1).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697e4082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5 — Split (5-fold estratificado)\n",
    "N_FOLDS = 5\n",
    "FOLD = 0\n",
    "\n",
    "try:\n",
    "    from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "    skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)\n",
    "    folds = np.zeros(len(train_samples), dtype=np.int64)\n",
    "    for fold_id, (_, val_idx) in enumerate(skf.split(np.zeros(len(train_samples)), y)):\n",
    "        folds[val_idx] = int(fold_id)\n",
    "except Exception:\n",
    "    print(\"[ERRO] Falha ao usar scikit-learn para split; usando split simples (não estratificado).\")\n",
    "    traceback.print_exc()\n",
    "    folds = np.arange(len(train_samples), dtype=np.int64) % int(N_FOLDS)\n",
    "\n",
    "train_idx = np.where(folds != int(FOLD))[0]\n",
    "val_idx = np.where(folds == int(FOLD))[0]\n",
    "\n",
    "print(f\"fold={FOLD}: train={len(train_idx)} val={len(val_idx)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6fe779",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 6 — Dataset/DataLoader (patch-based, sem depender de libs externas)\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    print(\"[WARN] tqdm indisponível; usando loop simples.\")\n",
    "\n",
    "    def tqdm(x, **kwargs):  # type: ignore\n",
    "        return x\n",
    "\n",
    "\n",
    "IMAGENET_MEAN = (0.485, 0.456, 0.406)\n",
    "IMAGENET_STD = (0.229, 0.224, 0.225)\n",
    "\n",
    "PATCH_SIZE = 512\n",
    "BATCH_SIZE = 8\n",
    "NUM_WORKERS = 2 if is_kaggle() else 4\n",
    "\n",
    "\n",
    "def load_image_rgb(path: Path) -> np.ndarray:\n",
    "    img = Image.open(path).convert(\"RGB\")\n",
    "    return np.asarray(img)\n",
    "\n",
    "\n",
    "def load_mask_binary(path: Path | None, shape_hw: tuple[int, int]) -> np.ndarray:\n",
    "    if path is None or not path.exists():\n",
    "        return np.zeros(shape_hw, dtype=np.uint8)\n",
    "\n",
    "    arr = np.load(path)\n",
    "    arr = np.asarray(arr)\n",
    "\n",
    "    if arr.ndim == 2:\n",
    "        m = arr\n",
    "    elif arr.ndim == 3:\n",
    "        # comum: (N, H, W) instâncias ou (H, W, C)\n",
    "        if arr.shape[0] < 32 and arr.shape[1:] == shape_hw:\n",
    "            m = arr.max(axis=0)\n",
    "        elif arr.shape[:2] == shape_hw:\n",
    "            m = arr.max(axis=2)\n",
    "        else:\n",
    "            # fallback conservador\n",
    "            m = arr.max(axis=0)\n",
    "    else:\n",
    "        raise ValueError(f\"mask array com shape inesperado: {arr.shape}\")\n",
    "\n",
    "    return (np.asarray(m) > 0).astype(np.uint8)\n",
    "\n",
    "\n",
    "def _pad_to_min(image: np.ndarray, mask: np.ndarray, min_h: int, min_w: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    h, w = image.shape[:2]\n",
    "    pad_h = max(int(min_h) - int(h), 0)\n",
    "    pad_w = max(int(min_w) - int(w), 0)\n",
    "    if pad_h == 0 and pad_w == 0:\n",
    "        return image, mask\n",
    "    image_p = np.pad(image, ((0, pad_h), (0, pad_w), (0, 0)), mode=\"constant\", constant_values=0)\n",
    "    mask_p = np.pad(mask, ((0, pad_h), (0, pad_w)), mode=\"constant\", constant_values=0)\n",
    "    return image_p, mask_p\n",
    "\n",
    "\n",
    "def _random_crop(image: np.ndarray, mask: np.ndarray, size: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    image, mask = _pad_to_min(image, mask, size, size)\n",
    "    h, w = image.shape[:2]\n",
    "    y0 = random.randint(0, h - size)\n",
    "    x0 = random.randint(0, w - size)\n",
    "    return image[y0 : y0 + size, x0 : x0 + size], mask[y0 : y0 + size, x0 : x0 + size]\n",
    "\n",
    "\n",
    "def _augment(image: np.ndarray, mask: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    if random.random() < 0.5:\n",
    "        image = np.ascontiguousarray(image[:, ::-1])\n",
    "        mask = np.ascontiguousarray(mask[:, ::-1])\n",
    "    if random.random() < 0.5:\n",
    "        image = np.ascontiguousarray(image[::-1, :])\n",
    "        mask = np.ascontiguousarray(mask[::-1, :])\n",
    "    if random.random() < 0.20:\n",
    "        k = random.randint(0, 3)\n",
    "        if k:\n",
    "            image = np.ascontiguousarray(np.rot90(image, k))\n",
    "            mask = np.ascontiguousarray(np.rot90(mask, k))\n",
    "    return image, mask\n",
    "\n",
    "\n",
    "def normalize_image(image: np.ndarray, mean=IMAGENET_MEAN, std=IMAGENET_STD) -> np.ndarray:\n",
    "    x = image.astype(np.float32)\n",
    "    if x.max() > 1.0:\n",
    "        x /= 255.0\n",
    "    mean = np.array(mean, dtype=np.float32)\n",
    "    std = np.array(std, dtype=np.float32)\n",
    "    return (x - mean) / std\n",
    "\n",
    "\n",
    "class SegPatchDataset(Dataset):\n",
    "    def __init__(self, samples: list[SegSample], train: bool):\n",
    "        self.samples = samples\n",
    "        self.train = bool(train)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        s = self.samples[int(idx)]\n",
    "        image = load_image_rgb(s.image_path)\n",
    "        h, w = image.shape[:2]\n",
    "        mask = load_mask_binary(s.mask_path, shape_hw=(h, w))\n",
    "\n",
    "        image, mask = _random_crop(image, mask, int(PATCH_SIZE))\n",
    "        if self.train:\n",
    "            image, mask = _augment(image, mask)\n",
    "\n",
    "        image = normalize_image(image)\n",
    "        x = torch.from_numpy(image).permute(2, 0, 1).float()\n",
    "        y = torch.from_numpy(mask[None, :, :]).float()\n",
    "        return x, y\n",
    "\n",
    "\n",
    "ds_train = SegPatchDataset([train_samples[i] for i in train_idx.tolist()], train=True)\n",
    "ds_val = SegPatchDataset([train_samples[i] for i in val_idx.tolist()], train=False)\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    ds_train,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=True,\n",
    ")\n",
    "dl_val = DataLoader(\n",
    "    ds_val,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=(DEVICE == \"cuda\"),\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "print(\"dl_train batches:\", len(dl_train), \"dl_val batches:\", len(dl_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e7ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 7 — Modelo (SMP preferencial; fallback torchvision)\n",
    "try:\n",
    "    import segmentation_models_pytorch as smp\n",
    "except Exception:\n",
    "    smp = None\n",
    "    print(\"[WARN] segmentation_models_pytorch indisponível.\")\n",
    "    traceback.print_exc()\n",
    "    # tenta vendor via Kaggle Dataset GitHub\n",
    "    add_local_package_to_syspath(\"segmentation_models_pytorch\")\n",
    "    add_local_package_to_syspath(\"segmentation_models_pytorch\".replace(\"-\", \"_\"))\n",
    "    try:\n",
    "        import segmentation_models_pytorch as smp  # type: ignore\n",
    "    except Exception:\n",
    "        smp = None\n",
    "\n",
    "\n",
    "def _tv_deeplabv3_resnet50() -> nn.Module:\n",
    "    try:\n",
    "        from torchvision.models.segmentation import deeplabv3_resnet50\n",
    "\n",
    "        # compat com versões diferentes do torchvision\n",
    "        try:\n",
    "            m = deeplabv3_resnet50(weights=None, weights_backbone=None)\n",
    "        except TypeError:\n",
    "            m = deeplabv3_resnet50(pretrained=False)\n",
    "\n",
    "        # troca head para 1 canal\n",
    "        head = m.classifier[-1]\n",
    "        m.classifier[-1] = nn.Conv2d(head.in_channels, 1, kernel_size=1)\n",
    "\n",
    "        class _Wrap(nn.Module):\n",
    "            def __init__(self, base: nn.Module):\n",
    "                super().__init__()\n",
    "                self.base = base\n",
    "\n",
    "            def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "                out = self.base(x)\n",
    "                if isinstance(out, dict):\n",
    "                    out = out[\"out\"]\n",
    "                return out\n",
    "\n",
    "        return _Wrap(m)\n",
    "    except Exception:\n",
    "        print(\"[ERRO] falha ao criar torchvision deeplabv3_resnet50.\")\n",
    "        traceback.print_exc()\n",
    "        raise\n",
    "\n",
    "\n",
    "def build_seg_model() -> tuple[nn.Module, dict]:\n",
    "    if smp is not None:\n",
    "        try:\n",
    "            m = smp.UnetPlusPlus(\n",
    "                encoder_name=\"efficientnet-b4\",\n",
    "                encoder_weights=None,  # evita download\n",
    "                classes=1,\n",
    "                activation=None,\n",
    "            )\n",
    "            cfg = {\n",
    "                \"backend\": \"smp\",\n",
    "                \"arch\": \"UnetPlusPlus\",\n",
    "                \"encoder_name\": \"efficientnet-b4\",\n",
    "                \"classes\": 1,\n",
    "            }\n",
    "            return m, cfg\n",
    "        except Exception:\n",
    "            print(\"[ERRO] falha ao criar SMP UnetPlusPlus.\")\n",
    "            traceback.print_exc()\n",
    "            raise\n",
    "\n",
    "    m = _tv_deeplabv3_resnet50()\n",
    "    cfg = {\"backend\": \"torchvision\", \"arch\": \"deeplabv3_resnet50\", \"classes\": 1}\n",
    "    return m, cfg\n",
    "\n",
    "\n",
    "model, model_cfg = build_seg_model()\n",
    "model = model.to(DEVICE)\n",
    "print(\"model cfg:\", model_cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a86fc8f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Célula 8 — Loss (BCE + Dice)\n",
    "\n",
    "\n",
    "def dice_loss_with_logits(logits: torch.Tensor, targets: torch.Tensor, eps: float = 1e-6) -> torch.Tensor:\n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs = probs.view(probs.size(0), -1)\n",
    "    targets = targets.view(targets.size(0), -1)\n",
    "    inter = (probs * targets).sum(dim=1)\n",
    "    den = probs.sum(dim=1) + targets.sum(dim=1)\n",
    "    dice = (2.0 * inter + eps) / (den + eps)\n",
    "    return 1.0 - dice.mean()\n",
    "\n",
    "\n",
    "def bce_dice_loss(logits: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    bce = torch.nn.functional.binary_cross_entropy_with_logits(logits, targets)\n",
    "    d = dice_loss_with_logits(logits, targets)\n",
    "    return bce + d\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def dice_score_from_logits(logits: torch.Tensor, targets: torch.Tensor, threshold: float = 0.5, eps: float = 1e-6) -> float:\n",
    "    probs = torch.sigmoid(logits)\n",
    "    pred = (probs >= float(threshold)).float()\n",
    "    inter = (pred * targets).sum().item()\n",
    "    den = pred.sum().item() + targets.sum().item()\n",
    "    return float((2.0 * inter + eps) / (den + eps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef4817c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Célula 9 — Treino (com progresso) + checkpoint\n",
    "from time import time\n",
    "\n",
    "LR = 1e-3\n",
    "EPOCHS = 15\n",
    "WEIGHT_DECAY = 1e-2\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(DEVICE == \"cuda\"))\n",
    "\n",
    "\n",
    "def output_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        return Path(\"/kaggle/working\")\n",
    "    return Path(\".\").resolve()\n",
    "\n",
    "\n",
    "MODEL_ID = \"unetpp_effb4\" if model_cfg.get(\"backend\") == \"smp\" else \"deeplabv3_r50\"\n",
    "SAVE_DIR = output_root() / \"outputs\" / \"models_seg\" / MODEL_ID / f\"fold_{int(FOLD)}\"\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "BEST_PATH = SAVE_DIR / \"best.pt\"\n",
    "\n",
    "best_dice = -1.0\n",
    "\n",
    "\n",
    "def train_one_epoch(model: nn.Module, loader: DataLoader) -> float:\n",
    "    model.train()\n",
    "    losses: list[float] = []\n",
    "    for x, yb in tqdm(loader, desc=\"train\", leave=False):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with torch.cuda.amp.autocast(enabled=(DEVICE == \"cuda\")):\n",
    "            logits = model(x)\n",
    "            loss = bce_dice_loss(logits, yb)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        losses.append(float(loss.item()))\n",
    "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model: nn.Module, loader: DataLoader) -> dict:\n",
    "    model.eval()\n",
    "    losses: list[float] = []\n",
    "    dices: list[float] = []\n",
    "    for x, yb in tqdm(loader, desc=\"val\", leave=False):\n",
    "        x = x.to(DEVICE, non_blocking=True)\n",
    "        yb = yb.to(DEVICE, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        losses.append(float(bce_dice_loss(logits, yb).item()))\n",
    "        dices.append(dice_score_from_logits(logits, yb, threshold=0.5))\n",
    "    return {\"loss\": float(np.mean(losses)) if losses else float(\"nan\"), \"dice@0.5\": float(np.mean(dices)) if dices else 0.0}\n",
    "\n",
    "\n",
    "for epoch in range(1, int(EPOCHS) + 1):\n",
    "    t0 = time()\n",
    "    tr_loss = train_one_epoch(model, dl_train)\n",
    "    val = evaluate(model, dl_val)\n",
    "    elapsed = time() - t0\n",
    "    print(\n",
    "        f\"epoch {epoch:02d}/{EPOCHS} | train_loss={tr_loss:.4f} | val_loss={val['loss']:.4f} | \"\n",
    "        f\"dice@0.5={val['dice@0.5']:.4f} | time={elapsed:.1f}s\"\n",
    "    )\n",
    "\n",
    "    if float(val[\"dice@0.5\"]) > best_dice:\n",
    "        best_dice = float(val[\"dice@0.5\"])\n",
    "        ckpt = {\n",
    "            \"model_state\": model.state_dict(),\n",
    "            \"config\": {\n",
    "                **model_cfg,\n",
    "                \"model_id\": MODEL_ID,\n",
    "                \"patch_size\": int(PATCH_SIZE),\n",
    "                \"fold\": int(FOLD),\n",
    "                \"seed\": int(SEED),\n",
    "            },\n",
    "            \"score\": float(best_dice),\n",
    "        }\n",
    "        torch.save(ckpt, BEST_PATH)\n",
    "        print(\"  saved best ->\", BEST_PATH)\n",
    "\n",
    "print(\"best dice:\", best_dice)\n",
    "print(\"saved dir:\", SAVE_DIR)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
