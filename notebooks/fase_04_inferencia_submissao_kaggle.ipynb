{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6c1da8c",
   "metadata": {},
   "source": [
    "# Fase 4 — Inferência + submissão (Kaggle)\n",
    "\n",
    "Objetivo:\n",
    "- Carregar checkpoints treinados (segmentação + opcional classificador).\n",
    "- Rodar inferência no `test_images/` e gerar `submission.csv`.\n",
    "\n",
    "**Importante**\n",
    "- Este notebook **importa o código do projeto** em `src/forgeryseg/` (modularizado).\n",
    "- Internet pode estar OFF: use wheels offline se necessário.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3e6e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 1 — Sanidade Kaggle (lembrete)\n",
    "print(\"Kaggle submission constraints (lembrete):\")\n",
    "print(\"- Submissions via Notebook\")\n",
    "print(\"- Runtime <= 4h (CPU/GPU)\")\n",
    "print(\"- Internet: OFF no submit\")\n",
    "print(\"- Output: /kaggle/working/submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fefcb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2 — Imports + ambiente\n",
    "import csv\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import traceback\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "warnings.simplefilter(\"default\")\n",
    "\n",
    "\n",
    "def is_kaggle() -> bool:\n",
    "    return bool(os.environ.get(\"KAGGLE_URL_BASE\")) or Path(\"/kaggle\").exists()\n",
    "\n",
    "\n",
    "def set_seed(seed: int = 42) -> None:\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "\n",
    "SEED = 42\n",
    "set_seed(SEED)\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if DEVICE == \"cuda\":\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "print(\"python:\", sys.version.split()[0])\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c164a119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2b — Instalação offline (opcional): wheels via Kaggle Dataset (sem internet)\n",
    "#\n",
    "# Instala TODAS as wheels encontradas com `--no-deps` para evitar o pip tentar puxar\n",
    "# dependências do torch (ex.: nvidia-cuda-*) quando o Kaggle está offline.\n",
    "import subprocess\n",
    "\n",
    "\n",
    "def _find_offline_bundle() -> Path | None:\n",
    "    if not is_kaggle():\n",
    "        return None\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return None\n",
    "\n",
    "    candidates: list[Path] = []\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        for base in (ds, ds / \"recodai_bundle\"):\n",
    "            if (base / \"wheels\").exists():\n",
    "                candidates.append(base)\n",
    "\n",
    "    if not candidates:\n",
    "        return None\n",
    "    if len(candidates) > 1:\n",
    "        print(\"[OFFLINE INSTALL] múltiplos bundles com wheels encontrados; usando o primeiro:\")\n",
    "        for c in candidates:\n",
    "            print(\" -\", c)\n",
    "    return candidates[0]\n",
    "\n",
    "\n",
    "def _is_competition_dataset_dir(path: Path) -> bool:\n",
    "    return (path / \"train_images\").exists() or (path / \"test_images\").exists() or (path / \"train_masks\").exists()\n",
    "\n",
    "\n",
    "def _candidate_python_roots(base: Path) -> list[Path]:\n",
    "    roots = [\n",
    "        base,\n",
    "        base / \"src\",\n",
    "        base / \"vendor\",\n",
    "        base / \"third_party\",\n",
    "        base / \"recodai_bundle\",\n",
    "        base / \"recodai_bundle\" / \"src\",\n",
    "        base / \"recodai_bundle\" / \"vendor\",\n",
    "        base / \"recodai_bundle\" / \"third_party\",\n",
    "    ]\n",
    "    return [r for r in roots if r.exists()]\n",
    "\n",
    "\n",
    "def add_local_package_to_syspath(package_dir_name: str) -> list[Path]:\n",
    "    \"\"\"\n",
    "    Procura por `package_dir_name/__init__.py` em `/kaggle/input/*` (exceto o dataset da competição)\n",
    "    e adiciona o root correspondente ao `sys.path`.\n",
    "    \"\"\"\n",
    "    added: list[Path] = []\n",
    "    if not is_kaggle():\n",
    "        return added\n",
    "\n",
    "    kaggle_input = Path(\"/kaggle/input\")\n",
    "    if not kaggle_input.exists():\n",
    "        return added\n",
    "\n",
    "    for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "        if _is_competition_dataset_dir(ds):\n",
    "            continue\n",
    "        for root in _candidate_python_roots(ds):\n",
    "            pkg = root / package_dir_name\n",
    "            if (pkg / \"__init__.py\").exists():\n",
    "                if str(root) not in sys.path:\n",
    "                    sys.path.insert(0, str(root))\n",
    "                    added.append(root)\n",
    "                continue\n",
    "            try:\n",
    "                for child in sorted(p for p in root.glob(\"*\") if p.is_dir()):\n",
    "                    pkg2 = child / package_dir_name\n",
    "                    if (pkg2 / \"__init__.py\").exists():\n",
    "                        if str(child) not in sys.path:\n",
    "                            sys.path.insert(0, str(child))\n",
    "                            added.append(child)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    if added:\n",
    "        uniq = []\n",
    "        for p in added:\n",
    "            if p not in uniq:\n",
    "                uniq.append(p)\n",
    "        print(f\"[LOCAL IMPORT] adicionado ao sys.path para '{package_dir_name}':\")\n",
    "        for p in uniq[:10]:\n",
    "            print(\" -\", p)\n",
    "        if len(uniq) > 10:\n",
    "            print(\" ...\")\n",
    "        return uniq\n",
    "\n",
    "    print(f\"[LOCAL IMPORT] não encontrei '{package_dir_name}/__init__.py' em `/kaggle/input/*` (fora do dataset da competição).\")\n",
    "    return added\n",
    "\n",
    "\n",
    "OFFLINE_BUNDLE = _find_offline_bundle()\n",
    "if OFFLINE_BUNDLE is None:\n",
    "    print(\"[OFFLINE INSTALL] nenhum bundle com `wheels/` encontrado em `/kaggle/input`.\")\n",
    "else:\n",
    "    wheel_dir = OFFLINE_BUNDLE / \"wheels\"\n",
    "    whls = sorted(str(p) for p in wheel_dir.glob(\"*.whl\"))\n",
    "    print(\"[OFFLINE INSTALL] bundle:\", OFFLINE_BUNDLE)\n",
    "    print(\"[OFFLINE INSTALL] wheels:\", len(whls))\n",
    "    if not whls:\n",
    "        print(\"[OFFLINE INSTALL] aviso: `wheels/` existe, mas está vazio.\")\n",
    "    else:\n",
    "        cmd = [\n",
    "            sys.executable,\n",
    "            \"-m\",\n",
    "            \"pip\",\n",
    "            \"install\",\n",
    "            \"--no-index\",\n",
    "            \"--find-links\",\n",
    "            str(wheel_dir),\n",
    "            \"--no-deps\",\n",
    "            *whls,\n",
    "        ]\n",
    "        print(\"[OFFLINE INSTALL] executando:\", \" \".join(cmd[:9]), \"...\", f\"(+{len(whls)} wheels)\")\n",
    "        subprocess.check_call(cmd)\n",
    "        print(\"[OFFLINE INSTALL] OK.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82f48a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 2c — Import do projeto (src/forgeryseg)\n",
    "try:\n",
    "    import forgeryseg  # type: ignore\n",
    "except Exception:\n",
    "    # Local\n",
    "    local_src = Path(\"src\").resolve()\n",
    "    if (local_src / \"forgeryseg\" / \"__init__.py\").exists() and str(local_src) not in sys.path:\n",
    "        sys.path.insert(0, str(local_src))\n",
    "    # Kaggle (dataset com o repo)\n",
    "    if is_kaggle():\n",
    "        add_local_package_to_syspath(\"forgeryseg\")\n",
    "    import forgeryseg  # type: ignore\n",
    "\n",
    "print(\"forgeryseg:\", Path(forgeryseg.__file__).resolve())\n",
    "\n",
    "from forgeryseg.checkpoints import build_classifier_from_config, build_segmentation_from_config, load_checkpoint  # noqa: E402\n",
    "from forgeryseg.constants import AUTHENTIC_LABEL  # noqa: E402\n",
    "from forgeryseg.dataset import build_test_index, load_image  # noqa: E402\n",
    "from forgeryseg.inference import normalize_image, predict_image  # noqa: E402\n",
    "from forgeryseg.postprocess import binarize, extract_components  # noqa: E402\n",
    "from forgeryseg.rle import encode_instances  # noqa: E402"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4205390e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 3 — Dataset (test)\n",
    "\n",
    "\n",
    "def find_dataset_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        base = Path(\"/kaggle/input/recodai-luc-scientific-image-forgery-detection\")\n",
    "        if base.exists():\n",
    "            return base\n",
    "        kaggle_input = Path(\"/kaggle/input\")\n",
    "        if kaggle_input.exists():\n",
    "            for ds in sorted(kaggle_input.glob(\"*\")):\n",
    "                if (ds / \"train_images\").exists() and (ds / \"test_images\").exists():\n",
    "                    return ds\n",
    "\n",
    "    base = Path(\"data\").resolve()\n",
    "    if (base / \"train_images\").exists() and (base / \"test_images\").exists():\n",
    "        return base\n",
    "\n",
    "    raise FileNotFoundError(\"Dataset não encontrado. No Kaggle: anexe o dataset da competição.\")\n",
    "\n",
    "\n",
    "DATA_ROOT = find_dataset_root()\n",
    "test_samples = build_test_index(DATA_ROOT)\n",
    "print(\"DATA_ROOT:\", DATA_ROOT)\n",
    "print(\"test images:\", len(test_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7969b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 4 — Localizar diretórios de checkpoints (Kaggle/local)\n",
    "\n",
    "\n",
    "def output_root() -> Path:\n",
    "    if is_kaggle():\n",
    "        return Path(\"/kaggle/working\")\n",
    "    return Path(\".\").resolve()\n",
    "\n",
    "\n",
    "def _find_models_dir(dir_name: str) -> Path | None:\n",
    "    # Preferência: Kaggle Dataset anexado contendo outputs/\n",
    "    if is_kaggle():\n",
    "        ki = Path(\"/kaggle/input\")\n",
    "        if ki.exists():\n",
    "            candidates = []\n",
    "            for ds in sorted(ki.glob(\"*\")):\n",
    "                for base in (ds, ds / \"recodai_bundle\"):\n",
    "                    cand = base / \"outputs\" / dir_name\n",
    "                    if cand.exists():\n",
    "                        candidates.append(cand)\n",
    "            if candidates:\n",
    "                if len(candidates) > 1:\n",
    "                    print(f\"[CKPT] múltiplos candidatos para outputs/{dir_name}; usando o primeiro:\")\n",
    "                    for c in candidates:\n",
    "                        print(\" -\", c)\n",
    "                return candidates[0]\n",
    "\n",
    "    local = output_root() / \"outputs\" / dir_name\n",
    "    if local.exists():\n",
    "        return local\n",
    "    return None\n",
    "\n",
    "\n",
    "MODELS_SEG_DIR = _find_models_dir(\"models_seg\")\n",
    "MODELS_CLS_DIR = _find_models_dir(\"models_cls\")\n",
    "\n",
    "print(\"MODELS_SEG_DIR:\", MODELS_SEG_DIR)\n",
    "print(\"MODELS_CLS_DIR:\", MODELS_CLS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdd1418",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Célula 5 — Carregar checkpoints\n",
    "\n",
    "SEG_MODELS: list[torch.nn.Module] = []\n",
    "SEG_MODEL_TAGS: list[str] = []\n",
    "\n",
    "if MODELS_SEG_DIR is None:\n",
    "    print(\"[ERRO] MODELS_SEG_DIR não encontrado. Treine e salve em outputs/models_seg/... ou anexe um dataset com isso.\")\n",
    "else:\n",
    "    seg_ckpts = sorted(MODELS_SEG_DIR.glob(\"*/*/best.pt\"))\n",
    "    print(\"seg checkpoints encontrados:\", len(seg_ckpts))\n",
    "    for p in seg_ckpts:\n",
    "        try:\n",
    "            state, cfg = load_checkpoint(p)\n",
    "            m = build_segmentation_from_config(cfg)\n",
    "            m.load_state_dict(state)\n",
    "            m.to(DEVICE)\n",
    "            m.eval()\n",
    "            SEG_MODELS.append(m)\n",
    "            SEG_MODEL_TAGS.append(str(p.relative_to(MODELS_SEG_DIR)))\n",
    "        except Exception:\n",
    "            print(\"[ERRO] falha ao carregar seg checkpoint:\", p)\n",
    "            traceback.print_exc()\n",
    "\n",
    "print(\"loaded seg models:\", SEG_MODEL_TAGS[:10], (\"...\" if len(SEG_MODEL_TAGS) > 10 else \"\"))\n",
    "if not SEG_MODELS:\n",
    "    raise RuntimeError(\"Nenhum modelo de segmentação foi carregado. Veja os erros acima.\")\n",
    "\n",
    "\n",
    "CLS_MODELS: list[torch.nn.Module] = []\n",
    "CLS_INFER_IMAGE_SIZE = 384\n",
    "CLS_SKIP_THRESHOLD = 0.30  # mude aqui se quiser (favorece recall em forged com threshold mais baixo)\n",
    "\n",
    "if MODELS_CLS_DIR is None:\n",
    "    print(\"[CLS] MODELS_CLS_DIR não encontrado; gating desativado.\")\n",
    "else:\n",
    "    cls_ckpts = sorted(MODELS_CLS_DIR.glob(\"fold_*/best.pt\"))\n",
    "    print(\"cls checkpoints encontrados:\", len(cls_ckpts))\n",
    "    for p in cls_ckpts:\n",
    "        try:\n",
    "            state, cfg = load_checkpoint(p)\n",
    "            m, image_size = build_classifier_from_config(cfg)\n",
    "            CLS_INFER_IMAGE_SIZE = int(image_size)\n",
    "            m.load_state_dict(state)\n",
    "            m.to(DEVICE)\n",
    "            m.eval()\n",
    "            CLS_MODELS.append(m)\n",
    "        except Exception:\n",
    "            print(\"[ERRO] falha ao carregar cls checkpoint:\", p)\n",
    "            traceback.print_exc()\n",
    "\n",
    "print(\"loaded cls models:\", len(CLS_MODELS), \"CLS_INFER_IMAGE_SIZE:\", CLS_INFER_IMAGE_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180eb30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Célula 6 — Inferência (ensemble + TTA) + submissão\n",
    "try:\n",
    "    from tqdm.auto import tqdm\n",
    "except Exception:\n",
    "    print(\"[WARN] tqdm indisponível; usando loop simples.\")\n",
    "\n",
    "    def tqdm(x, **kwargs):  # type: ignore\n",
    "        return x\n",
    "\n",
    "\n",
    "TILE_SIZE = 1024\n",
    "OVERLAP = 128\n",
    "MAX_SIZE = 0\n",
    "\n",
    "THRESHOLD = 0.50\n",
    "MIN_AREA = 32\n",
    "\n",
    "USE_TTA = True\n",
    "TTA_MODES = (\"none\", \"hflip\", \"vflip\")\n",
    "\n",
    "\n",
    "def _apply_tta(image: np.ndarray, mode: str) -> np.ndarray:\n",
    "    if mode == \"none\":\n",
    "        return image\n",
    "    if mode == \"hflip\":\n",
    "        return np.ascontiguousarray(image[:, ::-1])\n",
    "    if mode == \"vflip\":\n",
    "        return np.ascontiguousarray(image[::-1, :])\n",
    "    raise ValueError(f\"tta mode inválido: {mode}\")\n",
    "\n",
    "\n",
    "def _undo_tta(mask: np.ndarray, mode: str) -> np.ndarray:\n",
    "    if mode == \"none\":\n",
    "        return mask\n",
    "    if mode == \"hflip\":\n",
    "        return np.ascontiguousarray(mask[:, ::-1])\n",
    "    if mode == \"vflip\":\n",
    "        return np.ascontiguousarray(mask[::-1, :])\n",
    "    raise ValueError(f\"tta mode inválido: {mode}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def predict_prob_forged(image: np.ndarray) -> float:\n",
    "    if not CLS_MODELS:\n",
    "        raise RuntimeError(\"CLS_MODELS vazio\")\n",
    "    import torch.nn.functional as F\n",
    "\n",
    "    img = normalize_image(image)\n",
    "    x = torch.from_numpy(img).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n",
    "    if CLS_INFER_IMAGE_SIZE and x.shape[-2:] != (CLS_INFER_IMAGE_SIZE, CLS_INFER_IMAGE_SIZE):\n",
    "        x = F.interpolate(x, size=(CLS_INFER_IMAGE_SIZE, CLS_INFER_IMAGE_SIZE), mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "    probs: list[float] = []\n",
    "    for m in CLS_MODELS:\n",
    "        logits = m(x).view(-1)\n",
    "        probs.append(float(torch.sigmoid(logits)[0].item()))\n",
    "    return float(np.mean(probs))\n",
    "\n",
    "\n",
    "def predict_seg_ensemble_prob(image: np.ndarray) -> np.ndarray:\n",
    "    probs_sum: np.ndarray | None = None\n",
    "    count = 0\n",
    "    modes = TTA_MODES if USE_TTA else (\"none\",)\n",
    "    for mode in modes:\n",
    "        img_t = _apply_tta(image, mode)\n",
    "        ens: np.ndarray | None = None\n",
    "        for m in SEG_MODELS:\n",
    "            p = predict_image(m, img_t, DEVICE, tile_size=TILE_SIZE, overlap=OVERLAP, max_size=MAX_SIZE)\n",
    "            ens = p if ens is None else (ens + p)\n",
    "        assert ens is not None\n",
    "        ens = ens / float(len(SEG_MODELS))\n",
    "        ens = _undo_tta(ens, mode)\n",
    "        probs_sum = ens if probs_sum is None else (probs_sum + ens)\n",
    "        count += 1\n",
    "    assert probs_sum is not None\n",
    "    return probs_sum / float(max(count, 1))\n",
    "\n",
    "\n",
    "def predict_instances(image: np.ndarray) -> list[np.ndarray]:\n",
    "    prob = predict_seg_ensemble_prob(image)\n",
    "    bin_mask = binarize(prob, threshold=THRESHOLD)\n",
    "    # pós-processamento simples: remove componentes muito pequenas\n",
    "    return extract_components(bin_mask, min_area=int(MIN_AREA))\n",
    "\n",
    "\n",
    "SUBMISSION_PATH = Path(\"/kaggle/working/submission.csv\") if is_kaggle() else (output_root() / \"submission.csv\")\n",
    "SUBMISSION_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with SUBMISSION_PATH.open(\"w\", newline=\"\") as f:\n",
    "    writer = csv.DictWriter(f, fieldnames=[\"case_id\", \"annotation\"])\n",
    "    writer.writeheader()\n",
    "\n",
    "    for s in tqdm(test_samples, desc=\"infer\"):\n",
    "        img = load_image(s.image_path)\n",
    "\n",
    "        if CLS_MODELS:\n",
    "            p_forged = predict_prob_forged(img)\n",
    "            if float(p_forged) < float(CLS_SKIP_THRESHOLD):\n",
    "                writer.writerow({\"case_id\": s.case_id, \"annotation\": AUTHENTIC_LABEL})\n",
    "                continue\n",
    "\n",
    "        inst = predict_instances(img)\n",
    "        writer.writerow({\"case_id\": s.case_id, \"annotation\": encode_instances(inst)})\n",
    "\n",
    "print(\"wrote:\", SUBMISSION_PATH)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
